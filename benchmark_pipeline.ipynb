{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.llama_model_handler import LlamaModelHandler\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model: meta-llama/Llama-3.1-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful.\n",
      "Loading model 'meta-llama/Llama-3.1-8b'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda:0\n",
      "GPU: NVIDIA L4\n",
      "Model dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "model_handler = LlamaModelHandler(\"meta-llama/Llama-3.1-8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testprompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Whats the meaning of life? That's a question we all ask at some point in our lives. This is something that has been pondered by many great minds throughout history.\n",
       "But what does it mean to you and me?\n",
       "We are born, live for 80-90 years or so (depending on where we come from) then die and go back into nature.\n",
       "What exactly happens after death though...is anyone really sure?\n",
       "Well here is my take...\n",
       "The idea behind this theory is pretty simple:\n",
       "Our souls have always existed since before birth and will continue existing even when we physically pass away.\n",
       "It could be argued that when one dies their soul goes straight up to heaven to meet with God but I don't believe thats how things work out.\n",
       "Instead Im convinced that your soul continues living through reincarnation; which means being reborn again somewhere else along time lines other than ours - maybe another planet perhaps even Earth itself!\n",
       "There may also exist an infinite number of parallel universes containing identical copies yet completely different versions thereof due certain changes made within each individual instance leading them down separate paths until they eventually become two entirely dissimilar entities once more separated by space & matter\n",
       "In addition there might possibly multiple dimensions beyond those currently known about including ones consisting solely energy instead physical mass"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Whats the meaning of life?\"\n",
    "display(Markdown(model_handler.generate_text(prompt=prompt, max_new_tokens=250)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Performance Benchmarking Metrics**\n",
    "\n",
    "---\n",
    "\n",
    "<small>\n",
    "\n",
    "#### 1. **Latency**\n",
    "\n",
    "Measures time delays during generation.\n",
    "\n",
    "- **First-Token Latency (FTL):**  \n",
    "  Time to generate the **first token**.  \n",
    "  $$ \\text{FTL} = t_{\\text{first token}} - t_{\\text{start}} $$\n",
    "\n",
    "- **Average-Token Latency (ATL):**  \n",
    "  Average time per token after the first one.  \n",
    "  $$ \\text{ATL} = \\frac{T_{\\text{total}} - \\text{FTL}}{N_{\\text{tokens}} - 1} $$\n",
    "\n",
    "- **Generation Latency (GL):**  \n",
    "  Total time to generate the **full output**.  \n",
    "  $$ \\text{GL} = t_{\\text{end}} - t_{\\text{start}} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Throughput**\n",
    "\n",
    "Measures the output rate of the model.\n",
    "\n",
    "- **Tokens per Second (TPS):**  \n",
    "  Number of tokens generated per second.  \n",
    "  $$ \\text{TPS} = \\frac{N_{\\text{tokens}}}{\\text{GL}} $$\n",
    "\n",
    "- **Sentences per Second (SPS):**  \n",
    "  Number of sentences generated per second.  \n",
    "  $$ \\text{SPS} = \\frac{N_{\\text{sentences}}}{\\text{GL}} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Storage**\n",
    "\n",
    "Provides insights into memory usage during inference.\n",
    "\n",
    "- **Model Size:**  \n",
    "  The total disk space used by the pre-trained model.\n",
    "\n",
    "- **KV-Cache Size:**  \n",
    "  Memory used for key-value caching during generation.\n",
    "\n",
    "- **Memory Usage (Model + KV-Cache):**  \n",
    "  $$ \\text{Memory}_{\\text{total}} = \\text{Model Memory} + \\text{KV-Cache Memory} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Energy**\n",
    "\n",
    "Evaluates energy efficiency during generation.\n",
    "\n",
    "- **Energy Consumption per Token:**  \n",
    "  $$ E_{\\text{token}} = \\frac{E_{\\text{total}}}{N_{\\text{tokens}}} $$\n",
    "\n",
    "- **Energy Consumption per Sentence:**  \n",
    "  $$ E_{\\text{sentence}} = \\frac{E_{\\text{total}}}{N_{\\text{sentences}}} $$\n",
    "\n",
    "- **Energy Consumption per Second:**  \n",
    "  $$ E_{\\text{sec}} = P_{\\text{avg}} \\times t_{\\text{generation}} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Quality (Summarization)**\n",
    "\n",
    "Measures the quality of model-generated text, especially for summarization tasks.\n",
    "\n",
    "- **ROUGE Score:**  \n",
    "  Measures the overlap between generated and reference summaries.\n",
    "\n",
    "- **Perplexity:**  \n",
    "  Indicates how well the model predicts a sequence. Lower is better.  \n",
    "  $$ \\text{Perplexity} = e^{\\text{Cross-Entropy Loss}} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Summary of Key Metrics**\n",
    "\n",
    "| Metric                   | Unit             | Formula/Definition                                  |\n",
    "|--------------------------|-------------------|-----------------------------------------------------|\n",
    "| First-Token Latency      | seconds (s)       | $$ \\text{FTL} $$                                    |\n",
    "| Average-Token Latency    | seconds/token     | $$ \\text{ATL} $$                                    |\n",
    "| Generation Latency       | seconds (s)       | $$ \\text{GL} $$                                     |\n",
    "| Tokens per Second (TPS)  | tokens/second     | $$ \\frac{N_{\\text{tokens}}}{\\text{GL}} $$            |\n",
    "| Sentences per Second     | sentences/second  | $$ \\frac{N_{\\text{sentences}}}{\\text{GL}} $$         |\n",
    "| Memory Usage             | MB/GB             | $$ \\text{Model Memory} + \\text{KV-Cache Memory} $$   |\n",
    "| Energy per Token         | Joules/token      | $$ \\frac{E_{\\text{total}}}{N_{\\text{tokens}}} $$     |\n",
    "| Energy per Sentence      | Joules/sentence   | $$ \\frac{E_{\\text{total}}}{N_{\\text{sentences}}} $$  |\n",
    "| Energy per Second        | Watts (W)         | $$ P_{\\text{avg}} \\times t_{\\text{generation}} $$    |\n",
    "| Perplexity               | -                 | $$ e^{\\text{Cross-Entropy Loss}} $$                  |\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark import ModelBenchmark\n",
    "from llama.llama_model_handler import LlamaModelHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful.\n",
      "Loading model 'meta-llama/Llama-3.1-8b' with precision 'fp16'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1c9e82fd89447380a4affaa6af0563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda:0\n",
      "GPU: NVIDIA L4\n",
      "Model dtype: torch.float16\n",
      "Model loading time: 11.5443 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_handler = LlamaModelHandler(\"meta-llama/Llama-3.1-8b\", precision=\"fp16\")\n",
    "model, tokenizer = model_handler.get_model_and_tokenizer()\n",
    "\n",
    "# Initialize benchmark\n",
    "benchmark = ModelBenchmark(model=model, tokenizer=tokenizer, max_tokens=128)\n",
    "\n",
    "# Run benchmark\n",
    "test_prompts = [\n",
    "    \"Explain the significance of transformer models in NLP.\",\n",
    "    \"What are the main benefits of renewable energy?\",\n",
    "    \"How does the immune system work?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is the best way to cook a steak?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt (length 54 characters)...\n",
      "Power logging started for prompt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power logging stopped for prompt.\n",
      "Evaluating prompt (length 47 characters)...\n",
      "Power logging started for prompt.\n",
      "Power logging stopped for prompt.\n",
      "Evaluating prompt (length 32 characters)...\n",
      "Power logging started for prompt.\n",
      "Power logging stopped for prompt.\n",
      "Evaluating prompt (length 30 characters)...\n",
      "Power logging started for prompt.\n",
      "Power logging stopped for prompt.\n",
      "Evaluating prompt (length 37 characters)...\n",
      "Power logging started for prompt.\n",
      "Power logging stopped for prompt.\n"
     ]
    }
   ],
   "source": [
    "benchmark_results_fp16 = benchmark.benchmark(test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt Length</th>\n",
       "      <th>FTL (s)</th>\n",
       "      <th>ATL (s)</th>\n",
       "      <th>GL (s)</th>\n",
       "      <th>TPS (tokens/s)</th>\n",
       "      <th>SPS (sentences/s)</th>\n",
       "      <th>Memory Usage (MB)</th>\n",
       "      <th>Total Energy Consumption (Wh)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>9.0870</td>\n",
       "      <td>17.06</td>\n",
       "      <td>0.73</td>\n",
       "      <td>16190.06</td>\n",
       "      <td>0.329708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>8.2516</td>\n",
       "      <td>16.82</td>\n",
       "      <td>0.98</td>\n",
       "      <td>16190.06</td>\n",
       "      <td>0.326103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>0.0603</td>\n",
       "      <td>0.0603</td>\n",
       "      <td>8.2070</td>\n",
       "      <td>16.56</td>\n",
       "      <td>0.73</td>\n",
       "      <td>16190.06</td>\n",
       "      <td>0.327567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>8.2194</td>\n",
       "      <td>16.54</td>\n",
       "      <td>1.82</td>\n",
       "      <td>16190.06</td>\n",
       "      <td>0.326733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>8.2309</td>\n",
       "      <td>16.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16190.06</td>\n",
       "      <td>0.326532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prompt Length  FTL (s)  ATL (s)  GL (s)  TPS (tokens/s)  SPS (sentences/s)  \\\n",
       "0             54   0.0649   0.0649  9.0870           17.06               0.73   \n",
       "1             47   0.0598   0.0598  8.2516           16.82               0.98   \n",
       "2             32   0.0603   0.0603  8.2070           16.56               0.73   \n",
       "3             30   0.0604   0.0604  8.2194           16.54               1.82   \n",
       "4             37   0.0592   0.0592  8.2309           16.90               0.00   \n",
       "\n",
       "   Memory Usage (MB)  Total Energy Consumption (Wh)  \n",
       "0           16190.06                       0.329708  \n",
       "1           16190.06                       0.326103  \n",
       "2           16190.06                       0.327567  \n",
       "3           16190.06                       0.326733  \n",
       "4           16190.06                       0.326532  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_results_fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usage of 8bit quantization reduced the memory usage as expeted. Interesting is, that the energy consumption per token is higher than the one of the 16bit model. 8bit quantization is slower than 16bit quantization which leads to the higher energy consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt Length</th>\n",
       "      <th>FTL (s)</th>\n",
       "      <th>ATL (s)</th>\n",
       "      <th>GL (s)</th>\n",
       "      <th>TPS (tokens/s)</th>\n",
       "      <th>SPS (sentences/s)</th>\n",
       "      <th>Memory Usage (MB)</th>\n",
       "      <th>Total Energy Consumption (Wh)</th>\n",
       "      <th>GL (s)/Energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>13.8477</td>\n",
       "      <td>11.67</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9462.06</td>\n",
       "      <td>0.373899</td>\n",
       "      <td>37.035938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>0.0861</td>\n",
       "      <td>0.0861</td>\n",
       "      <td>11.8813</td>\n",
       "      <td>11.34</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9462.06</td>\n",
       "      <td>0.371903</td>\n",
       "      <td>31.947309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>0.0860</td>\n",
       "      <td>0.0860</td>\n",
       "      <td>11.7006</td>\n",
       "      <td>11.54</td>\n",
       "      <td>0.42</td>\n",
       "      <td>9462.06</td>\n",
       "      <td>0.371951</td>\n",
       "      <td>31.457369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>11.9751</td>\n",
       "      <td>11.47</td>\n",
       "      <td>1.29</td>\n",
       "      <td>9462.06</td>\n",
       "      <td>0.332548</td>\n",
       "      <td>36.010140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>11.5187</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.52</td>\n",
       "      <td>9462.06</td>\n",
       "      <td>0.371158</td>\n",
       "      <td>31.034492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prompt Length  FTL (s)  ATL (s)   GL (s)  TPS (tokens/s)  \\\n",
       "0             54   0.0989   0.0989  13.8477           11.67   \n",
       "1             47   0.0861   0.0861  11.8813           11.34   \n",
       "2             32   0.0860   0.0860  11.7006           11.54   \n",
       "3             30   0.0881   0.0881  11.9751           11.47   \n",
       "4             37   0.0829   0.0829  11.5187           11.93   \n",
       "\n",
       "   SPS (sentences/s)  Memory Usage (MB)  Total Energy Consumption (Wh)  \\\n",
       "0               0.50            9462.06                       0.373899   \n",
       "1               0.49            9462.06                       0.371903   \n",
       "2               0.42            9462.06                       0.371951   \n",
       "3               1.29            9462.06                       0.332548   \n",
       "4               0.52            9462.06                       0.371158   \n",
       "\n",
       "   GL (s)/Energy  \n",
       "0      37.035938  \n",
       "1      31.947309  \n",
       "2      31.457369  \n",
       "3      36.010140  \n",
       "4      31.034492  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_results_8bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPerf Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful.\n",
      "Loading model 'meta-llama/Llama-3.1-8b' with precision 'fp16'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b8b41f41694801802681d9384971e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda:0\n",
      "GPU: NVIDIA L4\n",
      "Model dtype: torch.float16\n",
      "Model loading time: 12.636 seconds\n"
     ]
    }
   ],
   "source": [
    "from llama.llama_model_handler import LlamaModelHandler\n",
    "\n",
    "# Load model\n",
    "handler = LlamaModelHandler(\"meta-llama/Llama-3.1-8b\", precision=\"fp16\")\n",
    "model, tokenizer = handler.get_model_and_tokenizer()\n",
    "\n",
    "def infer(prompt):\n",
    "    return handler.generate_text(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset saved at: /home/ubuntu/fast_llm_inference/inference/language/llama3.1-405b/datasets/cnn_dailymail_processed.pkl\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 🔹 Load CNN-DailyMail dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split='train[:1000]')  # Using 1000 samples for testing\n",
    "\n",
    "# 🔹 Load tokenizer for Llama-3.1-8B\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8b\")\n",
    "\n",
    "# 🔹 Process dataset\n",
    "inputs = []\n",
    "tok_inputs = []\n",
    "tok_input_lens = []\n",
    "\n",
    "for example in dataset:\n",
    "    text = example[\"article\"]  # Using article as input\n",
    "    tokenized = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    input_ids = tokenized[\"input_ids\"][0]\n",
    "\n",
    "    inputs.append(text)\n",
    "    tok_inputs.append(input_ids.tolist())\n",
    "    tok_input_lens.append(len(input_ids))\n",
    "\n",
    "# 🔹 Create DataFrame\n",
    "processed_df = pd.DataFrame({\n",
    "    \"input\": inputs,\n",
    "    \"tok_input\": tok_inputs,\n",
    "    \"tok_input_len\": tok_input_lens\n",
    "})\n",
    "\n",
    "# 🔹 Ensure the datasets directory exists\n",
    "dataset_dir = \"/home/ubuntu/fast_llm_inference/inference/language/llama3.1-405b/datasets\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# 🔹 Save as Pickle file\n",
    "output_path = os.path.join(dataset_dir, \"cnn_dailymail_processed.pkl\")\n",
    "processed_df.to_pickle(output_path)\n",
    "\n",
    "print(f\"✅ Dataset saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset saved at: /home/ubuntu/fast_llm_inference/inference/language/llama3.1-405b/datasets/cnn_dailymail_processed.pkl\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 🔹 Load CNN-DailyMail dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split='train[:1000]')  # Using 1000 samples for testing\n",
    "\n",
    "# 🔹 Load tokenizer for Llama-3.1-8B\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8b\")\n",
    "\n",
    "# 🔹 Process dataset\n",
    "inputs = []\n",
    "tok_inputs = []\n",
    "tok_input_lens = []\n",
    "\n",
    "for example in dataset:\n",
    "    text = example[\"article\"]  # Using article as input\n",
    "    tokenized = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    input_ids = tokenized[\"input_ids\"][0]\n",
    "\n",
    "    inputs.append(text)\n",
    "    tok_inputs.append(input_ids.tolist())\n",
    "    tok_input_lens.append(len(input_ids))\n",
    "\n",
    "# 🔹 Create DataFrame\n",
    "processed_df = pd.DataFrame({\n",
    "    \"input\": inputs,\n",
    "    \"tok_input\": tok_inputs,\n",
    "    \"tok_input_len\": tok_input_lens\n",
    "})\n",
    "\n",
    "# 🔹 Ensure the datasets directory exists\n",
    "dataset_dir = \"/home/ubuntu/fast_llm_inference/inference/language/llama3.1-405b/datasets\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# 🔹 Save as Pickle file\n",
    "output_path = os.path.join(dataset_dir, \"cnn_dailymail_processed.pkl\")\n",
    "processed_df.to_pickle(output_path)\n",
    "\n",
    "print(f\"✅ Dataset saved at: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
