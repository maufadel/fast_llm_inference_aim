{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.llama_model_handler import LlamaModelHandler\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model: meta-llama/Llama-3.1-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful.\n",
      "Loading model 'meta-llama/Llama-3.1-8b'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda:0\n",
      "GPU: NVIDIA L4\n",
      "Model dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "model_handler = LlamaModelHandler(\"meta-llama/Llama-3.1-8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testprompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Whats the meaning of life? That's a question we all ask at some point in our lives. This is something that has been pondered by many great minds throughout history.\n",
       "But what does it mean to you and me?\n",
       "We are born, live for 80-90 years or so (depending on where we come from) then die and go back into nature.\n",
       "What exactly happens after death though...is anyone really sure?\n",
       "Well here is my take...\n",
       "The idea behind this theory is pretty simple:\n",
       "Our souls have always existed since before birth and will continue existing even when we physically pass away.\n",
       "It could be argued that when one dies their soul goes straight up to heaven to meet with God but I don't believe thats how things work out.\n",
       "Instead Im convinced that your soul continues living through reincarnation; which means being reborn again somewhere else along time lines other than ours - maybe another planet perhaps even Earth itself!\n",
       "There may also exist an infinite number of parallel universes containing identical copies yet completely different versions thereof due certain changes made within each individual instance leading them down separate paths until they eventually become two entirely dissimilar entities once more separated by space & matter\n",
       "In addition there might possibly multiple dimensions beyond those currently known about including ones consisting solely energy instead physical mass"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Whats the meaning of life?\"\n",
    "display(Markdown(model_handler.generate_text(prompt=prompt, max_new_tokens=250)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Performance Benchmarking Metrics**\n",
    "\n",
    "---\n",
    "\n",
    "<small>\n",
    "\n",
    "#### 1. **Latency**\n",
    "\n",
    "Measures time delays during generation.\n",
    "\n",
    "- **First-Token Latency (FTL):**  \n",
    "  Time to generate the **first token**.  \n",
    "  $$ \\text{FTL} = t_{\\text{first token}} - t_{\\text{start}} $$\n",
    "\n",
    "- **Average-Token Latency (ATL):**  \n",
    "  Average time per token after the first one.  \n",
    "  $$ \\text{ATL} = \\frac{T_{\\text{total}} - \\text{FTL}}{N_{\\text{tokens}} - 1} $$\n",
    "\n",
    "- **Generation Latency (GL):**  \n",
    "  Total time to generate the **full output**.  \n",
    "  $$ \\text{GL} = t_{\\text{end}} - t_{\\text{start}} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Throughput**\n",
    "\n",
    "Measures the output rate of the model.\n",
    "\n",
    "- **Tokens per Second (TPS):**  \n",
    "  Number of tokens generated per second.  \n",
    "  $$ \\text{TPS} = \\frac{N_{\\text{tokens}}}{\\text{GL}} $$\n",
    "\n",
    "- **Sentences per Second (SPS):**  \n",
    "  Number of sentences generated per second.  \n",
    "  $$ \\text{SPS} = \\frac{N_{\\text{sentences}}}{\\text{GL}} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Storage**\n",
    "\n",
    "Provides insights into memory usage during inference.\n",
    "\n",
    "- **Model Size:**  \n",
    "  The total disk space used by the pre-trained model.\n",
    "\n",
    "- **KV-Cache Size:**  \n",
    "  Memory used for key-value caching during generation.\n",
    "\n",
    "- **Memory Usage (Model + KV-Cache):**  \n",
    "  $$ \\text{Memory}_{\\text{total}} = \\text{Model Memory} + \\text{KV-Cache Memory} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Energy**\n",
    "\n",
    "Evaluates energy efficiency during generation.\n",
    "\n",
    "- **Energy Consumption per Token:**  \n",
    "  $$ E_{\\text{token}} = \\frac{E_{\\text{total}}}{N_{\\text{tokens}}} $$\n",
    "\n",
    "- **Energy Consumption per Sentence:**  \n",
    "  $$ E_{\\text{sentence}} = \\frac{E_{\\text{total}}}{N_{\\text{sentences}}} $$\n",
    "\n",
    "- **Energy Consumption per Second:**  \n",
    "  $$ E_{\\text{sec}} = P_{\\text{avg}} \\times t_{\\text{generation}} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Quality (Summarization)**\n",
    "\n",
    "Measures the quality of model-generated text, especially for summarization tasks.\n",
    "\n",
    "- **ROUGE Score:**  \n",
    "  Measures the overlap between generated and reference summaries.\n",
    "\n",
    "- **Perplexity:**  \n",
    "  Indicates how well the model predicts a sequence. Lower is better.  \n",
    "  $$ \\text{Perplexity} = e^{\\text{Cross-Entropy Loss}} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Summary of Key Metrics**\n",
    "\n",
    "| Metric                   | Unit             | Formula/Definition                                  |\n",
    "|--------------------------|-------------------|-----------------------------------------------------|\n",
    "| **Latency**                  |                |                                                     |\n",
    "| First-Token Latency      | seconds (s)       | $$ \\text{FTL} $$                                    |\n",
    "| Average-Token Latency    | seconds/token     | $$ \\text{ATL} $$                                    |\n",
    "| Generation Latency       | seconds (s)       | $$ \\text{GL} $$                                     |\n",
    "| **Throughput**               |                  |                                                     |\n",
    "| Tokens per Second (TPS)  | tokens/second     | $$ \\frac{N_{\\text{tokens}}}{\\text{GL}} $$            |\n",
    "| Sentences per Second     | sentences/second  | $$ \\frac{N_{\\text{sentences}}}{\\text{GL}} $$         |\n",
    "| **Storage**                  |                  |                                                     |\n",
    "| Model Size               | MB/GB             | Disk space used by the pre-trained model.           |\n",
    "| KV-Cache Size            | MB/GB             | Memory used for key-value caching during generation.|\n",
    "| Memory Usage             | MB/GB             | $$ \\text{Model Memory} + \\text{KV-Cache Memory} $$   |\n",
    "| **Energy**                   |                  |                                                     |\n",
    "| Energy per Token         | Joules/token      | $$ \\frac{E_{\\text{total}}}{N_{\\text{tokens}}} $$     |\n",
    "| Energy per Sentence      | Joules/sentence   | $$ \\frac{E_{\\text{total}}}{N_{\\text{sentences}}} $$  |\n",
    "| Energy per Second        | Watts (W)         | $$ P_{\\text{avg}} \\times t_{\\text{generation}} $$    |\n",
    "| **Quality**                  |                  |                                                     |\n",
    "| ROUGE Score              |                  | Measures the overlap between generated and reference summaries. |\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "from LLM_engines.llama_model_handler import LlamaModelHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful.\n",
      "Loading model 'meta-llama/Llama-3.1-8b' with precision 'fp16'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfd70f164504bdebba7a0e899433554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda:0\n",
      "GPU: NVIDIA L4\n",
      "Model dtype: torch.float16\n",
      "Model loading time: 11.4783 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "\n",
    "precision = [\"int8\", \"fp16\"]\n",
    "\n",
    "model_handler = LlamaModelHandler(\"meta-llama/Llama-3.1-8b\", precision=precision[1])\n",
    "model, tokenizer = model_handler.get_model_and_tokenizer()\n",
    "\n",
    "# Initialize benchmark\n",
    "benchmark = ModelBenchmark(model=model, tokenizer=tokenizer, max_tokens=128)\n",
    "\n",
    "# Run benchmark\n",
    "test_prompts = [\n",
    "    \"Explain the significance of transformer models in NLP.\",\n",
    "    \"What are the main benefits of renewable energy?\",\n",
    "    \"How does the immune system work?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is the best way to cook a steak?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt (length 54 characters)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt (length 47 characters)...\n",
      "Evaluating prompt (length 32 characters)...\n",
      "Evaluating prompt (length 30 characters)...\n",
      "Evaluating prompt (length 37 characters)...\n"
     ]
    }
   ],
   "source": [
    "benchmark_results_fp16 = benchmark.benchmark(test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt Length</th>\n",
       "      <th>FTL (s)</th>\n",
       "      <th>ATL (s)</th>\n",
       "      <th>GL (s)</th>\n",
       "      <th>TPS (tokens/s)</th>\n",
       "      <th>SPS (sentences/s)</th>\n",
       "      <th>Memory Usage (MB)</th>\n",
       "      <th>Model Size (MB)</th>\n",
       "      <th>Total Energy (Wh)</th>\n",
       "      <th>Energy per Token (J/token)</th>\n",
       "      <th>Energy per Sentence (J/sentence)</th>\n",
       "      <th>Energy per Second (W)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>8.9889</td>\n",
       "      <td>15.57</td>\n",
       "      <td>0.44</td>\n",
       "      <td>16190.06</td>\n",
       "      <td>15325.276917</td>\n",
       "      <td>0.160639</td>\n",
       "      <td>4.131985</td>\n",
       "      <td>146.215909</td>\n",
       "      <td>64.335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>0.0709</td>\n",
       "      <td>0.0709</td>\n",
       "      <td>8.1539</td>\n",
       "      <td>16.92</td>\n",
       "      <td>0.49</td>\n",
       "      <td>16190.06</td>\n",
       "      <td>15325.276917</td>\n",
       "      <td>0.153651</td>\n",
       "      <td>4.009338</td>\n",
       "      <td>138.444898</td>\n",
       "      <td>67.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>0.0680</td>\n",
       "      <td>0.0680</td>\n",
       "      <td>8.1625</td>\n",
       "      <td>16.66</td>\n",
       "      <td>0.74</td>\n",
       "      <td>16190.06</td>\n",
       "      <td>15325.276917</td>\n",
       "      <td>0.148260</td>\n",
       "      <td>3.924910</td>\n",
       "      <td>88.363514</td>\n",
       "      <td>65.389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>8.1692</td>\n",
       "      <td>16.65</td>\n",
       "      <td>0.73</td>\n",
       "      <td>16190.06</td>\n",
       "      <td>15325.276917</td>\n",
       "      <td>0.129554</td>\n",
       "      <td>3.428949</td>\n",
       "      <td>78.208219</td>\n",
       "      <td>57.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>0.0746</td>\n",
       "      <td>0.0746</td>\n",
       "      <td>8.2051</td>\n",
       "      <td>16.94</td>\n",
       "      <td>0.61</td>\n",
       "      <td>16190.06</td>\n",
       "      <td>15325.276917</td>\n",
       "      <td>0.146933</td>\n",
       "      <td>3.805608</td>\n",
       "      <td>105.683607</td>\n",
       "      <td>64.467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prompt Length  FTL (s)  ATL (s)  GL (s)  TPS (tokens/s)  SPS (sentences/s)  \\\n",
       "0             54   0.0864   0.0864  8.9889           15.57               0.44   \n",
       "1             47   0.0709   0.0709  8.1539           16.92               0.49   \n",
       "2             32   0.0680   0.0680  8.1625           16.66               0.74   \n",
       "3             30   0.0763   0.0763  8.1692           16.65               0.73   \n",
       "4             37   0.0746   0.0746  8.2051           16.94               0.61   \n",
       "\n",
       "   Memory Usage (MB)  Model Size (MB)  Total Energy (Wh)  \\\n",
       "0           16190.06     15325.276917           0.160639   \n",
       "1           16190.06     15325.276917           0.153651   \n",
       "2           16190.06     15325.276917           0.148260   \n",
       "3           16190.06     15325.276917           0.129554   \n",
       "4           16190.06     15325.276917           0.146933   \n",
       "\n",
       "   Energy per Token (J/token)  Energy per Sentence (J/sentence)  \\\n",
       "0                    4.131985                        146.215909   \n",
       "1                    4.009338                        138.444898   \n",
       "2                    3.924910                         88.363514   \n",
       "3                    3.428949                         78.208219   \n",
       "4                    3.805608                        105.683607   \n",
       "\n",
       "   Energy per Second (W)  \n",
       "0                 64.335  \n",
       "1                 67.838  \n",
       "2                 65.389  \n",
       "3                 57.092  \n",
       "4                 64.467  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_results_fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vLLM\n",
    "\n",
    "#### Model Initialization Parameters in vLLM\n",
    "\n",
    "<small>\n",
    "\n",
    "| **Parameter** | **Type** | **Possible Values** | **Default** | **Effect** | **Optimization Strategy** |\n",
    "|--------------|---------|-----------------|------------|----------------|--------------------------|\n",
    "| **`model`** | `str` | Model name from Hugging Face | Required | Loads the model (e.g., `\"meta-llama/Llama-3.1-8b\"`) | Choose a model suited to your VRAM (use **smaller models or quantized models** for limited memory). |\n",
    "| **`tokenizer`** | `str \\| None` | `None`, `\"auto\"`, custom tokenizer path | `None` | Defines the tokenizer to use. | **Set to `None` to use the default model tokenizer.** Custom tokenizer paths are useful for fine-tuned models. |\n",
    "| **`tokenizer_mode`** | `str` | `\"auto\"`, `\"slow\"`, `\"fast\"` | `\"auto\"` | Defines tokenizer behavior. | **Use `\"auto\"` (default) for best performance**. `\"slow\"` is only needed if `\"fast\"` is buggy. |\n",
    "| **`skip_tokenizer_init`** | `bool` | `True`, `False` | `False` | Skips tokenizer loading. | **Set to `True` if using an external tokenizer** to reduce memory usage. |\n",
    "| **`trust_remote_code`** | `bool` | `True`, `False` | `False` | Enables loading remote model code. | **Enable (`True`) only for custom models that require additional scripts**. |\n",
    "| **`allowed_local_media_path`** | `str` | Any path | `\"\"` | Defines a local path for media files. | Used for **multimodal models** (not needed for LLaMA). |\n",
    "| **`tensor_parallel_size`** | `int` | `1` or more | `1` | Number of GPUs for inference. | **Set `2+` for multi-GPU setups (A100, H100, multiple RTX 4090s).** |\n",
    "| **`dtype`** | `str` | `\"float16\"`, `\"bfloat16\"`, `\"float32\"` | `\"auto\"` | Model precision (affects memory & speed). | **Use `\"bfloat16\"` for A100/H100, `\"float16\"` for all other GPUs**. `\"float32\"` is **not recommended** due to memory cost. |\n",
    "| **`quantization`** | `str \\| None` | `\"awq\"`, `\"gptq\"`, `None` | `None` | Enables quantization for lower memory usage. | **Use `\"awq\"` to halve VRAM usage with minimal performance drop**. `\"gptq\"` is useful for **8GB GPUs**. |\n",
    "| **`revision`** | `str \\| None` | Hugging Face model commit hash | `None` | Specifies a model version. | **Use a specific revision if testing different versions** of a model. |\n",
    "| **`tokenizer_revision`** | `str \\| None` | Hugging Face tokenizer commit hash | `None` | Specifies tokenizer version. | Usually **not needed**, but useful if a tokenizer update affects behavior. |\n",
    "| **`seed`** | `int` | Any integer | `0` | Sets random seed for reproducibility. | **Set to a fixed value (`42`) if you need consistent results.** |\n",
    "\n",
    "\n",
    "#### Recommended Settings for Different Hardware\n",
    "\n",
    "| **Use Case** | **Model** | **dtype** | **tensor_parallel_size** | **quantization** | **Optimization Notes** |\n",
    "|-------------|---------|--------|-------------------|--------------|--------------------|\n",
    "| **Best Performance (High VRAM, A100/H100, 80GB VRAM)** | `\"meta-llama/Llama-3.1-8b\"` | `\"bfloat16\"` | `2+` | `None` | Multi-GPU setup, fastest inference. |\n",
    "| **Optimized for Consumer GPUs (RTX 3090/4090, 24GB VRAM)** | `\"meta-llama/Llama-3.1-8b\"` | `\"float16\"` | `1-2` | `\"awq\"` | Halves VRAM usage with minimal loss in accuracy. |\n",
    "| **Low VRAM Setup (RTX 3060, 12GB VRAM)** | `\"meta-llama/Llama-3.1-8b\"` | `\"float16\"` | `1` | `\"gptq\"` | Optimized for **low-memory GPUs**, reduces precision slightly. |\n",
    "| **CPU-Only (Mac M1/M2, Non-GPU systems)** | `\"meta-llama/Llama-3.1-8b\"` | `\"float32\"` | `1` | `\"gptq\"` | `\"float32\"` is needed since Mac doesn’t support `\"float16\"`. |\n",
    "\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-09 12:23:56 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 03-09 12:23:57 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-09 12:24:05 config.py:549] This model supports multiple tasks: {'embed', 'classify', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 03-09 12:24:05 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='meta-llama/Llama-3.1-8b', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=24576, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.1-8b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-09 12:24:08 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-09 12:24:09 model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8b...\n",
      "INFO 03-09 12:24:10 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a52475e95a74b52bb60f840a7da0812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-09 12:24:18 model_runner.py:1115] Loading model weights took 14.9888 GB\n",
      "INFO 03-09 12:24:25 worker.py:267] Memory profiling takes 7.08 seconds\n",
      "INFO 03-09 12:24:25 worker.py:267] the current vLLM instance can use total_gpu_memory (22.05GiB) x gpu_memory_utilization (0.99) = 21.82GiB\n",
      "INFO 03-09 12:24:25 worker.py:267] model weights take 14.99GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 2.54GiB; the rest of the memory reserved for KV Cache is 4.25GiB.\n",
      "INFO 03-09 12:24:26 executor_base.py:111] # cuda blocks: 2176, # CPU blocks: 2048\n",
      "INFO 03-09 12:24:26 executor_base.py:116] Maximum concurrency for 24576 tokens per request: 1.42x\n",
      "INFO 03-09 12:24:29 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:27<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-09 12:24:57 model_runner.py:1562] Graph capturing finished in 28 secs, took 0.26 GiB\n",
      "INFO 03-09 12:24:57 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 38.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "from LLM_engines.vLLM_model_handler import vLLMHandler\n",
    "\n",
    "vllm_handler = vLLMHandler(\n",
    "    model_name=\"meta-llama/Llama-3.1-8b\", \n",
    "    dtype=\"float16\", \n",
    "    gpu_util=0.99, \n",
    "    quantization=None, \n",
    "    seed=42)\n",
    "\n",
    "benchmark_vllm = ModelBenchmark(vllm_handler, backend=\"vllm\")\n",
    "\n",
    "# Run benchmark\n",
    "test_prompts = [\n",
    "    \"Explain the significance of transformer models in NLP.\",\n",
    "    \"What are the main benefits of renewable energy?\",\n",
    "    \"How does the immune system work?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is the best way to cook a steak?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt (length 54 characters)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.07s/it, est. speed input: 0.39 toks/s, output: 16.48 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.09s/it, est. speed input: 0.39 toks/s, output: 16.47 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt (length 47 characters)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.73s/it, est. speed input: 0.44 toks/s, output: 16.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.11s/it, est. speed input: 0.32 toks/s, output: 16.46 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt (length 32 characters)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.11s/it, est. speed input: 0.26 toks/s, output: 16.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.08s/it, est. speed input: 0.38 toks/s, output: 16.46 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt (length 30 characters)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.11s/it, est. speed input: 0.26 toks/s, output: 16.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.19s/it, est. speed input: 0.26 toks/s, output: 16.42 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt (length 37 characters)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.11s/it, est. speed input: 0.35 toks/s, output: 16.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.11s/it, est. speed input: 0.35 toks/s, output: 16.46 toks/s]\n"
     ]
    }
   ],
   "source": [
    "vllm_results = benchmark_vllm.benchmark(test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt Length</th>\n",
       "      <th>FTL (s)</th>\n",
       "      <th>ATL (s)</th>\n",
       "      <th>GL (s)</th>\n",
       "      <th>TPS (tokens/s)</th>\n",
       "      <th>SPS (sentences/s)</th>\n",
       "      <th>Memory Usage (MB)</th>\n",
       "      <th>Model Size (MB)</th>\n",
       "      <th>KV-Cache Size Estimation (MB)</th>\n",
       "      <th>Total Energy (Wh)</th>\n",
       "      <th>Energy per Token (J/token)</th>\n",
       "      <th>Energy per Sentence (J/sentence)</th>\n",
       "      <th>Energy per Second (W)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>0.0728</td>\n",
       "      <td>0.0728</td>\n",
       "      <td>31.0805</td>\n",
       "      <td>14.22</td>\n",
       "      <td>0.68</td>\n",
       "      <td>20696.06</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.621860</td>\n",
       "      <td>5.065331</td>\n",
       "      <td>105.925000</td>\n",
       "      <td>72.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>22.7282</td>\n",
       "      <td>18.44</td>\n",
       "      <td>1.28</td>\n",
       "      <td>20696.06</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.455038</td>\n",
       "      <td>3.908623</td>\n",
       "      <td>56.308594</td>\n",
       "      <td>72.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>0.0715</td>\n",
       "      <td>0.0715</td>\n",
       "      <td>31.1140</td>\n",
       "      <td>9.51</td>\n",
       "      <td>0.67</td>\n",
       "      <td>20696.06</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.621554</td>\n",
       "      <td>7.562145</td>\n",
       "      <td>107.337313</td>\n",
       "      <td>71.916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0784</td>\n",
       "      <td>0.0784</td>\n",
       "      <td>31.1178</td>\n",
       "      <td>10.09</td>\n",
       "      <td>0.80</td>\n",
       "      <td>20696.06</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.621829</td>\n",
       "      <td>7.129732</td>\n",
       "      <td>89.923750</td>\n",
       "      <td>71.939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>31.1133</td>\n",
       "      <td>11.80</td>\n",
       "      <td>1.16</td>\n",
       "      <td>20696.06</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.621013</td>\n",
       "      <td>6.089407</td>\n",
       "      <td>61.943966</td>\n",
       "      <td>71.855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prompt Length  FTL (s)  ATL (s)   GL (s)  TPS (tokens/s)  \\\n",
       "0             54   0.0728   0.0728  31.0805           14.22   \n",
       "1             47   0.0689   0.0689  22.7282           18.44   \n",
       "2             32   0.0715   0.0715  31.1140            9.51   \n",
       "3             30   0.0784   0.0784  31.1178           10.09   \n",
       "4             37   0.0691   0.0691  31.1133           11.80   \n",
       "\n",
       "   SPS (sentences/s)  Memory Usage (MB) Model Size (MB)  \\\n",
       "0               0.68           20696.06             N/A   \n",
       "1               1.28           20696.06             N/A   \n",
       "2               0.67           20696.06             N/A   \n",
       "3               0.80           20696.06             N/A   \n",
       "4               1.16           20696.06             N/A   \n",
       "\n",
       "  KV-Cache Size Estimation (MB)  Total Energy (Wh)  \\\n",
       "0                           N/A           0.621860   \n",
       "1                           N/A           0.455038   \n",
       "2                           N/A           0.621554   \n",
       "3                           N/A           0.621829   \n",
       "4                           N/A           0.621013   \n",
       "\n",
       "   Energy per Token (J/token)  Energy per Sentence (J/sentence)  \\\n",
       "0                    5.065331                        105.925000   \n",
       "1                    3.908623                         56.308594   \n",
       "2                    7.562145                        107.337313   \n",
       "3                    7.129732                         89.923750   \n",
       "4                    6.089407                         61.943966   \n",
       "\n",
       "   Energy per Second (W)  \n",
       "0                 72.029  \n",
       "1                 72.075  \n",
       "2                 71.916  \n",
       "3                 71.939  \n",
       "4                 71.855  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vllm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark\n",
    "\n",
    "Find optimal optimization frameworks for a specific:\n",
    "\n",
    "tasks: [text-to-SQL, summarization, QA, ...]\n",
    "\n",
    "scenario: [server, batch, single-stream, ...]\n",
    "\n",
    "models: [llama3.1-8B, Mixtral, ...]\n",
    "\n",
    "frameworks: [vLLM, tensorRT, ...] (trying different configurations)\n",
    "\n",
    "hardware: [L4, A100, H100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization Frameworks for LLM Inference\n",
    "\n",
    "<small>\n",
    "\n",
    "| **Framework** | **Quantization Support** | **Model Parallelism** | **Tensor / Kernel Optimizations** | **Memory Efficiency** | **Throughput & Latency** | **Financial / Energy Efficiency** |\n",
    "|--------------|--------------------------|-----------------------|-----------------------------------|-----------------------|--------------------------|-----------------------------------|\n",
    "| **vLLM** (UC Berkeley) | FP16/BF16, 8-bit, 4-bit (GPTQ, AWQ), experimental FP8 | Tensor Parallelism, Pipeline Parallelism | Custom **PagedAttention**, fused attention kernels, PyTorch fused ops | **Paged KV cache**, quantized KV cache, prefix reuse | **Continuous batching**, **speculative decoding**, auto-regressive scheduling | 2–4× throughput improvement, high GPU utilization, cost-efficient serving |\n",
    "| **NVIDIA TensorRT-LLM** | FP16/BF16, FP8 (Hopper), INT8 (SmoothQuant), 4-bit AWQ | Tensor, Pipeline, MoE Parallelism | Custom fused attention kernels, TensorRT graph optimization, kernel fusion | **Paged KV caching**, weight compression, memory-optimized inference engines | **Dynamic batching**, **speculative decoding (ReDrafter)**, automatic cache sharing | FP8/INT8 for lower power, throughput-focused, optimized GPU utilization |\n",
    "| **DeepSpeed-Inference** | INT8 (MoQ), 4-bit weight quantization | Tensor, Pipeline, **Expert Parallelism (MoE)** | Transformer kernel injection, optimized CUDA fused ops | **ZeRO-Inference** (offload KV cache to CPU/NVMe) | **Massive batch sizes**, multi-GPU scaling, ZeRO latency reduction | INT8 models ~1.7× faster, **CPU memory offloading** reduces GPU costs |\n",
    "| **Hugging Face TGI** | 8-bit (bitsandbytes), 4-bit (GPTQ, AWQ), FP8 experimental | Tensor Parallelism, Model Sharding | FlashAttention, PagedAttention, fused sampling ops | **Paged KV cache**, weight mapping via Safetensors | **Continuous batching**, **speculative decoding (Medusa)** | High throughput (up to 13× faster on long inputs), flexible multi-platform support |\n",
    "| **NVIDIA FasterTransformer** | FP16/BF16, INT8, FP8, 4-bit compression | Tensor, Pipeline Parallelism | Fused MLP layers, fused multi-head attention, optimized GEMM | **Persistent KV cache**, optimized weight storage | **Optimized CUDA scheduling**, efficient multi-GPU serving | Low-latency, high-throughput GPU inference, FP16/INT8 for energy efficiency |\n",
    "| **ONNX Runtime** (HF Optimum) | INT8 post-training quantization, FP16, BF16 | No native multi-GPU, requires manual graph partitioning | Graph optimizations, operator fusion, execution provider optimizations | **Memory-efficient graph storage**, weight pre-packing | **Batching support**, CPU multi-threading | Cost-efficient CPU/GPU inference, flexible hardware deployment |\n",
    "| **llama.cpp / GGML** | 16-bit, 8-bit, 6-bit, 5-bit, 4-bit, 3-bit, 2-bit | No native multi-GPU, but supports CPU-GPU offloading | **SIMD-optimized** (AVX, NEON), bit-packed matrix multiplication | **Memory-mapped weights**, CPU RAM paging for large models | **Single-request optimized**, multi-threaded CPU processing | Extreme low-cost inference, edge deployment, power-efficient CPU inference |\n",
    "\n",
    "</small>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
