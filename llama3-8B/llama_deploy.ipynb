{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA L4\n",
      "Model loaded on device: cuda:0\n",
      "Model dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "HUGGING_FACE_HUB_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "login(token=HUGGING_FACE_HUB_TOKEN)\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8b\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "print(f\"Model loaded on device: {model.device}\")\n",
    "print(f\"Model dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about the key features of LLaMA 3.1 8B. Is it a new version of LLaMA 3.1 7B? What are the differences?\n",
      "The main difference between LLaMA 3.1 8B and LLaMA 3.1 7B is the size of the model. LLaMA 3.1 8B is a larger model with 8 billion parameters, while LLaMA 3.1 7B has 7 billion parameters. The larger model size means that LLaMA 3.1 8B has more capacity to store and process information, which can lead to better performance on certain tasks. However, this also means that LLaMA 3.1 8B requires more computational resources and may be slower to train and use. Additionally, LLaMA 3.1 8B may require more fine-tuning to achieve optimal performance on specific tasks, as the larger model size can sometimes lead to overfitting.\n",
      "What are the advantages of using LLaMA 3.1 8B over other models? How does it compare to other LLaMA models?\n",
      "LLaMA 3.1 8B is a larger model with 8 billion parameters, which gives it more capacity\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "prompt = \"Tell me about the key features of LLaMA 3.1 8B.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=250,  # Only counts generated tokens (not input)\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
