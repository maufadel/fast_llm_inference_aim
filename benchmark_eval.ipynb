{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Performance Benchmarking Metrics**\n",
    "\n",
    "---\n",
    "\n",
    "<small>\n",
    "\n",
    "#### 1. **Latency**\n",
    "\n",
    "Measures time delays during generation.\n",
    "\n",
    "- **First-Token Latency (FTL):**  \n",
    "  Time to generate the **first token**.  \n",
    "  $$ \\text{FTL} = t_{\\text{first token}} - t_{\\text{start}} $$\n",
    "\n",
    "- **Average-Token Latency (ATL):**  \n",
    "  Average time per token after the first one.  \n",
    "  $$ \\text{ATL} = \\frac{T_{\\text{total}} - \\text{FTL}}{N_{\\text{tokens}} - 1} $$\n",
    "\n",
    "- **Generation Latency (GL):**  \n",
    "  Total time to generate the **full output**.  \n",
    "  $$ \\text{GL} = t_{\\text{end}} - t_{\\text{start}} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Throughput**\n",
    "\n",
    "Measures the output rate of the model.\n",
    "\n",
    "- **Tokens per Second (TPS):**  \n",
    "  Number of tokens generated per second.  \n",
    "  $$ \\text{TPS} = \\frac{N_{\\text{tokens}}}{\\text{GL}} $$\n",
    "\n",
    "- **Sentences per Second (SPS):**  \n",
    "  Number of sentences generated per second.  \n",
    "  $$ \\text{SPS} = \\frac{N_{\\text{sentences}}}{\\text{GL}} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Storage**\n",
    "\n",
    "Provides insights into memory usage during inference.\n",
    "\n",
    "- **Model Size:**  \n",
    "  The total disk space used by the pre-trained model.\n",
    "\n",
    "- **KV-Cache Size:**  \n",
    "  Memory used for key-value caching during generation.\n",
    "\n",
    "- **Memory Usage (Model + KV-Cache):**  \n",
    "  $$ \\text{Memory}_{\\text{total}} = \\text{Model Memory} + \\text{KV-Cache Memory} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Energy**\n",
    "\n",
    "Evaluates energy efficiency during generation.\n",
    "\n",
    "- **Energy Consumption per Token:**  \n",
    "  $$ E_{\\text{token}} = \\frac{E_{\\text{total}}}{N_{\\text{tokens}}} $$\n",
    "\n",
    "- **Energy Consumption per Sentence:**  \n",
    "  $$ E_{\\text{sentence}} = \\frac{E_{\\text{total}}}{N_{\\text{sentences}}} $$\n",
    "\n",
    "- **Energy Consumption per Second:**  \n",
    "  $$ E_{\\text{sec}} = P_{\\text{avg}} \\times t_{\\text{generation}} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Quality (Summarization)**\n",
    "\n",
    "Measures the quality of model-generated text, especially for summarization tasks.\n",
    "\n",
    "- **ROUGE Score:**  \n",
    "  Measures the overlap between generated and reference summaries.\n",
    "\n",
    "- **Perplexity:**  \n",
    "  Indicates how well the model predicts a sequence. Lower is better.  \n",
    "  $$ \\text{Perplexity} = e^{\\text{Cross-Entropy Loss}} $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Summary of Key Metrics**\n",
    "\n",
    "| Metric                   | Unit             | Formula/Definition                                  |\n",
    "|--------------------------|-------------------|-----------------------------------------------------|\n",
    "| **Latency**                  |                |                                                     |\n",
    "| First-Token Latency      | seconds (s)       | $$ \\text{FTL} $$                                    |\n",
    "| Average-Token Latency    | seconds/token     | $$ \\text{ATL} $$                                    |\n",
    "| Generation Latency       | seconds (s)       | $$ \\text{GL} $$                                     |\n",
    "| **Throughput**               |                  |                                                     |\n",
    "| Tokens per Second (TPS)  | tokens/second     | $$ \\frac{N_{\\text{tokens}}}{\\text{GL}} $$            |\n",
    "| Sentences per Second     | sentences/second  | $$ \\frac{N_{\\text{sentences}}}{\\text{GL}} $$         |\n",
    "| **Storage**                  |                  |                                                     |\n",
    "| Model Size               | MB/GB             | Disk space used by the pre-trained model.           |\n",
    "| KV-Cache Size            | MB/GB             | Memory used for key-value caching during generation.|\n",
    "| Memory Usage             | MB/GB             | $$ \\text{Model Memory} + \\text{KV-Cache Memory} $$   |\n",
    "| **Energy**                   |                  |                                                     |\n",
    "| Energy per Token         | Joules/token      | $$ \\frac{E_{\\text{total}}}{N_{\\text{tokens}}} $$     |\n",
    "| Energy per Sentence      | Joules/sentence   | $$ \\frac{E_{\\text{total}}}{N_{\\text{sentences}}} $$  |\n",
    "| Energy per Second        | Watts (W)         | $$ P_{\\text{avg}} \\times t_{\\text{generation}} $$    |\n",
    "| **Quality**                  |                  |                                                     |\n",
    "| ROUGE Score              |                  | Measures the overlap between generated and reference summaries. |\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "from LLM_engines.llama_model_handler import LlamaModelHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful.\n",
      "Loading model 'meta-llama/Llama-3.1-8b' with precision 'int8'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afb08fc54b448558574ada57240e301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda:0\n",
      "GPU: NVIDIA L4\n",
      "Model dtype: torch.float16\n",
      "Model loading time: 11.4986 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "\n",
    "precision = [\"int8\", \"fp16\"]\n",
    "\n",
    "model_handler = LlamaModelHandler(\"meta-llama/Llama-3.1-8b\", precision=precision[0])\n",
    "model, tokenizer = model_handler.get_model_and_tokenizer()\n",
    "\n",
    "# Initialize benchmark\n",
    "benchmark = ModelBenchmark(model=model, tokenizer=tokenizer, max_tokens=256)\n",
    "\n",
    "# Run benchmark\n",
    "test_prompts = [\n",
    "    \"Explain the significance of transformer models in NLP.\",\n",
    "    \"What are the main benefits of renewable energy?\",\n",
    "    \"How does the immune system work?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is the best way to cook a steak?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt (length 54 characters)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Explain the significance of transformer models in NLP. Discuss the advantages of using transformer models over traditional word embedding methods.\\nTransformer models have revolutionized the field of natural language processing (NLP) by enabling better understanding and generation of language. These models have become the standard for many NLP tasks, such as machine translation, text summarization, and question answering.\\nThe transformer model was first introduced in 2017 by Vaswani et al. in their paper \\u201cAttention is all you need.\\u201d The transformer model is a type of neural network architecture that uses attention mechanisms to capture long-range dependencies in the input data. Unlike traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs), which process data sequentially, transformer models can process data in parallel, making them more efficient and effective for processing large amounts of data.\\nOne of the key advantages of transformer models over traditional word embedding methods is their ability to capture contextual information. Word embeddings, such as word2vec or GloVe, represent words as vectors in a high-dimensional space, but they do not capture the context in which the words are used. This can lead to issues such as word ambiguity and word polysemy, where a word can have multiple meanings depending on the context.\\nTransformer models, on the other hand, use attention mechanisms to capture the\"\n",
      "Evaluating prompt (length 47 characters)...\n",
      "\"What are the main benefits of renewable energy? What are the main benefits of renewable energy?\\nRenewable energy is a type of energy that comes from natural sources such as the sun, wind, water, and geothermal heat. Renewable energy sources are virtually inexhaustible and do not produce greenhouse gases or other pollutants when used to generate electricity. They also have the potential to create jobs and boost local economies.\\nThere are many benefits of renewable energy. One of the main benefits is that it is a clean and sustainable source of energy. Renewable energy does not produce greenhouse gases or other pollutants, and it can help to reduce our dependence on fossil fuels. Additionally, renewable energy can create jobs and boost local economies.\\nWhat are the main benefits of renewable energy? 1\\nAnother benefit of renewable energy is that it can help to reduce our dependence on foreign oil. The United States currently imports about 60% of its oil from other countries, and this dependence can be costly and unstable. By investing in renewable energy sources, we can reduce our reliance on foreign oil and make our economy more stable and secure.\\nRenewable energy is also a more reliable source of energy than fossil fuels. Fossil fuels are a finite resource, and their supply is constantly decreasing. In contrast, renewable energy sources are virtually inexhaustible. This\"\n",
      "Evaluating prompt (length 32 characters)...\n",
      "\"How does the immune system work? What is it? What is its function? What is the difference between an allergy and an autoimmune disease? What are the types of immunity? What are the different cells that make up the immune system? What are the different organs that make up the immune system? How does the immune system protect the body? What is the role of the immune system? What are the different types of immune cells? What is the role of the immune system in the body? What are the different types of immune responses? What are the different types of immune system disorders? What are the different types of immune system diseases? What are the different types of immune system infections? What are the different types of immune system problems? What are the different types of immune system disorders? What are the different types of immune system diseases? What are the different types of immune system infections? What are the different types of immune system problems? What are the different types of immune system disorders? What are the different types of immune system diseases? What are the different types of immune system infections? What are the different types of immune system problems? What are the different types of immune system disorders? What are the different types of immune system diseases? What are the different types of immune system infections? What are the different types of\"\n",
      "Evaluating prompt (length 30 characters)...\n",
      "\"What is the capital of France? What is the capital of the United States? These are two very common questions that are asked on a daily basis. If you are an American, you probably know the answer to the first question, but you may not know the answer to the second. In this article, we will discuss the differences between the two countries\\u2019 capitals and how to identify them.\\nThe capital of France is Paris, which is located in the country\\u2019s northwest. The city is home to the Eiffel Tower, the Louvre, and Notre Dame Cathedral. It is also home to many of the world\\u2019s most famous museums and art galleries.\\nThe capital of the United States is Washington, D.C., which is located in the country\\u2019s northeast. The city is home to the White House, the Capitol Building, and the Washington Monument. It is also home to many of the world\\u2019s most important government buildings and monuments.\\nSo, how can you tell the difference between the two capitals? The easiest way is to look at the flags. The French flag is blue, white, and red, while the American flag is red, white, and blue. Another way to tell the difference is to look at the buildings. The Eiffel Tower is a famous landmark in Paris, while the Washington Monument is\"\n",
      "Evaluating prompt (length 37 characters)...\n",
      "\"What is the best way to cook a steak? It depends on what kind of steak you\\u2019re cooking. For the most part, it\\u2019s best to cook a steak at a high temperature, but not too high. If you cook a steak at too high a temperature, the meat will become tough and chewy. If you cook a steak at too low a temperature, the meat will become dry and tough. The best way to cook a steak is to cook it at a medium-high temperature, which is about 400 degrees Fahrenheit. This will give you a nice, juicy steak that is cooked to perfection.\\nHow do you cook a steak on the stove without a cast iron skillet?\\nA cast iron skillet is not necessary to cook a steak on the stove. You can use any skillet that is oven-safe and can withstand high heat. If you don\\u2019t have a cast iron skillet, you can use a stainless steel skillet or a non-stick skillet. Just make sure that the skillet is oven-safe and can withstand high heat.\\nHow do you cook a steak on the stove without oil?\\nThere are a few ways to cook a steak on the stove without oil. One way is to use a non-stick pan. Another way is to use a cast iron pan. You can also use a grill pan. Just make sure that\"\n"
     ]
    }
   ],
   "source": [
    "benchmark_results_fp16 = benchmark.benchmark(test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt Length</th>\n",
       "      <th>FTL (s)</th>\n",
       "      <th>ATL (s)</th>\n",
       "      <th>GL (s)</th>\n",
       "      <th>TPS (tokens/s)</th>\n",
       "      <th>SPS (sentences/s)</th>\n",
       "      <th>Memory Usage (MB)</th>\n",
       "      <th>Model Size (MB)</th>\n",
       "      <th>KV-Cache Size Estimation (MB)</th>\n",
       "      <th>Total Energy (Wh)</th>\n",
       "      <th>Energy per Token (J/token)</th>\n",
       "      <th>Energy per Sentence (J/sentence)</th>\n",
       "      <th>Energy per Second (W)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>0.0942</td>\n",
       "      <td>0.0942</td>\n",
       "      <td>25.2383</td>\n",
       "      <td>10.62</td>\n",
       "      <td>0.48</td>\n",
       "      <td>9488.06</td>\n",
       "      <td>15325.276917</td>\n",
       "      <td>-5837.216917</td>\n",
       "      <td>0.358685</td>\n",
       "      <td>4.818156</td>\n",
       "      <td>107.605489</td>\n",
       "      <td>51.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>0.0940</td>\n",
       "      <td>0.0940</td>\n",
       "      <td>24.9938</td>\n",
       "      <td>10.64</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9488.06</td>\n",
       "      <td>15325.276917</td>\n",
       "      <td>-5837.216917</td>\n",
       "      <td>0.302216</td>\n",
       "      <td>4.090146</td>\n",
       "      <td>77.712772</td>\n",
       "      <td>43.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>0.0943</td>\n",
       "      <td>0.0943</td>\n",
       "      <td>24.8984</td>\n",
       "      <td>10.60</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9488.06</td>\n",
       "      <td>15325.276917</td>\n",
       "      <td>-5837.216917</td>\n",
       "      <td>0.374168</td>\n",
       "      <td>5.102287</td>\n",
       "      <td>1347.003831</td>\n",
       "      <td>54.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0889</td>\n",
       "      <td>0.0889</td>\n",
       "      <td>23.4603</td>\n",
       "      <td>11.25</td>\n",
       "      <td>0.64</td>\n",
       "      <td>9488.06</td>\n",
       "      <td>15325.276917</td>\n",
       "      <td>-5837.216917</td>\n",
       "      <td>0.342462</td>\n",
       "      <td>4.669932</td>\n",
       "      <td>82.190798</td>\n",
       "      <td>52.551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>23.3123</td>\n",
       "      <td>11.45</td>\n",
       "      <td>0.64</td>\n",
       "      <td>9488.06</td>\n",
       "      <td>15325.276917</td>\n",
       "      <td>-5837.216917</td>\n",
       "      <td>0.301292</td>\n",
       "      <td>4.062363</td>\n",
       "      <td>72.310065</td>\n",
       "      <td>46.527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prompt Length  FTL (s)  ATL (s)   GL (s)  TPS (tokens/s)  \\\n",
       "0             54   0.0942   0.0942  25.2383           10.62   \n",
       "1             47   0.0940   0.0940  24.9938           10.64   \n",
       "2             32   0.0943   0.0943  24.8984           10.60   \n",
       "3             30   0.0889   0.0889  23.4603           11.25   \n",
       "4             37   0.0873   0.0873  23.3123           11.45   \n",
       "\n",
       "   SPS (sentences/s)  Memory Usage (MB)  Model Size (MB)  \\\n",
       "0               0.48            9488.06     15325.276917   \n",
       "1               0.56            9488.06     15325.276917   \n",
       "2               0.04            9488.06     15325.276917   \n",
       "3               0.64            9488.06     15325.276917   \n",
       "4               0.64            9488.06     15325.276917   \n",
       "\n",
       "   KV-Cache Size Estimation (MB)  Total Energy (Wh)  \\\n",
       "0                   -5837.216917           0.358685   \n",
       "1                   -5837.216917           0.302216   \n",
       "2                   -5837.216917           0.374168   \n",
       "3                   -5837.216917           0.342462   \n",
       "4                   -5837.216917           0.301292   \n",
       "\n",
       "   Energy per Token (J/token)  Energy per Sentence (J/sentence)  \\\n",
       "0                    4.818156                        107.605489   \n",
       "1                    4.090146                         77.712772   \n",
       "2                    5.102287                       1347.003831   \n",
       "3                    4.669932                         82.190798   \n",
       "4                    4.062363                         72.310065   \n",
       "\n",
       "   Energy per Second (W)  \n",
       "0                 51.163  \n",
       "1                 43.530  \n",
       "2                 54.100  \n",
       "3                 52.551  \n",
       "4                 46.527  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_results_fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vLLM\n",
    "\n",
    "#### Model Initialization Parameters in vLLM\n",
    "\n",
    "<small>\n",
    "\n",
    "| **Parameter** | **Type** | **Possible Values** | **Default** | **Effect** | **Optimization Strategy** |\n",
    "|--------------|---------|-----------------|------------|----------------|--------------------------|\n",
    "| **`model`** | `str` | Model name from Hugging Face | Required | Loads the model (e.g., `\"meta-llama/Llama-3.1-8b\"`) | Choose a model suited to your VRAM (use **smaller models or quantized models** for limited memory). |\n",
    "| **`tokenizer`** | `str \\| None` | `None`, `\"auto\"`, custom tokenizer path | `None` | Defines the tokenizer to use. | **Set to `None` to use the default model tokenizer.** Custom tokenizer paths are useful for fine-tuned models. |\n",
    "| **`tokenizer_mode`** | `str` | `\"auto\"`, `\"slow\"`, `\"fast\"` | `\"auto\"` | Defines tokenizer behavior. | **Use `\"auto\"` (default) for best performance**. `\"slow\"` is only needed if `\"fast\"` is buggy. |\n",
    "| **`skip_tokenizer_init`** | `bool` | `True`, `False` | `False` | Skips tokenizer loading. | **Set to `True` if using an external tokenizer** to reduce memory usage. |\n",
    "| **`trust_remote_code`** | `bool` | `True`, `False` | `False` | Enables loading remote model code. | **Enable (`True`) only for custom models that require additional scripts**. |\n",
    "| **`allowed_local_media_path`** | `str` | Any path | `\"\"` | Defines a local path for media files. | Used for **multimodal models** (not needed for LLaMA). |\n",
    "| **`tensor_parallel_size`** | `int` | `1` or more | `1` | Number of GPUs for inference. | **Set `2+` for multi-GPU setups (A100, H100, multiple RTX 4090s).** |\n",
    "| **`dtype`** | `str` | `\"float16\"`, `\"bfloat16\"`, `\"float32\"` | `\"auto\"` | Model precision (affects memory & speed). | **Use `\"bfloat16\"` for A100/H100, `\"float16\"` for all other GPUs**. `\"float32\"` is **not recommended** due to memory cost. |\n",
    "| **`quantization`** | `str \\| None` | `\"awq\"`, `\"gptq\"`, `None` | `None` | Enables quantization for lower memory usage. | **Use `\"awq\"` to halve VRAM usage with minimal performance drop**. `\"gptq\"` is useful for **8GB GPUs**. |\n",
    "| **`revision`** | `str \\| None` | Hugging Face model commit hash | `None` | Specifies a model version. | **Use a specific revision if testing different versions** of a model. |\n",
    "| **`tokenizer_revision`** | `str \\| None` | Hugging Face tokenizer commit hash | `None` | Specifies tokenizer version. | Usually **not needed**, but useful if a tokenizer update affects behavior. |\n",
    "| **`seed`** | `int` | Any integer | `0` | Sets random seed for reproducibility. | **Set to a fixed value (`42`) if you need consistent results.** |\n",
    "\n",
    "\n",
    "#### Recommended Settings for Different Hardware\n",
    "\n",
    "| **Use Case** | **Model** | **dtype** | **tensor_parallel_size** | **quantization** | **Optimization Notes** |\n",
    "|-------------|---------|--------|-------------------|--------------|--------------------|\n",
    "| **Best Performance (High VRAM, A100/H100, 80GB VRAM)** | `\"meta-llama/Llama-3.1-8b\"` | `\"bfloat16\"` | `2+` | `None` | Multi-GPU setup, fastest inference. |\n",
    "| **Optimized for Consumer GPUs (RTX 3090/4090, 24GB VRAM)** | `\"meta-llama/Llama-3.1-8b\"` | `\"float16\"` | `1-2` | `\"awq\"` | Halves VRAM usage with minimal loss in accuracy. |\n",
    "| **Low VRAM Setup (RTX 3060, 12GB VRAM)** | `\"meta-llama/Llama-3.1-8b\"` | `\"float16\"` | `1` | `\"gptq\"` | Optimized for **low-memory GPUs**, reduces precision slightly. |\n",
    "| **CPU-Only (Mac M1/M2, Non-GPU systems)** | `\"meta-llama/Llama-3.1-8b\"` | `\"float32\"` | `1` | `\"gptq\"` | `\"float32\"` is needed since Mac doesn’t support `\"float16\"`. |\n",
    "\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-09 12:23:56 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 03-09 12:23:57 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-09 12:24:05 config.py:549] This model supports multiple tasks: {'embed', 'classify', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 03-09 12:24:05 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='meta-llama/Llama-3.1-8b', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=24576, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.1-8b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-09 12:24:08 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-09 12:24:09 model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8b...\n",
      "INFO 03-09 12:24:10 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a52475e95a74b52bb60f840a7da0812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-09 12:24:18 model_runner.py:1115] Loading model weights took 14.9888 GB\n",
      "INFO 03-09 12:24:25 worker.py:267] Memory profiling takes 7.08 seconds\n",
      "INFO 03-09 12:24:25 worker.py:267] the current vLLM instance can use total_gpu_memory (22.05GiB) x gpu_memory_utilization (0.99) = 21.82GiB\n",
      "INFO 03-09 12:24:25 worker.py:267] model weights take 14.99GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 2.54GiB; the rest of the memory reserved for KV Cache is 4.25GiB.\n",
      "INFO 03-09 12:24:26 executor_base.py:111] # cuda blocks: 2176, # CPU blocks: 2048\n",
      "INFO 03-09 12:24:26 executor_base.py:116] Maximum concurrency for 24576 tokens per request: 1.42x\n",
      "INFO 03-09 12:24:29 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:27<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-09 12:24:57 model_runner.py:1562] Graph capturing finished in 28 secs, took 0.26 GiB\n",
      "INFO 03-09 12:24:57 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 38.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "from LLM_engines.vLLM_model_handler import vLLMHandler\n",
    "\n",
    "vllm_handler = vLLMHandler(\n",
    "    model_name=\"meta-llama/Llama-3.1-8b\", \n",
    "    dtype=\"float16\", \n",
    "    gpu_util=0.99, \n",
    "    quantization=None, \n",
    "    seed=42)\n",
    "\n",
    "benchmark_vllm = ModelBenchmark(vllm_handler, backend=\"vllm\")\n",
    "\n",
    "# Run benchmark\n",
    "test_prompts = [\n",
    "    \"Explain the significance of transformer models in NLP.\",\n",
    "    \"What are the main benefits of renewable energy?\",\n",
    "    \"How does the immune system work?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is the best way to cook a steak?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt (length 54 characters)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.07s/it, est. speed input: 0.39 toks/s, output: 16.48 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.09s/it, est. speed input: 0.39 toks/s, output: 16.47 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt (length 47 characters)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.73s/it, est. speed input: 0.44 toks/s, output: 16.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.11s/it, est. speed input: 0.32 toks/s, output: 16.46 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt (length 32 characters)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.11s/it, est. speed input: 0.26 toks/s, output: 16.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.08s/it, est. speed input: 0.38 toks/s, output: 16.46 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt (length 30 characters)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.11s/it, est. speed input: 0.26 toks/s, output: 16.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.19s/it, est. speed input: 0.26 toks/s, output: 16.42 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompt (length 37 characters)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.11s/it, est. speed input: 0.35 toks/s, output: 16.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:31<00:00, 31.11s/it, est. speed input: 0.35 toks/s, output: 16.46 toks/s]\n"
     ]
    }
   ],
   "source": [
    "vllm_results = benchmark_vllm.benchmark(test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt Length</th>\n",
       "      <th>FTL (s)</th>\n",
       "      <th>ATL (s)</th>\n",
       "      <th>GL (s)</th>\n",
       "      <th>TPS (tokens/s)</th>\n",
       "      <th>SPS (sentences/s)</th>\n",
       "      <th>Memory Usage (MB)</th>\n",
       "      <th>Model Size (MB)</th>\n",
       "      <th>KV-Cache Size Estimation (MB)</th>\n",
       "      <th>Total Energy (Wh)</th>\n",
       "      <th>Energy per Token (J/token)</th>\n",
       "      <th>Energy per Sentence (J/sentence)</th>\n",
       "      <th>Energy per Second (W)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>0.0728</td>\n",
       "      <td>0.0728</td>\n",
       "      <td>31.0805</td>\n",
       "      <td>14.22</td>\n",
       "      <td>0.68</td>\n",
       "      <td>20696.06</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.621860</td>\n",
       "      <td>5.065331</td>\n",
       "      <td>105.925000</td>\n",
       "      <td>72.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>22.7282</td>\n",
       "      <td>18.44</td>\n",
       "      <td>1.28</td>\n",
       "      <td>20696.06</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.455038</td>\n",
       "      <td>3.908623</td>\n",
       "      <td>56.308594</td>\n",
       "      <td>72.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>0.0715</td>\n",
       "      <td>0.0715</td>\n",
       "      <td>31.1140</td>\n",
       "      <td>9.51</td>\n",
       "      <td>0.67</td>\n",
       "      <td>20696.06</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.621554</td>\n",
       "      <td>7.562145</td>\n",
       "      <td>107.337313</td>\n",
       "      <td>71.916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0784</td>\n",
       "      <td>0.0784</td>\n",
       "      <td>31.1178</td>\n",
       "      <td>10.09</td>\n",
       "      <td>0.80</td>\n",
       "      <td>20696.06</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.621829</td>\n",
       "      <td>7.129732</td>\n",
       "      <td>89.923750</td>\n",
       "      <td>71.939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>31.1133</td>\n",
       "      <td>11.80</td>\n",
       "      <td>1.16</td>\n",
       "      <td>20696.06</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.621013</td>\n",
       "      <td>6.089407</td>\n",
       "      <td>61.943966</td>\n",
       "      <td>71.855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prompt Length  FTL (s)  ATL (s)   GL (s)  TPS (tokens/s)  \\\n",
       "0             54   0.0728   0.0728  31.0805           14.22   \n",
       "1             47   0.0689   0.0689  22.7282           18.44   \n",
       "2             32   0.0715   0.0715  31.1140            9.51   \n",
       "3             30   0.0784   0.0784  31.1178           10.09   \n",
       "4             37   0.0691   0.0691  31.1133           11.80   \n",
       "\n",
       "   SPS (sentences/s)  Memory Usage (MB) Model Size (MB)  \\\n",
       "0               0.68           20696.06             N/A   \n",
       "1               1.28           20696.06             N/A   \n",
       "2               0.67           20696.06             N/A   \n",
       "3               0.80           20696.06             N/A   \n",
       "4               1.16           20696.06             N/A   \n",
       "\n",
       "  KV-Cache Size Estimation (MB)  Total Energy (Wh)  \\\n",
       "0                           N/A           0.621860   \n",
       "1                           N/A           0.455038   \n",
       "2                           N/A           0.621554   \n",
       "3                           N/A           0.621829   \n",
       "4                           N/A           0.621013   \n",
       "\n",
       "   Energy per Token (J/token)  Energy per Sentence (J/sentence)  \\\n",
       "0                    5.065331                        105.925000   \n",
       "1                    3.908623                         56.308594   \n",
       "2                    7.562145                        107.337313   \n",
       "3                    7.129732                         89.923750   \n",
       "4                    6.089407                         61.943966   \n",
       "\n",
       "   Energy per Second (W)  \n",
       "0                 72.029  \n",
       "1                 72.075  \n",
       "2                 71.916  \n",
       "3                 71.939  \n",
       "4                 71.855  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vllm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark\n",
    "\n",
    "Find optimal optimization frameworks for a specific:\n",
    "\n",
    "tasks: [text-to-SQL, summarization, QA, ...]\n",
    "\n",
    "scenario: [server, batch, single-stream, ...]\n",
    "\n",
    "models: [llama3.1-8B, Mixtral, ...]\n",
    "\n",
    "frameworks: [vLLM, tensorRT, ...] (trying different configurations)\n",
    "\n",
    "hardware: [L4, A100, H100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization Frameworks for LLM Inference\n",
    "\n",
    "<small>\n",
    "\n",
    "| **Framework** | **Quantization Support** | **Model Parallelism** | **Tensor / Kernel Optimizations** | **Memory Efficiency** | **Throughput & Latency** | **Financial / Energy Efficiency** |\n",
    "|--------------|--------------------------|-----------------------|-----------------------------------|-----------------------|--------------------------|-----------------------------------|\n",
    "| **vLLM** (UC Berkeley) | FP16/BF16, 8-bit, 4-bit (GPTQ, AWQ), experimental FP8 | Tensor Parallelism, Pipeline Parallelism | Custom **PagedAttention**, fused attention kernels, PyTorch fused ops | **Paged KV cache**, quantized KV cache, prefix reuse | **Continuous batching**, **speculative decoding**, auto-regressive scheduling | 2–4× throughput improvement, high GPU utilization, cost-efficient serving |\n",
    "| **NVIDIA TensorRT-LLM** | FP16/BF16, FP8 (Hopper), INT8 (SmoothQuant), 4-bit AWQ | Tensor, Pipeline, MoE Parallelism | Custom fused attention kernels, TensorRT graph optimization, kernel fusion | **Paged KV caching**, weight compression, memory-optimized inference engines | **Dynamic batching**, **speculative decoding (ReDrafter)**, automatic cache sharing | FP8/INT8 for lower power, throughput-focused, optimized GPU utilization |\n",
    "| **DeepSpeed-Inference** | INT8 (MoQ), 4-bit weight quantization | Tensor, Pipeline, **Expert Parallelism (MoE)** | Transformer kernel injection, optimized CUDA fused ops | **ZeRO-Inference** (offload KV cache to CPU/NVMe) | **Massive batch sizes**, multi-GPU scaling, ZeRO latency reduction | INT8 models ~1.7× faster, **CPU memory offloading** reduces GPU costs |\n",
    "| **Hugging Face TGI** | 8-bit (bitsandbytes), 4-bit (GPTQ, AWQ), FP8 experimental | Tensor Parallelism, Model Sharding | FlashAttention, PagedAttention, fused sampling ops | **Paged KV cache**, weight mapping via Safetensors | **Continuous batching**, **speculative decoding (Medusa)** | High throughput (up to 13× faster on long inputs), flexible multi-platform support |\n",
    "| **NVIDIA FasterTransformer** | FP16/BF16, INT8, FP8, 4-bit compression | Tensor, Pipeline Parallelism | Fused MLP layers, fused multi-head attention, optimized GEMM | **Persistent KV cache**, optimized weight storage | **Optimized CUDA scheduling**, efficient multi-GPU serving | Low-latency, high-throughput GPU inference, FP16/INT8 for energy efficiency |\n",
    "| **ONNX Runtime** (HF Optimum) | INT8 post-training quantization, FP16, BF16 | No native multi-GPU, requires manual graph partitioning | Graph optimizations, operator fusion, execution provider optimizations | **Memory-efficient graph storage**, weight pre-packing | **Batching support**, CPU multi-threading | Cost-efficient CPU/GPU inference, flexible hardware deployment |\n",
    "| **llama.cpp / GGML** | 16-bit, 8-bit, 6-bit, 5-bit, 4-bit, 3-bit, 2-bit | No native multi-GPU, but supports CPU-GPU offloading | **SIMD-optimized** (AVX, NEON), bit-packed matrix multiplication | **Memory-mapped weights**, CPU RAM paging for large models | **Single-request optimized**, multi-threaded CPU processing | Extreme low-cost inference, edge deployment, power-efficient CPU inference |\n",
    "\n",
    "</small>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
