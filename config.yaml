# choose one of: huggingface, vllm, llama.cpp, deepspeed_mii, lmdeploy
backend: tgi

# path to your model directory or .gguf file
model_path: mistralai/Mistral-7B-Instruct-v0.3
model_name: Mistral-7B-Instruct-v0.3

# task & scenario
task: qa    # or "qa", "sql", "summarization"
scenario: batch        # "single", "batch", or "server"

##########################################################

# only for batch
samples: 16
batch_size: 4

##########################################################

# only for single
# samples: 16

##########################################################

# only for server
#run_time: 60
#concurrent_users: 10
#requests_per_user_per_min: 10

# optional overrides
sample_interval: 0.1
quality_metric: true

# prefix for output CSVs
output_prefix: results_benchmark/llama31b_qa_batch