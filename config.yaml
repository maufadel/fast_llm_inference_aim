# choose one of: huggingface, vllm, llama.cpp, deepspeed_mii, lmdeploy
backend: sglang

# path to your model directory or .gguf file
model_path: mistralai/Mistral-7B-Instruct-v0.3
model_name: Mistral-7B-Instruct-v0.3

# task & scenario
task: qa    # or "qa", "sql"
scenario: batch        # "single", "batch", or "server"

# only for batch/single
samples: 8
batch_size: 2

# only for server
# run_time: 30
# requests_per_sec: 5
# max_batch_size: 8

# optional overrides
sample_interval: 0.1
quality_metric: true

# prefix for output CSVs
output_prefix: results_benchmark/llama31b_qa_batch