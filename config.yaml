# choose one of: tgi, vllm, sglang, lmdeploy
backend: lmdeploy

# huggingface model path
model_path: Qwen/Qwen2.5-7B-Instruct

# model name
model_name: Qwen2.5-7B-Instruct

# choose a scenario of: "server", "single", "batch", "long_context"
scenario: batch        

# choose a task of: "summarization", "sql", "qa", "long_context_qa"
task: qa 

# sample interval in seconds
sample_interval: 0.1

# tracking quality metrics
quality_metric: true

##########################################################

# only for batch
samples: 64
batch_size: 32

##########################################################

# only for single
# samples: 128

##########################################################

# only for server
#run_time: 60
#concurrent_users: 10
#requests_per_user_per_min: 10

##########################################################

# only for long context qa
# number of samples per context length (3k, 4k, 8k, 16k, 32k tokens)
# samples: 100

##########################################################
