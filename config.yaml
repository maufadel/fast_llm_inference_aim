# choose one of: huggingface, vllm, llama.cpp, deepspeed_mii, lmdeploy
backend: tgi

# path to your model directory or .gguf file
model_path: mistralai/Mistral-7B-Instruct-v0.3
model_name: Mistral-7B-Instruct-v0.3

# task & scenario
task: qa    # or "qa", "sql", "summarization"
scenario: batch        # "single", "batch", or "server"

# only for batch/single
samples: 16
batch_size: 4

# only for server
# run_time: 30
# requests_per_sec: 5
# max_batch_size: 8

# optional overrides
sample_interval: 0.1
quality_metric: true

# prefix for output CSVs
output_prefix: results_benchmark/llama31b_qa_batch