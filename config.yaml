# choose one of: huggingface, vllm, llama.cpp, deepspeed_mii, lmdeploy
backend: huggingface

# path to your model directory or .gguf file
model_path: /home/ubuntu/fast_llm_inference/models/llama-3.2-3b-instruct
model_name: llama-3.1-8B

# task & scenario
task: qa    # or "qa", "sql"
scenario: batch        # "single", "batch", or "server"

# only for batch/single
samples: 8
batch_size: 2

# only for server
# run_time: 30
# requests_per_sec: 5
# max_batch_size: 8

# optional overrides
sample_interval: 0.1
quality_metric: true

# prefix for output CSVs
output_prefix: results_benchmark/llama31b_qa_batch