{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc37054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-28 15:18:51 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 15:18:51.873981: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748445531.898733  176014 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748445531.906032  176014 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748445531.922127  176014 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748445531.922144  176014 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748445531.922146  176014 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748445531.922148  176014 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-28 15:18:51.927486: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-28 15:19:13 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'classify', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 05-28 15:19:15 [config.py:830] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-28 15:19:15 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-28 15:19:16 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', speculative_config=None, tokenizer='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-28 15:19:17 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f96e6afd350>\n",
      "INFO 05-28 15:19:19 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-28 15:19:19 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-28 15:19:19 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-28 15:19:19 [gpu_model_runner.py:1329] Starting to load model /home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit...\n",
      "INFO 05-28 15:19:19 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb022f2dfa94b35ad9c237e984079db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba2eb982d2943698eff9e56c83dff69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-28 15:19:47 [gpu_model_runner.py:1347] Model loading took 5.3132 GiB and 27.324152 seconds\n",
      "INFO 05-28 15:20:02 [backends.py:420] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/0f63e24e8b/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-28 15:20:02 [backends.py:430] Dynamo bytecode transform time: 14.88 s\n",
      "INFO 05-28 15:20:11 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 6.773 s\n",
      "INFO 05-28 15:20:14 [monitor.py:33] torch.compile takes 14.88 s in total\n",
      "INFO 05-28 15:20:17 [kv_cache_utils.py:634] GPU KV cache size: 106,880 tokens\n",
      "INFO 05-28 15:20:17 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 26.09x\n",
      "INFO 05-28 15:21:20 [gpu_model_runner.py:1686] Graph capturing finished in 64 secs, took 1.54 GiB\n",
      "INFO 05-28 15:21:21 [core.py:159] init engine (profile, create kv cache, warmup model) took 93.87 seconds\n",
      "INFO 05-28 15:21:21 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and setup\n",
    "import os\n",
    "import math\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# (Optional) adjust your model path here\n",
    "MODEL_PATH = \"/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit\"\n",
    "\n",
    "# Cell 2: Load model and define prompts\n",
    "model = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    max_model_len=4096,\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"What is the purpose of life?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09340280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b8dee6ec1345c58bd5dc5ebdcc32cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prompt 1: The quick brown fox jumps over the lazy dog. ===\n",
      "Generated: The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick\n",
      "\n",
      "       Token |  LogProb |      PPL\n",
      "----------------------------------\n",
      "        ĠThe |  -0.9727 |   2.6450\n",
      "      Ġquick |  -0.3296 |   1.3905\n",
      "      Ġbrown |  -0.0029 |   1.0029\n",
      "        Ġfox |  -0.0016 |   1.0016\n",
      "      Ġjumps |  -0.0055 |   1.0055\n",
      "       Ġover |  -0.0005 |   1.0005\n",
      "        Ġthe |  -0.0005 |   1.0005\n",
      "       Ġlazy |  -0.0005 |   1.0005\n",
      "        Ġdog |  -0.0004 |   1.0004\n",
      "           . |  -0.3360 |   1.3993\n",
      "        ĠThe |  -0.0372 |   1.0379\n",
      "      Ġquick |  -0.0015 |   1.0015\n",
      "      Ġbrown |  -0.0012 |   1.0012\n",
      "        Ġfox |  -0.0007 |   1.0007\n",
      "      Ġjumps |  -0.0013 |   1.0013\n",
      "       Ġover |  -0.0008 |   1.0008\n",
      "        Ġthe |  -0.0010 |   1.0010\n",
      "       Ġlazy |  -0.0010 |   1.0010\n",
      "        Ġdog |  -0.0005 |   1.0005\n",
      "           . |  -0.2609 |   1.2981\n",
      "        ĠThe |  -0.0465 |   1.0476\n",
      "      Ġquick |  -0.0021 |   1.0021\n",
      "      Ġbrown |  -0.0006 |   1.0006\n",
      "        Ġfox |  -0.0014 |   1.0014\n",
      "      Ġjumps |  -0.0008 |   1.0008\n",
      "       Ġover |  -0.0002 |   1.0002\n",
      "        Ġthe |  -0.0008 |   1.0008\n",
      "       Ġlazy |  -0.0007 |   1.0007\n",
      "        Ġdog |  -0.0004 |   1.0004\n",
      "           . |  -0.1384 |   1.1484\n",
      "        ĠThe |  -0.0204 |   1.0206\n",
      "      Ġquick |  -0.0011 |   1.0011\n",
      "\n",
      "Sequence-level Perplexity: 1.0702\n",
      "\n",
      "=== Prompt 2: What is the purpose of life? ===\n",
      "Generated: Is it to find happiness, to achieve success, or to make a difference in the world? The answer to this question is subjective and can vary greatly from person\n",
      "\n",
      "       Token |  LogProb |      PPL\n",
      "----------------------------------\n",
      "         ĠIs |  -1.4004 |   4.0569\n",
      "         Ġit |  -0.1798 |   1.1970\n",
      "         Ġto |  -0.0869 |   1.0908\n",
      "       Ġfind |  -0.9659 |   2.6272\n",
      "  Ġhappiness |  -0.0480 |   1.0492\n",
      "           , |  -0.0587 |   1.0604\n",
      "         Ġto |  -0.8425 |   2.3222\n",
      "    Ġachieve |  -0.7216 |   2.0577\n",
      "    Ġsuccess |  -0.0438 |   1.0448\n",
      "           , |  -0.0013 |   1.0013\n",
      "         Ġto |  -0.6181 |   1.8554\n",
      "         Ġto |  -0.0315 |   1.0320\n",
      "       Ġmake |  -0.7800 |   2.1815\n",
      "          Ġa |  -0.0186 |   1.0188\n",
      " Ġdifference |  -0.1679 |   1.1829\n",
      "         Ġin |  -0.1038 |   1.1094\n",
      "        Ġthe |  -0.0010 |   1.0010\n",
      "      Ġworld |  -0.0022 |   1.0022\n",
      "           ? |  -0.1106 |   1.1170\n",
      "        ĠThe |  -1.3354 |   3.8015\n",
      "     Ġanswer |  -0.2585 |   1.2950\n",
      "         Ġto |  -0.9216 |   2.5133\n",
      "       Ġthis |  -0.0481 |   1.0493\n",
      "   Ġquestion |  -0.0216 |   1.0219\n",
      "         Ġis |  -0.5206 |   1.6830\n",
      " Ġsubjective |  -1.2614 |   3.5304\n",
      "        Ġand |  -0.0497 |   1.0509\n",
      "        Ġcan |  -0.4695 |   1.5992\n",
      "       Ġvary |  -0.0099 |   1.0099\n",
      "    Ġgreatly |  -0.3733 |   1.4525\n",
      "       Ġfrom |  -0.1455 |   1.1567\n",
      "     Ġperson |  -0.0396 |   1.0404\n",
      "\n",
      "Sequence-level Perplexity: 1.4386\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Configure SamplingParams for logprobs & perplexity\n",
    "params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    max_tokens=32,\n",
    "    logprobs=1,\n",
    "    prompt_logprobs=1\n",
    ")\n",
    "\n",
    "# Cell 4: Run generation and display results in a table plus sequence PPL\n",
    "outputs = model.generate(prompts, params)\n",
    "\n",
    "for i, gen_out in enumerate(outputs):\n",
    "    sample    = gen_out.outputs[0]\n",
    "    text      = sample.text.lstrip()\n",
    "    lp_list   = sample.logprobs            # list of dicts\n",
    "    token_ids = sample.token_ids\n",
    "\n",
    "    # 1) Extract the chosen-token strings & logprobs\n",
    "    tokens, logps = [], []\n",
    "    for entry in lp_list:\n",
    "        # each entry is {token_id: Logprob(...), ...}\n",
    "        for tid, lp_obj in entry.items():\n",
    "            if lp_obj.rank == 1:\n",
    "                tokens.append(lp_obj.decoded_token)\n",
    "                logps.append(lp_obj.logprob)\n",
    "                break\n",
    "\n",
    "    # 2) Compute per-token perplexity\n",
    "    ppl = [math.exp(-lp) for lp in logps]\n",
    "\n",
    "    # 3) Print per-token table\n",
    "    print(f\"\\n=== Prompt {i+1}: {prompts[i]} ===\")\n",
    "    print(f\"Generated: {text}\\n\")\n",
    "    print(f\"{'Token':>12} | {'LogProb':>8} | {'PPL':>8}\")\n",
    "    print(\"-\" * 34)\n",
    "    for tok, lp, p in zip(tokens, logps, ppl):\n",
    "        print(f\"{tok:>12} | {lp:8.4f} | {p:8.4f}\")\n",
    "\n",
    "    # 4) Compute sequence-level perplexity\n",
    "    ppl_seq = math.exp(- sum(logps) / len(logps))\n",
    "    print(f\"\\nSequence-level Perplexity: {ppl_seq:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec43e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fast_llm_inference/experiments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9655c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# Insert the parent directory of the current file/notebook\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "from benchmark.backends.tgi_backend import TGIBackend\n",
    "\n",
    "tgi = TGIBackend(\n",
    "    model_path=\"meta-llama/Llama-3.1-8B-Instruct\", # \"Qwen/Qwen2.5-7B-Instruct\", \"google/gemma-2-9b-it\"\n",
    ")\n",
    "tgi.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b1bcd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.tasks.qa import QATask\n",
    "\n",
    "qa_task = QATask()\n",
    "\n",
    "queries = qa_task.generate_prompts(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "975559f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = tgi.generate(queries[0], task_type=\"qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73e65389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lothar de Maizière.  ### SYSTEM\\nYou are a question-answering assistant. Answer in exactly **ONE** line. If the',\n",
       " 'Complexity classes with complicated definitions.  ### SYSTEM\\nYou are a question-answering assistant. Answer in exactly **ONE** line. If the answer is',\n",
       " 'GTE.  ### SYSTEM\\nYou are a question-answering assistant. Answer in exactly **ONE** line. If the answer is not contained in']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "714a3ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585afb7f8e27854288e91d4d4187a395f6a3d02a4a1fc983a151cad491841342\n"
     ]
    }
   ],
   "source": [
    "tgi.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95a34e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-29 11:47:55 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Insert the parent directory of the current file/notebook\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "from benchmark.benchmark import ModelBenchmark\n",
    "\n",
    "bm = ModelBenchmark(\n",
    "    backend=\"tgi\",\n",
    "    model_name=\"Llama-3.1-8B-Instruct\",\n",
    "    model_path=\"meta-llama/Llama-3.1-8B-Instruct\", # \"Qwen/Qwen2.5-7B-Instruct\", \"google/gemma-2-9b-it\"\n",
    "    model_size_mb=16_100,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79e8e11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8375e5b408c7de89710217c32cb43db8137b0b79cff5fd1e4dce23ad9e2dc80c\n"
     ]
    }
   ],
   "source": [
    "run_report, details_df = bm.run(\n",
    "    task=\"summarization\",\n",
    "    scenario=\"batch\",\n",
    "    samples=16,\n",
    "    batch_size=8,\n",
    "    sample_interval=0.1,\n",
    "    quality_metric=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f1e914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_size_mb</th>\n",
       "      <th>backend</th>\n",
       "      <th>startup_time_sec</th>\n",
       "      <th>load_model_time_sec</th>\n",
       "      <th>ttft_sec</th>\n",
       "      <th>cold_start_sec</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>num_queries</th>\n",
       "      <th>total_generation_time_s</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_ATL</th>\n",
       "      <th>avg_GL</th>\n",
       "      <th>avg_TPS</th>\n",
       "      <th>avg_SPS</th>\n",
       "      <th>avg_energy_per_token</th>\n",
       "      <th>avg_energy_per_sentence</th>\n",
       "      <th>avg_estimated_query_cost_usd</th>\n",
       "      <th>avg_ROUGE-1</th>\n",
       "      <th>avg_ROUGE-2</th>\n",
       "      <th>avg_ROUGE-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llama-3.1-8B-Instruct</td>\n",
       "      <td>16100</td>\n",
       "      <td>tgi</td>\n",
       "      <td>0.002</td>\n",
       "      <td>22.0575</td>\n",
       "      <td>0.0755</td>\n",
       "      <td>22.1349</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>44.639383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021154</td>\n",
       "      <td>2.789961</td>\n",
       "      <td>47.725</td>\n",
       "      <td>4.0175</td>\n",
       "      <td>1.440265</td>\n",
       "      <td>17.721448</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.298337</td>\n",
       "      <td>0.11193</td>\n",
       "      <td>0.190076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              model_name  model_size_mb backend  startup_time_sec  \\\n",
       "0  Llama-3.1-8B-Instruct          16100     tgi             0.002   \n",
       "\n",
       "   load_model_time_sec  ttft_sec  cold_start_sec  batch_size  num_queries  \\\n",
       "0              22.0575    0.0755         22.1349           8           16   \n",
       "\n",
       "   total_generation_time_s  ...   avg_ATL    avg_GL  avg_TPS  avg_SPS  \\\n",
       "0                44.639383  ...  0.021154  2.789961   47.725   4.0175   \n",
       "\n",
       "   avg_energy_per_token  avg_energy_per_sentence  \\\n",
       "0              1.440265                17.721448   \n",
       "\n",
       "   avg_estimated_query_cost_usd  avg_ROUGE-1  avg_ROUGE-2  avg_ROUGE-L  \n",
       "0                      0.000278     0.298337      0.11193     0.190076  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
