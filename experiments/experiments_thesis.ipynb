{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc37054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-28 15:18:51 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 15:18:51.873981: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748445531.898733  176014 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748445531.906032  176014 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748445531.922127  176014 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748445531.922144  176014 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748445531.922146  176014 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748445531.922148  176014 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-28 15:18:51.927486: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-28 15:19:13 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'classify', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 05-28 15:19:15 [config.py:830] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-28 15:19:15 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-28 15:19:16 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', speculative_config=None, tokenizer='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-28 15:19:17 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f96e6afd350>\n",
      "INFO 05-28 15:19:19 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-28 15:19:19 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-28 15:19:19 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-28 15:19:19 [gpu_model_runner.py:1329] Starting to load model /home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit...\n",
      "INFO 05-28 15:19:19 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb022f2dfa94b35ad9c237e984079db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba2eb982d2943698eff9e56c83dff69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-28 15:19:47 [gpu_model_runner.py:1347] Model loading took 5.3132 GiB and 27.324152 seconds\n",
      "INFO 05-28 15:20:02 [backends.py:420] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/0f63e24e8b/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-28 15:20:02 [backends.py:430] Dynamo bytecode transform time: 14.88 s\n",
      "INFO 05-28 15:20:11 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 6.773 s\n",
      "INFO 05-28 15:20:14 [monitor.py:33] torch.compile takes 14.88 s in total\n",
      "INFO 05-28 15:20:17 [kv_cache_utils.py:634] GPU KV cache size: 106,880 tokens\n",
      "INFO 05-28 15:20:17 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 26.09x\n",
      "INFO 05-28 15:21:20 [gpu_model_runner.py:1686] Graph capturing finished in 64 secs, took 1.54 GiB\n",
      "INFO 05-28 15:21:21 [core.py:159] init engine (profile, create kv cache, warmup model) took 93.87 seconds\n",
      "INFO 05-28 15:21:21 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and setup\n",
    "import os\n",
    "import math\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# (Optional) adjust your model path here\n",
    "MODEL_PATH = \"/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit\"\n",
    "\n",
    "# Cell 2: Load model and define prompts\n",
    "model = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    max_model_len=4096,\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"What is the purpose of life?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09340280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b8dee6ec1345c58bd5dc5ebdcc32cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prompt 1: The quick brown fox jumps over the lazy dog. ===\n",
      "Generated: The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick\n",
      "\n",
      "       Token |  LogProb |      PPL\n",
      "----------------------------------\n",
      "        ĠThe |  -0.9727 |   2.6450\n",
      "      Ġquick |  -0.3296 |   1.3905\n",
      "      Ġbrown |  -0.0029 |   1.0029\n",
      "        Ġfox |  -0.0016 |   1.0016\n",
      "      Ġjumps |  -0.0055 |   1.0055\n",
      "       Ġover |  -0.0005 |   1.0005\n",
      "        Ġthe |  -0.0005 |   1.0005\n",
      "       Ġlazy |  -0.0005 |   1.0005\n",
      "        Ġdog |  -0.0004 |   1.0004\n",
      "           . |  -0.3360 |   1.3993\n",
      "        ĠThe |  -0.0372 |   1.0379\n",
      "      Ġquick |  -0.0015 |   1.0015\n",
      "      Ġbrown |  -0.0012 |   1.0012\n",
      "        Ġfox |  -0.0007 |   1.0007\n",
      "      Ġjumps |  -0.0013 |   1.0013\n",
      "       Ġover |  -0.0008 |   1.0008\n",
      "        Ġthe |  -0.0010 |   1.0010\n",
      "       Ġlazy |  -0.0010 |   1.0010\n",
      "        Ġdog |  -0.0005 |   1.0005\n",
      "           . |  -0.2609 |   1.2981\n",
      "        ĠThe |  -0.0465 |   1.0476\n",
      "      Ġquick |  -0.0021 |   1.0021\n",
      "      Ġbrown |  -0.0006 |   1.0006\n",
      "        Ġfox |  -0.0014 |   1.0014\n",
      "      Ġjumps |  -0.0008 |   1.0008\n",
      "       Ġover |  -0.0002 |   1.0002\n",
      "        Ġthe |  -0.0008 |   1.0008\n",
      "       Ġlazy |  -0.0007 |   1.0007\n",
      "        Ġdog |  -0.0004 |   1.0004\n",
      "           . |  -0.1384 |   1.1484\n",
      "        ĠThe |  -0.0204 |   1.0206\n",
      "      Ġquick |  -0.0011 |   1.0011\n",
      "\n",
      "Sequence-level Perplexity: 1.0702\n",
      "\n",
      "=== Prompt 2: What is the purpose of life? ===\n",
      "Generated: Is it to find happiness, to achieve success, or to make a difference in the world? The answer to this question is subjective and can vary greatly from person\n",
      "\n",
      "       Token |  LogProb |      PPL\n",
      "----------------------------------\n",
      "         ĠIs |  -1.4004 |   4.0569\n",
      "         Ġit |  -0.1798 |   1.1970\n",
      "         Ġto |  -0.0869 |   1.0908\n",
      "       Ġfind |  -0.9659 |   2.6272\n",
      "  Ġhappiness |  -0.0480 |   1.0492\n",
      "           , |  -0.0587 |   1.0604\n",
      "         Ġto |  -0.8425 |   2.3222\n",
      "    Ġachieve |  -0.7216 |   2.0577\n",
      "    Ġsuccess |  -0.0438 |   1.0448\n",
      "           , |  -0.0013 |   1.0013\n",
      "         Ġto |  -0.6181 |   1.8554\n",
      "         Ġto |  -0.0315 |   1.0320\n",
      "       Ġmake |  -0.7800 |   2.1815\n",
      "          Ġa |  -0.0186 |   1.0188\n",
      " Ġdifference |  -0.1679 |   1.1829\n",
      "         Ġin |  -0.1038 |   1.1094\n",
      "        Ġthe |  -0.0010 |   1.0010\n",
      "      Ġworld |  -0.0022 |   1.0022\n",
      "           ? |  -0.1106 |   1.1170\n",
      "        ĠThe |  -1.3354 |   3.8015\n",
      "     Ġanswer |  -0.2585 |   1.2950\n",
      "         Ġto |  -0.9216 |   2.5133\n",
      "       Ġthis |  -0.0481 |   1.0493\n",
      "   Ġquestion |  -0.0216 |   1.0219\n",
      "         Ġis |  -0.5206 |   1.6830\n",
      " Ġsubjective |  -1.2614 |   3.5304\n",
      "        Ġand |  -0.0497 |   1.0509\n",
      "        Ġcan |  -0.4695 |   1.5992\n",
      "       Ġvary |  -0.0099 |   1.0099\n",
      "    Ġgreatly |  -0.3733 |   1.4525\n",
      "       Ġfrom |  -0.1455 |   1.1567\n",
      "     Ġperson |  -0.0396 |   1.0404\n",
      "\n",
      "Sequence-level Perplexity: 1.4386\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Configure SamplingParams for logprobs & perplexity\n",
    "params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    max_tokens=32,\n",
    "    logprobs=1,\n",
    "    prompt_logprobs=1\n",
    ")\n",
    "\n",
    "# Cell 4: Run generation and display results in a table plus sequence PPL\n",
    "outputs = model.generate(prompts, params)\n",
    "\n",
    "for i, gen_out in enumerate(outputs):\n",
    "    sample    = gen_out.outputs[0]\n",
    "    text      = sample.text.lstrip()\n",
    "    lp_list   = sample.logprobs            # list of dicts\n",
    "    token_ids = sample.token_ids\n",
    "\n",
    "    # 1) Extract the chosen-token strings & logprobs\n",
    "    tokens, logps = [], []\n",
    "    for entry in lp_list:\n",
    "        # each entry is {token_id: Logprob(...), ...}\n",
    "        for tid, lp_obj in entry.items():\n",
    "            if lp_obj.rank == 1:\n",
    "                tokens.append(lp_obj.decoded_token)\n",
    "                logps.append(lp_obj.logprob)\n",
    "                break\n",
    "\n",
    "    # 2) Compute per-token perplexity\n",
    "    ppl = [math.exp(-lp) for lp in logps]\n",
    "\n",
    "    # 3) Print per-token table\n",
    "    print(f\"\\n=== Prompt {i+1}: {prompts[i]} ===\")\n",
    "    print(f\"Generated: {text}\\n\")\n",
    "    print(f\"{'Token':>12} | {'LogProb':>8} | {'PPL':>8}\")\n",
    "    print(\"-\" * 34)\n",
    "    for tok, lp, p in zip(tokens, logps, ppl):\n",
    "        print(f\"{tok:>12} | {lp:8.4f} | {p:8.4f}\")\n",
    "\n",
    "    # 4) Compute sequence-level perplexity\n",
    "    ppl_seq = math.exp(- sum(logps) / len(logps))\n",
    "    print(f\"\\nSequence-level Perplexity: {ppl_seq:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d64be",
   "metadata": {},
   "source": [
    "Using the official Docker images to launch the inference engines:\n",
    "\n",
    "\n",
    "##### TGI (check)\n",
    "\n",
    "docker run --rm \\\n",
    "  --gpus all \\\n",
    "  -v \"$HOME/.cache/huggingface:/data\" \\\n",
    "  -v \"$HOME/.cache/huggingface:/root/.cache/huggingface\" \\\n",
    "  -e HF_TOKEN=\"$HF_TOKEN\" \\\n",
    "  -p 127.0.0.1:23333:23333 \\\n",
    "  ghcr.io/huggingface/text-generation-inference:3.3.1 \\\n",
    "    --model-id mistralai/Mistral-7B-Instruct-v0.3 \\\n",
    "    --trust-remote-code \\\n",
    "    --port 23333 \\\n",
    "    --max-client-batch-size 128\n",
    "\n",
    "\n",
    "##### vLLM (check)\n",
    "\n",
    "docker run --rm \\\n",
    "  --runtime=nvidia --gpus all \\\n",
    "  -v \"$HOME/.cache/huggingface:/root/.cache/huggingface\" \\\n",
    "  -e HUGGING_FACE_HUB_TOKEN=\"$HF_TOKEN\" \\\n",
    "  -p 127.0.0.1:23333:23333 \\\n",
    "  --ipc=host \\\n",
    "  vllm/vllm-openai:latest \\\n",
    "    --model mistralai/Mistral-7B-Instruct-v0.3 \\\n",
    "    --port 23333\n",
    "\n",
    "\n",
    "##### LMDeploy (check)\n",
    "\n",
    "docker run --rm \\\n",
    "  --runtime=nvidia --gpus all \\\n",
    "  -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\n",
    "  -e HUGGING_FACE_HUB_TOKEN=$HF_TOKEN \\\n",
    "  -p 127.0.0.1:23333:23333 \\\n",
    "  --ipc=host \\\n",
    "  openmmlab/lmdeploy:latest \\\n",
    "    lmdeploy serve api_server mistralai/Mistral-7B-Instruct-v0.3 \\\n",
    "    --server-port 23333                               \n",
    "\n",
    "\n",
    "##### SGLang (check)\n",
    "\n",
    "docker run --gpus all \\\n",
    "  -p 127.0.0.1:23333:23333 \\\n",
    "  -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    "  --ipc=host \\\n",
    "  lmsysorg/sglang:latest \\\n",
    "  bash -c \"\\\n",
    "    pip install --no-cache-dir protobuf sentencepiece && \\\n",
    "    python3 -m sglang.launch_server \\\n",
    "      --model-path mistralai/Mistral-7B-Instruct-v0.3 \\\n",
    "      --host 0.0.0.0 \\\n",
    "      --port 23333 \\\n",
    "  \"\n",
    "\n",
    "\n",
    "##### Deepspeed-MII (check)\n",
    "\n",
    "docker run --runtime=nvidia --gpus all \\\n",
    "  -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\n",
    "  -e HUGGING_FACE_HUB_TOKEN=$HF_TOKEN \\\n",
    "  -p 127.0.0.1:23333:23333 \\\n",
    "  --ipc=host \\\n",
    "  slinusc/deepspeed-mii:latest \\\n",
    "  --model mistralai/Mistral-7B-Instruct-v0.3 \\\n",
    "  --port 23333\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d55bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "class InferenceEngineClient:\n",
    "    \"\"\"\n",
    "    A simple wrapper for an OpenAI‐compatible server,\n",
    "    defaulting to Mistral-7B-Instruct-v0.3.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_url=\"http://localhost:23333/v1\", api_key=\"none\"):\n",
    "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.default_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "    def completion(self,\n",
    "                   prompt,\n",
    "                   model: str | None = None,\n",
    "                   temperature: float = 0.7,\n",
    "                   max_tokens: int = 512,\n",
    "                   top_p: float = 0.9,\n",
    "                   stream: bool = False):\n",
    "        \"\"\"\n",
    "        Send one or more prompts.\n",
    "        :param prompt: a single string or a list of strings\n",
    "        :return: if single prompt, returns str; if list, returns List[str]\n",
    "        \"\"\"\n",
    "        model = model or self.default_model\n",
    "        is_batch = isinstance(prompt, (list, tuple))\n",
    "\n",
    "        resp = self.client.completions.create(\n",
    "            model=model,\n",
    "            prompt=prompt,        # can be str or list[str]\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=top_p,\n",
    "            stream=stream\n",
    "        )\n",
    "\n",
    "        if stream:\n",
    "            # streaming with batching is a bit more involved; you’ll get interleaved chunks\n",
    "            return resp\n",
    "\n",
    "        # non‐streaming: choices is a list with one entry per prompt\n",
    "        texts = [c.text for c in resp.choices]\n",
    "        return texts if is_batch else texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "923325d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# Insert the parent directory of the current file/notebook\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "from benchmark.tasks.qa import QATask\n",
    "\n",
    "qa_task = QATask()\n",
    "\n",
    "queries = qa_task.generate_prompts(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2d241d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↪ \n",
      "Complexity classes\n",
      "\n",
      "↪ \n",
      "GTE\n",
      "\n",
      "↪ \n",
      "Cuba\n",
      "\n",
      "↪ \n",
      "Underground\n",
      "\n",
      "↪ \n",
      "NP-complete\n",
      "\n",
      "↪ \n",
      "Boolean circuits\n",
      "\n",
      "↪ \n",
      "Duisburg\n",
      "\n",
      "↪ \n",
      "Exodus\n",
      "\n",
      "↪ \n",
      "Israeli poet\n",
      "\n",
      "↪ \n",
      "Trio Tribe\n",
      "\n",
      "↪ \n",
      "Lower levels of inequality\n",
      "\n",
      "↪ \n",
      "9%\n",
      "\n",
      "↪ \n",
      "An attorney\n",
      "\n",
      "↪ \n",
      "Destruction of the forest\n",
      "\n",
      "↪ \n",
      "ca. 2 million\n",
      "\n",
      "↪ \n",
      "Outcome of most votes\n",
      "\n",
      "↪ \n",
      "Islamists\n",
      "\n",
      "↪ \n",
      "Alta California\n",
      "\n",
      "↪ \n",
      "Sir Isaac Newton\n",
      "\n",
      "↪ \n",
      "Lothar de Maizière\n",
      "\n",
      "↪ \n",
      "Store and forward switching\n",
      "\n",
      "↪ \n",
      "Paul Samuelson\n",
      "\n",
      "↪ \n",
      "East-west\n",
      "\n",
      "↪ \n",
      "Urban region\n",
      "\n",
      "↪ \n",
      "Water flow through the body cavity\n",
      "\n",
      "↪ \n",
      "May no longer exist.\n",
      "\n",
      "↪ \n",
      "Great Yuan\n",
      "\n",
      "↪ \n",
      "Unit\n",
      "\n",
      "↪ \n",
      "12 May 1705\n",
      "\n",
      "↪ \n",
      "\"Time derivative of the changing momentum\"\n",
      "\n",
      "↪ \n",
      "Mughal state\n",
      "\n",
      "↪ \n",
      "Westward\n",
      "\n",
      "↪ \n",
      "Opposed\n",
      "\n",
      "↪ \n",
      "Articles 106 and 107\n",
      "\n",
      "↪ \n",
      "Semantical problems and grammatical niceties\n",
      "\n",
      "↪ \n",
      "University Athletic Association (UAA)\n",
      "\n",
      "↪ \n",
      "182 million tons\n",
      "\n",
      "↪ \n",
      "Homicides definitions\n",
      "\n",
      "↪ \n",
      "Two poles: revolution/invasion and reformist strategy.\n",
      "\n",
      "↪ \n",
      "Users via leased lines and the public PAD service Telepad\n",
      "\n",
      "↪ \n",
      "Permanent pulmonary fibrosis\n",
      "\n",
      "↪ \n",
      "Francisco de Orellana\n",
      "\n",
      "↪ \n",
      "John M. Grunsfeld\n",
      "\n",
      "↪ \n",
      "transportation, sewer, hazardous waste, water\n",
      "\n",
      "↪ \n",
      "Electronic Frontier Foundation\n",
      "\n",
      "↪ \n",
      "260 miles east of Berlin\n",
      "\n",
      "↪ \n",
      "Laszlo Babai and Eugene Luks\n",
      "\n",
      "↪ \n",
      "Archipelago-like estuary\n",
      "\n",
      "↪ \n",
      "Courtyard adjoining the Assembly Hall\n",
      "\n",
      "↪ \n",
      "The Ruhr provides the region with drinking water.\n",
      "\n",
      "↪ \n",
      "Strengthening the role of backbenchers in their scrutiny of the government.\n",
      "\n",
      "↪ \n",
      "Curving parabolic path in the same direction as the motion of the vehicle.\n",
      "\n",
      "↪ \n",
      "Asynchronously using first-in, first-out buffering\n",
      "\n",
      "↪ \n",
      "Last Glacial Maximum (LGM) and subsequent deglaciation\n",
      "\n",
      "↪ \n",
      "The MSPs elect one MSP to serve as Presiding Officer.\n",
      "\n",
      "↪ \n",
      "Granted the Huguenots equality with Catholics and a degree of religious and political freedom within their domains.\n",
      "\n",
      "↪ \n",
      "Céloron informed British merchants and fur-traders of French claims and told them to leave.\n",
      "\n",
      "↪ \n",
      "A citizen or company can invoke a Directive, not just in a dispute with a public authority, but in a dispute with another citizen or company.\n",
      "\n",
      "↪ \n",
      "Colloblasts are specialized mushroom-shaped cells in the outer layer of the epidermis, densely covering the tentacles and tentilla of cydippid ctenophores, that capture prey by sticking to it.\n",
      "\n",
      "↪ \n",
      "2001 study found 1 square kilometer (62 acres)\n",
      "\n",
      "### INPUT\n",
      "Context:\n",
      "The biodiversity of plant species is the highest on Earth with one 2001 study finding a quarter square kilometer (62 acres) of Ecuadorian rainforest supports more than 1,100 tree species. A study in 1999 found one square kilometer (247 acres) of Amazon rainforest can contain about 90,790 tonnes of living plants. The average plant biomass is estimated at 356 ± 47 tonnes per hectare. To date, an estimated 438,000 species of plants of economic and social interest have been registered in the region with many more remaining to be discovered or catalogued. The total number of tree species in the region is estimated at 16,000.\n",
      "\n",
      "Question:\n",
      "What is the average plant biomass per hectare?\n",
      "\n",
      "### OUTPUT\n",
      "Answer:\n",
      "356 ± 47 tonnes per hectare\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cli = InferenceEngineClient()\n",
    "\n",
    "batch_results = cli.completion(queries[0], temperature=0.1)\n",
    "\n",
    "\n",
    "for inp, out in zip(queries[0], batch_results):\n",
    "    print(f\"↪ {out}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
