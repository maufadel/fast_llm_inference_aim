{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c8f55e",
   "metadata": {},
   "source": [
    "### RQ1 - Comparing different quantization levels\n",
    "\n",
    "**Quantization Dimension**\n",
    "\n",
    "How does quantization in different models and architectures affect system and task-specific metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef740d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 15:48:54.090065: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747064934.110013  260255 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747064934.115811  260255 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747064934.135655  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135683  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135685  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135687  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-12 15:48:54.141568: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 15:48:57 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-12 15:48:57 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-05-12 15:49:00,664] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_benchmark(backend, model_name, task, base_path, samples=500, verbose=False, batch_size=100):\n",
    "    print(f\"Running benchmark for {model_name} with {backend} on {task}\")\n",
    "    try:\n",
    "        bm = ModelBenchmark(\n",
    "            backend=backend,\n",
    "            model_name=model_name,\n",
    "            model_path=f\"{base_path}/models/{model_name}\",\n",
    "            base_path=base_path,\n",
    "            task=task,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        bm.run(samples=samples, batch_size=batch_size)\n",
    "        bm.close()\n",
    "        del bm\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"✅ Completed: {model_name} | {backend} | {task}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed: {model_name} | {backend} | {task} -- {e}\")\n",
    "        torch.cuda.empty_cache()  # ensure no memory leak on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac63b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference/\"\n",
    "\n",
    "backends = [\"vllm\"] #, \"huggingface\",\"deepspeed_mii\", \"llama.cpp\"]\n",
    "models   = [\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.1-8B-Instruct-4bit\",\n",
    "    \"llama-3.1-8B-Instruct-8bit\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "    \"llama-3.2-3b-instruct-4bit\",\n",
    "    \"llama-3.2-1b-instruct-4bit\",\n",
    "    \"llama-3.2-3b-instruct-8bit\",\n",
    "    \"llama-3.2-1b-instruct-8bit\",\n",
    "   \n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-7B-Instruct-4bit\",\n",
    "    \"Qwen2.5-7B-Instruct-8bit\", # some weird error\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct-4bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-3B-Instruct-8bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-8bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-8bit\",\n",
    "\n",
    "\n",
    "    \"gemma-2-9b-it-bnb4\",\n",
    "    \"gemma-2-9b-it-8bit\",\n",
    "    \"gemma-2-9b-it\", # too large\n",
    "    \"gemma-2-2b-it-4bit\",\n",
    "    \"gemma-2-2b-it-8bit\",\n",
    "    \"gemma-2-2b-it\",\n",
    "]\n",
    "\n",
    "tasks    = [\"summarization\", \"qa\", \"sql\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb85145",
   "metadata": {},
   "source": [
    "first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for backend in backends:\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=100,\n",
    "                verbose=False,\n",
    "                batch_size=100,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c7713",
   "metadata": {},
   "source": [
    "check if anything is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a66e3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models with missing files:\n",
      "Qwen2.5-7B-Instruct-8bit\n",
      "gemma-2-9b-it\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define your parameters\n",
    "backends = [\"vllm\"]\n",
    "models = [\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.1-8B-Instruct-4bit\",\n",
    "    \"llama-3.1-8B-Instruct-8bit\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "    \"llama-3.2-3b-instruct-4bit\",\n",
    "    \"llama-3.2-1b-instruct-4bit\",\n",
    "    \"llama-3.2-3b-instruct-8bit\",\n",
    "    \"llama-3.2-1b-instruct-8bit\",\n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-7B-Instruct-4bit\",\n",
    "    \"Qwen2.5-7B-Instruct-8bit\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct-4bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-3B-Instruct-8bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-8bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-8bit\",\n",
    "    \"gemma-2-9b-it-4bit\",\n",
    "    \"gemma-2-9b-it-8bit\",\n",
    "    \"gemma-2-9b-it\",\n",
    "    \"gemma-2-2b-it-4bit\",\n",
    "    \"gemma-2-2b-it-8bit\",\n",
    "    \"gemma-2-2b-it\",\n",
    "]\n",
    "tasks = [\"summarization\", \"qa\", \"sql\"]\n",
    "\n",
    "results_dir = \"./results/experiment_1/\"\n",
    "\n",
    "missing_models = set()\n",
    "\n",
    "for backend in backends:\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            filename = f\"{backend}_{model}_{task}.csv\"\n",
    "            filepath = os.path.join(results_dir, filename)\n",
    "            if not os.path.exists(filepath):\n",
    "                missing_models.add(model)\n",
    "                break  # No need to check more tasks if one is missing\n",
    "\n",
    "# Print models with missing files\n",
    "if missing_models:\n",
    "    print(\"Models with missing files:\")\n",
    "    for model in sorted(missing_models):\n",
    "        print(model)\n",
    "else:\n",
    "    print(\"✅ All models are complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e8200",
   "metadata": {},
   "source": [
    "try again with the missing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for backend in backends:\n",
    "    for model in list(missing_models):\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=500,\n",
    "                verbose=False,\n",
    "                batch_size=100,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2610ec",
   "metadata": {},
   "source": [
    "### RQ2 - Comparing different inference engines\n",
    "\n",
    "**Framework Dimension** \n",
    "\n",
    "Which inference framework (Transformers, vLLM, DeepSpeed MII,172\n",
    "LMDeploy, llama.cpp) strikes the best balance between system resource usage (e.g., GPU173\n",
    "utilization, joules/token) and system performance (tokens/s)?174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference/models\"\n",
    "\n",
    "backends = [\"vllm\", \"huggingface\", \"llama.cpp\"] #,\"deepspeed_mii\", \"huggingface\"]\n",
    "\n",
    "models   = [\n",
    "    \"gemma-2-9b-it\", \n",
    "    \"gemma-2-2b-it\",\n",
    "\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "   \n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "]\n",
    "\n",
    "tasks    = [\"summarization\", \"qa\", \"sql\",]\n",
    "\n",
    "for backend in backends:\n",
    "    if backend == \"llama.cpp\":\n",
    "        models = [\n",
    "            \"gemma-2-2b-it-fp16.gguf\",\n",
    "            \"gemma-2-9b-it-fp16.gguf\",\n",
    "\n",
    "            \"llama-3.1-8B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "            \n",
    "            \"qwen2.5-0.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-1.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-3b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-7B-instruct-fp16.gguf\",\n",
    "        ]\n",
    "        \n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=100,\n",
    "                verbose=False,\n",
    "                batch_size=20,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f940b1",
   "metadata": {},
   "source": [
    "#### RQ 3 - Comparing different use cases\n",
    "\n",
    "**Scenario/Workload Dimension**\n",
    "\n",
    "How do locally deployed LLMs and inference backends\n",
    "perform and scale across the three dominant inference scenarios—single - stream (single user),\n",
    "batched offline processing, and multi- user server workloads? Do system metrics – throughput,\n",
    "GPU utilization, joules/token — evolve as the average number of queries per second varies\n",
    "over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d90de05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 03:42:40.554419: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747366960.579138 1160723 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747366960.586688 1160723 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747366960.607277 1160723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747366960.607300 1160723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747366960.607303 1160723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747366960.607305 1160723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-16 03:42:40.613899: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-16 03:42:48 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-16 03:42:49 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-05-16 03:42:55,873] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from benchmark.benchmark import ModelBenchmark\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_benchmark(\n",
    "    backend: str,\n",
    "    model_name: str,\n",
    "    task: str,\n",
    "    base_path: str,\n",
    "    scenario: str = \"batch\",            # \"single\", \"batch\", or \"server\"\n",
    "    run_time: float = None,             # only for server: total time in seconds\n",
    "    requests_per_sec: float = None,     # only for server: λ (req/s)\n",
    "    batch_size: int = 100,              # only for batch\n",
    "    max_batch_size: int = None,         # only for server: cap per-batch size\n",
    "    sample_interval: float = 0.1,       # telemetry interval (s)\n",
    "    export_path: Optional[str] = None,  # custom export path for server scenario\n",
    "    verbose: bool = False\n",
    "):\n",
    "    print(f\"Running benchmark for {model_name} with {backend} on {task} [{scenario}]\")\n",
    "    try:\n",
    "        bm = ModelBenchmark(\n",
    "            backend=backend,\n",
    "            model_name=model_name,\n",
    "            model_path=f\"{base_path}/models/{model_name}\",\n",
    "            base_path=base_path,\n",
    "            task=task,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        if scenario == \"server\":\n",
    "            assert run_time is not None,    \"Must set run_time in server mode\"\n",
    "            assert requests_per_sec is not None, \"Must set requests_per_sec in server mode\"\n",
    "            df = bm.run(\n",
    "                scenario=\"server\",\n",
    "                run_time=run_time,\n",
    "                requests_per_sec=requests_per_sec,\n",
    "                sample_interval=sample_interval,\n",
    "                max_batch_size=max_batch_size,\n",
    "                export_path=export_path\n",
    "            )\n",
    "\n",
    "        elif scenario == \"single\":\n",
    "            df = bm.run(\n",
    "                samples=100,        # samples ignored\n",
    "                batch_size=1,\n",
    "                scenario=\"single\",\n",
    "                sample_interval=sample_interval,\n",
    "                export_path=export_path\n",
    "            )\n",
    "\n",
    "        elif scenario == \"batch\":\n",
    "            df = bm.run(\n",
    "                samples=100,        # samples ignored\n",
    "                batch_size=batch_size,\n",
    "                scenario=\"batch\",\n",
    "                sample_interval=sample_interval,\n",
    "                export_path=export_path\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scenario: {scenario}\")\n",
    "\n",
    "        bm.close()\n",
    "        del bm\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"✅ Completed: {model_name} | {backend} | {task} | {scenario}\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed: {model_name} | {backend} | {task} | {scenario} -- {e}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb1843",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference\"\n",
    "backends = [\"vllm\", \"llama.cpp\", \"huggingface\"]\n",
    "tasks    = [\"sql\"]\n",
    "server_rps      = [1, 2, 4, 8]\n",
    "run_time        = 120.0     # seconds\n",
    "sample_interval = 0.05      # s\n",
    "max_batch_size  = 64        # cap per batch\n",
    "\n",
    "for backend in backends:\n",
    "    if backend == \"llama.cpp\":\n",
    "        models = [\n",
    "            \"gemma-2-2b-it-fp16.gguf\",\n",
    "            \"gemma-2-9b-it-fp16.gguf\",\n",
    "            \"llama-3.1-8B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "            \"qwen2.5-0.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-1.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-3b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-7B-instruct-fp16.gguf\",\n",
    "        ]\n",
    "    else:\n",
    "        models = [\n",
    "            \"gemma-2-2b-it\",\n",
    "            \"llama-3.1-8B-Instruct\",\n",
    "            \"llama-3.2-3b-instruct\",\n",
    "            \"llama-3.2-1b-instruct\",\n",
    "            \"Qwen2.5-7B-Instruct\",\n",
    "            \"Qwen2.5-3B-Instruct\",\n",
    "            \"Qwen2.5-1.5B-Instruct\",\n",
    "            \"Qwen2.5-0.5B-Instruct\",\n",
    "        ]\n",
    "        if backend != \"vllm\":\n",
    "            models.append(\"gemma-2-9b-it\") # too large for vllm\n",
    "\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            for rps in server_rps:\n",
    "                export_path = f\"{base_path}/results/{backend}_{model}_{task}_{rps}QPS_{int(run_time)}s_server.csv\"\n",
    "                print(f\"→ {backend} | {model} | {task} @ {rps} QPS for {run_time}s -> {export_path}\")\n",
    "                run_benchmark(\n",
    "                    backend=backend,\n",
    "                    model_name=model,\n",
    "                    task=task,\n",
    "                    base_path=base_path,\n",
    "                    scenario=\"server\",\n",
    "                    run_time=run_time,\n",
    "                    requests_per_sec=rps,\n",
    "                    sample_interval=sample_interval,\n",
    "                    max_batch_size=max_batch_size,\n",
    "                    export_path=export_path,\n",
    "                    verbose=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a94ed06",
   "metadata": {},
   "source": [
    "part 2 - batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f16559",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference\"\n",
    "backends  = [\"huggingface\"] #\"vllm\", \"llama.cpp\", \n",
    "tasks     = [\"summarization\"]\n",
    "batch_sizes = [1, 8, 16, 32, 64]   # ← as requested\n",
    "sample_interval = 0.05             # s\n",
    "max_batch_size  = 64               # keep the same cap\n",
    "\n",
    "for backend in backends:\n",
    "    if backend == \"llama.cpp\":\n",
    "        models = [\n",
    "            \"gemma-2-2b-it-fp16.gguf\",\n",
    "            \"gemma-2-9b-it-fp16.gguf\",\n",
    "            \"llama-3.1-8B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "            \"qwen2.5-0.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-1.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-3b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-7B-instruct-fp16.gguf\",\n",
    "        ]\n",
    "    else:\n",
    "        models = [\n",
    "            #\"gemma-2-2b-it\",\n",
    "            \"llama-3.1-8B-Instruct\",\n",
    "            \"llama-3.2-3b-instruct\",\n",
    "            \"llama-3.2-1b-instruct\",\n",
    "            \"Qwen2.5-7B-Instruct\",\n",
    "            \"Qwen2.5-3B-Instruct\",\n",
    "            \"Qwen2.5-1.5B-Instruct\",\n",
    "            \"Qwen2.5-0.5B-Instruct\",\n",
    "        ]\n",
    "        if backend != \"vllm\":\n",
    "            models.append(\"gemma-2-9b-it\")   # too large for vllm\n",
    "\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            for bs in batch_sizes:\n",
    "                export_path = (\n",
    "                    f\"{base_path}/results/\"\n",
    "                    f\"{backend}_{model}_{task}_{bs}batch.csv\"\n",
    "                )\n",
    "                print(f\"→ {backend} | {model} | {task} @ batch={bs} -> {export_path}\")\n",
    "                run_benchmark(\n",
    "                    backend=backend,\n",
    "                    model_name=model,\n",
    "                    task=task,\n",
    "                    base_path=base_path,\n",
    "                    scenario=\"batch\",\n",
    "                    batch_size=bs,\n",
    "                    sample_interval=sample_interval,\n",
    "                    max_batch_size=max_batch_size,\n",
    "                    export_path=export_path,\n",
    "                    verbose=False,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11221c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-19 08:54:15 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 08:54:16.711341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747644856.737085   58511 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747644856.744882   58511 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747644856.767440   58511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747644856.767460   58511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747644856.767463   58511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747644856.767465   58511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-19 08:54:16.774362: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-19 08:54:35 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 05-19 08:54:37 [config.py:830] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-19 08:54:37 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-19 08:54:38 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', speculative_config=None, tokenizer='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-19 08:54:39 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x6ffd3be5c590>\n",
      "INFO 05-19 08:54:40 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-19 08:54:40 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-19 08:54:40 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-19 08:54:40 [gpu_model_runner.py:1329] Starting to load model /home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit...\n",
      "INFO 05-19 08:54:41 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c9487da48d448b995ce9ba8d5f7432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93d19597f79491cb38fd8c2d617e011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-19 08:54:45 [gpu_model_runner.py:1347] Model loading took 5.3132 GiB and 4.338470 seconds\n",
      "INFO 05-19 08:54:55 [backends.py:420] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/a89f85ea99/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-19 08:54:55 [backends.py:430] Dynamo bytecode transform time: 9.70 s\n",
      "INFO 05-19 08:55:04 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 6.534 s\n",
      "INFO 05-19 08:55:07 [monitor.py:33] torch.compile takes 9.70 s in total\n",
      "INFO 05-19 08:55:10 [kv_cache_utils.py:634] GPU KV cache size: 106,880 tokens\n",
      "INFO 05-19 08:55:10 [kv_cache_utils.py:637] Maximum concurrency for 8,192 tokens per request: 13.05x\n",
      "INFO 05-19 08:56:21 [gpu_model_runner.py:1686] Graph capturing finished in 71 secs, took 1.54 GiB\n",
      "INFO 05-19 08:56:21 [core.py:159] init engine (profile, create kv cache, warmup model) took 95.70 seconds\n",
      "INFO 05-19 08:56:21 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "from benchmark.backends.vllm_backend import VLLMBackend\n",
    "\n",
    "model = VLLMBackend(\"/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit\")\n",
    "model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec34ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(\"What is the purpose of life?\", perplexity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee5b9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-18 11:26:48 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 11:26:49.212117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747567609.229561   12031 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747567609.235248   12031 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747567609.250681   12031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747567609.250693   12031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747567609.250695   12031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747567609.250696   12031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-18 11:26:49.256273: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and setup\n",
    "import os\n",
    "import math\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# (Optional) adjust your model path here\n",
    "MODEL_PATH = \"/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit\"\n",
    "\n",
    "# Cell 2: Load model and define prompts\n",
    "model = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    max_model_len=4096,\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"What is the purpose of life?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd6017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acccb472a5d421db912f2ddeff6bdc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 3: Configure SamplingParams for logprobs & perplexity\n",
    "params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    max_tokens=32,\n",
    "    logprobs=1,\n",
    "    prompt_logprobs=1\n",
    ")\n",
    "\n",
    "# Cell 4: Run generation and display results in a table\n",
    "outputs = model.generate(prompts, params)\n",
    "\n",
    "for i, gen_out in enumerate(outputs):\n",
    "    sample  = gen_out.outputs[0]\n",
    "    text    = sample.text.lstrip()\n",
    "    lp_dict = sample.logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a6b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{2209: Logprob(logprob=-1.4004144668579102, rank=1, decoded_token='ĠIs')},\n",
       " {433: Logprob(logprob=-0.17977960407733917, rank=1, decoded_token='Ġit')},\n",
       " {311: Logprob(logprob=-0.0869283527135849, rank=1, decoded_token='Ġto')},\n",
       " {1505: Logprob(logprob=-0.9659126996994019, rank=1, decoded_token='Ġfind')},\n",
       " {23871: Logprob(logprob=-0.04803086444735527, rank=1, decoded_token='Ġhappiness')},\n",
       " {11: Logprob(logprob=-0.058653172105550766, rank=1, decoded_token=',')},\n",
       " {311: Logprob(logprob=-0.8425228595733643, rank=1, decoded_token='Ġto')},\n",
       " {11322: Logprob(logprob=-0.7215710878372192, rank=1, decoded_token='Ġachieve')},\n",
       " {2450: Logprob(logprob=-0.043824948370456696, rank=1, decoded_token='Ġsuccess')},\n",
       " {11: Logprob(logprob=-0.0013250865740701556, rank=1, decoded_token=',')},\n",
       " {477: Logprob(logprob=-0.7743627429008484, rank=2, decoded_token='Ġor'),\n",
       "  311: Logprob(logprob=-0.6181127429008484, rank=1, decoded_token='Ġto')},\n",
       " {311: Logprob(logprob=-0.031453102827072144, rank=1, decoded_token='Ġto')},\n",
       " {1304: Logprob(logprob=-0.7800344228744507, rank=1, decoded_token='Ġmake')},\n",
       " {264: Logprob(logprob=-0.018590614199638367, rank=1, decoded_token='Ġa')},\n",
       " {6811: Logprob(logprob=-0.16793595254421234, rank=1, decoded_token='Ġdifference')},\n",
       " {304: Logprob(logprob=-0.1038089394569397, rank=1, decoded_token='Ġin')},\n",
       " {279: Logprob(logprob=-0.0009804924484342337, rank=1, decoded_token='Ġthe')},\n",
       " {1917: Logprob(logprob=-0.0022238779347389936, rank=1, decoded_token='Ġworld')},\n",
       " {30: Logprob(logprob=-0.11063252389431, rank=1, decoded_token='?')},\n",
       " {578: Logprob(logprob=-1.3353930711746216, rank=1, decoded_token='ĠThe')},\n",
       " {4320: Logprob(logprob=-0.25850245356559753, rank=1, decoded_token='Ġanswer')},\n",
       " {311: Logprob(logprob=-0.9216098785400391, rank=1, decoded_token='Ġto')},\n",
       " {420: Logprob(logprob=-0.04812448099255562, rank=1, decoded_token='Ġthis')},\n",
       " {3488: Logprob(logprob=-0.02162298373878002, rank=1, decoded_token='Ġquestion')},\n",
       " {374: Logprob(logprob=-0.520580530166626, rank=1, decoded_token='Ġis')},\n",
       " {44122: Logprob(logprob=-1.261415958404541, rank=1, decoded_token='Ġsubjective')},\n",
       " {323: Logprob(logprob=-0.049682993441820145, rank=1, decoded_token='Ġand')},\n",
       " {649: Logprob(logprob=-0.46952304244041443, rank=1, decoded_token='Ġcan')},\n",
       " {13592: Logprob(logprob=-0.009865455329418182, rank=1, decoded_token='Ġvary')},\n",
       " {19407: Logprob(logprob=-0.37328964471817017, rank=1, decoded_token='Ġgreatly')},\n",
       " {505: Logprob(logprob=-0.14553403854370117, rank=1, decoded_token='Ġfrom')},\n",
       " {1732: Logprob(logprob=-0.0395626574754715, rank=1, decoded_token='Ġperson')}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb6b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975be3f036034fe9abea836af4f1d5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prompt 1: The quick brown fox jumps over the lazy dog. ===\n",
      "Generated: The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick\n",
      "\n",
      "       Token |  LogProb |      PPL\n",
      "----------------------------------\n",
      "        ĠThe |  -0.9727 |   2.6450\n",
      "      Ġquick |  -0.3296 |   1.3905\n",
      "      Ġbrown |  -0.0029 |   1.0029\n",
      "        Ġfox |  -0.0016 |   1.0016\n",
      "      Ġjumps |  -0.0055 |   1.0055\n",
      "       Ġover |  -0.0005 |   1.0005\n",
      "        Ġthe |  -0.0005 |   1.0005\n",
      "       Ġlazy |  -0.0005 |   1.0005\n",
      "        Ġdog |  -0.0004 |   1.0004\n",
      "           . |  -0.3360 |   1.3993\n",
      "        ĠThe |  -0.0372 |   1.0379\n",
      "      Ġquick |  -0.0015 |   1.0015\n",
      "      Ġbrown |  -0.0012 |   1.0012\n",
      "        Ġfox |  -0.0007 |   1.0007\n",
      "      Ġjumps |  -0.0013 |   1.0013\n",
      "       Ġover |  -0.0008 |   1.0008\n",
      "        Ġthe |  -0.0010 |   1.0010\n",
      "       Ġlazy |  -0.0010 |   1.0010\n",
      "        Ġdog |  -0.0005 |   1.0005\n",
      "           . |  -0.2609 |   1.2981\n",
      "        ĠThe |  -0.0465 |   1.0476\n",
      "      Ġquick |  -0.0021 |   1.0021\n",
      "      Ġbrown |  -0.0006 |   1.0006\n",
      "        Ġfox |  -0.0014 |   1.0014\n",
      "      Ġjumps |  -0.0008 |   1.0008\n",
      "       Ġover |  -0.0002 |   1.0002\n",
      "        Ġthe |  -0.0008 |   1.0008\n",
      "       Ġlazy |  -0.0007 |   1.0007\n",
      "        Ġdog |  -0.0004 |   1.0004\n",
      "           . |  -0.1384 |   1.1484\n",
      "        ĠThe |  -0.0204 |   1.0206\n",
      "      Ġquick |  -0.0011 |   1.0011\n",
      "\n",
      "=== Prompt 2: What is the purpose of life? ===\n",
      "Generated: Is it to find happiness, to achieve success, to make a difference, or to fulfill a purpose? The answer to this question is different for each individual,\n",
      "\n",
      "       Token |  LogProb |      PPL\n",
      "----------------------------------\n",
      "         ĠIs |  -1.4004 |   4.0569\n",
      "         Ġit |  -0.1798 |   1.1970\n",
      "         Ġto |  -0.0869 |   1.0908\n",
      "       Ġfind |  -0.9659 |   2.6272\n",
      "  Ġhappiness |  -0.0480 |   1.0492\n",
      "           , |  -0.0587 |   1.0604\n",
      "         Ġto |  -0.8425 |   2.3222\n",
      "    Ġachieve |  -0.7216 |   2.0577\n",
      "    Ġsuccess |  -0.0438 |   1.0448\n",
      "           , |  -0.0013 |   1.0013\n",
      "         Ġto |  -0.6181 |   1.8554\n",
      "       Ġmake |  -0.8055 |   2.2377\n",
      "          Ġa |  -0.0282 |   1.0286\n",
      " Ġdifference |  -0.0444 |   1.0454\n",
      "           , |  -0.5264 |   1.6929\n",
      "         Ġor |  -0.0248 |   1.0251\n",
      "  Ġsomething |  -0.7462 |   2.1090\n",
      "    Ġfulfill |  -1.0393 |   2.8273\n",
      "          Ġa |  -1.0664 |   2.9050\n",
      "    Ġpurpose |  -0.8271 |   2.2867\n",
      "           ? |  -0.4777 |   1.6124\n",
      "        ĠThe |  -0.9232 |   2.5173\n",
      "     Ġanswer |  -0.2054 |   1.2280\n",
      "         Ġto |  -1.1311 |   3.0989\n",
      "       Ġthis |  -0.0250 |   1.0253\n",
      "   Ġquestion |  -0.0269 |   1.0272\n",
      "         Ġis |  -0.6289 |   1.8755\n",
      " Ġsubjective |  -1.4515 |   4.2693\n",
      "        Ġfor |  -0.0012 |   1.0012\n",
      "       Ġeach |  -0.1205 |   1.1281\n",
      " Ġindividual |  -0.4515 |   1.5707\n",
      "           , |  -0.2584 |   1.2949\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Run generation and display results in a table plus sequence PPL\n",
    "outputs = model.generate(prompts, params)\n",
    "\n",
    "for i, gen_out in enumerate(outputs):\n",
    "    sample    = gen_out.outputs[0]\n",
    "    text      = sample.text.lstrip()\n",
    "    lp_list   = sample.logprobs            # list of dicts\n",
    "    token_ids = sample.token_ids\n",
    "\n",
    "    # 1) Extract the chosen-token strings & logprobs\n",
    "    tokens, logps = [], []\n",
    "    for entry in lp_list:\n",
    "        # each entry is {token_id: Logprob(...), ...}\n",
    "        for tid, lp_obj in entry.items():\n",
    "            if lp_obj.rank == 1:\n",
    "                tokens.append(lp_obj.decoded_token)\n",
    "                logps.append(lp_obj.logprob)\n",
    "                break\n",
    "\n",
    "    # 2) Compute per-token perplexity\n",
    "    ppl = [math.exp(-lp) for lp in logps]\n",
    "\n",
    "    # 3) Print per-token table\n",
    "    print(f\"\\n=== Prompt {i+1}: {prompts[i]} ===\")\n",
    "    print(f\"Generated: {text}\\n\")\n",
    "    print(f\"{'Token':>12} | {'LogProb':>8} | {'PPL':>8}\")\n",
    "    print(\"-\" * 34)\n",
    "    for tok, lp, p in zip(tokens, logps, ppl):\n",
    "        print(f\"{tok:>12} | {lp:8.4f} | {p:8.4f}\")\n",
    "\n",
    "    # 4) Compute sequence-level perplexity\n",
    "    ppl_seq = math.exp(- sum(logps) / len(logps))\n",
    "    print(f\"\\nSequence-level Perplexity: {ppl_seq:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c265094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-25 11:04:42 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 11:04:42.809369: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748171082.830786   43313 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748171082.836811   43313 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748171082.853168   43313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748171082.853191   43313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748171082.853193   43313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748171082.853195   43313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-25 11:04:42.858875: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import torch\n",
    "\n",
    "bm = ModelBenchmark(\n",
    "    backend=\"huggingface\",\n",
    "    model_name=\"llama-3.2-3b-instruct\",\n",
    "    model_path=\"/home/ubuntu/fast_llm_inference/models/llama-3.2-3b-instruct\",\n",
    "    base_path=\"/home/ubuntu/fast_llm_inference/\",\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a2de41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ccb55231f5480f9aefd35802f19aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Batch Run Report ===\n",
      "   startup_time_sec  load_model_time_sec  ttft_sec  cold_start_sec  \\\n",
      "0            0.0028               3.6709    0.1596          3.8332   \n",
      "\n",
      "              model_name  model_size_mb      backend  batch_size  num_queries  \\\n",
      "0  llama-3.2-3b-instruct    6138.690614  huggingface           8           16   \n",
      "\n",
      "   batch_id  ...   avg_ATL    avg_GL  avg_TPS  avg_SPS  avg_energy_per_token  \\\n",
      "0         0  ...  0.044919  2.943486    22.27  1.08375              3.162506   \n",
      "\n",
      "   avg_energy_per_sentence  avg_estimated_cost_usd  avg_ROUGE-1  avg_ROUGE-2  \\\n",
      "0                66.314669                0.002354     0.362087     0.118788   \n",
      "\n",
      "   avg_ROUGE-L  \n",
      "0     0.224013  \n",
      "\n",
      "[1 rows x 33 columns]\n",
      "\n",
      "=== Per-Query Details ===\n",
      "   batch_id                                             prompt  \\\n",
      "0         0  ### SYSTEM\\nYou are a news-summarization assis...   \n",
      "1         0  ### SYSTEM\\nYou are a news-summarization assis...   \n",
      "2         0  ### SYSTEM\\nYou are a news-summarization assis...   \n",
      "3         0  ### SYSTEM\\nYou are a news-summarization assis...   \n",
      "4         0  ### SYSTEM\\nYou are a news-summarization assis...   \n",
      "\n",
      "                                    generated_answer  \\\n",
      "0  President Barack Obama honored the New England...   \n",
      "1  Rangers FC has promised to investigate claims ...   \n",
      "2  Adam Gadahn, a 36-year-old American who was kn...   \n",
      "3  Former Liverpool players John Barnes and Jamie...   \n",
      "4  John Higgins and Graeme Dott led the Scottish ...   \n",
      "\n",
      "                                    reference_answer       ATL        GL  \\\n",
      "0  Brady cited 'prior family commitments' in bowi...  0.045718  3.017364   \n",
      "1  Reports emerged on social media  suggesting Mi...  0.045718  2.925928   \n",
      "2  In his final known video, Adam Gadahn called f...  0.045718  2.605905   \n",
      "3  John Barnes appeared as a guest on Sky's A Lea...  0.045718  3.565975   \n",
      "4  John Higgins beats Robert Milkins 10-5 to reac...  0.045718  3.063081   \n",
      "\n",
      "     TPS   SPS  energy_per_token  energy_per_sentence  estimated_cost_usd  \\\n",
      "0  21.87  1.33          3.217368            69.109068            0.002454   \n",
      "1  21.87  1.03          3.217368            69.109068            0.002454   \n",
      "2  21.87  1.15          3.217368            69.109068            0.002454   \n",
      "3  21.87  0.84          3.217368            69.109068            0.002454   \n",
      "4  21.87  0.98          3.217368            69.109068            0.002454   \n",
      "\n",
      "    ROUGE-1   ROUGE-2   ROUGE-L  \n",
      "0  0.208333  0.063830  0.125000  \n",
      "1  0.333333  0.094340  0.185185  \n",
      "2  0.462963  0.188679  0.314815  \n",
      "3  0.414815  0.180451  0.237037  \n",
      "4  0.327586  0.140351  0.155172  \n"
     ]
    }
   ],
   "source": [
    "run_report, details_df = bm.run(\n",
    "    task=\"summarization\",\n",
    "    scenario=\"batch\",\n",
    "    samples=16,\n",
    "    batch_size=8,\n",
    "    sample_interval=0.1,\n",
    "    quality_metric=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f21c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "startup_time_sec              0.002800\n",
       "load_model_time_sec           3.670900\n",
       "ttft_sec                      0.159600\n",
       "cold_start_sec                3.833200\n",
       "model_size_mb              6138.690614\n",
       "batch_size                    8.000000\n",
       "num_queries                  16.000000\n",
       "batch_id                      0.000000\n",
       "batch_time_s                 24.550367\n",
       "batch_tokens                537.000000\n",
       "batch_sentences              25.000000\n",
       "avg_gpu_mem_mb             7213.020000\n",
       "peak_gpu_mem_mb            7237.880000\n",
       "overhead_mb                1099.190000\n",
       "avg_gpu_util_pct             90.580000\n",
       "peak_gpu_util_pct           100.000000\n",
       "avg_cpu_util_pct              6.220000\n",
       "peak_cpu_util_pct             6.840000\n",
       "avg_power_w                  70.370000\n",
       "peak_power_w                 73.810000\n",
       "total_energy_wh               0.479924\n",
       "avg_ATL                       0.044919\n",
       "avg_GL                        2.943486\n",
       "avg_TPS                      22.270000\n",
       "avg_SPS                       1.083750\n",
       "avg_energy_per_token          3.162506\n",
       "avg_energy_per_sentence      66.314669\n",
       "avg_estimated_cost_usd        0.002354\n",
       "avg_ROUGE-1                   0.362087\n",
       "avg_ROUGE-2                   0.118788\n",
       "avg_ROUGE-L                   0.224013\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "run_report.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e91718e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>ATL</th>\n",
       "      <th>GL</th>\n",
       "      <th>TPS</th>\n",
       "      <th>SPS</th>\n",
       "      <th>energy_per_token</th>\n",
       "      <th>energy_per_sentence</th>\n",
       "      <th>estimated_cost_usd</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>President Barack Obama honored the New England...</td>\n",
       "      <td>Brady cited 'prior family commitments' in bowi...</td>\n",
       "      <td>0.049031</td>\n",
       "      <td>3.236063</td>\n",
       "      <td>20.40</td>\n",
       "      <td>1.24</td>\n",
       "      <td>3.443882</td>\n",
       "      <td>73.974595</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>Rangers FC has promised to investigate claims ...</td>\n",
       "      <td>Reports emerged on social media  suggesting Mi...</td>\n",
       "      <td>0.049031</td>\n",
       "      <td>3.138000</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.96</td>\n",
       "      <td>3.443882</td>\n",
       "      <td>73.974595</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.094340</td>\n",
       "      <td>0.185185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>Adam Gadahn, a 36-year-old American who was kn...</td>\n",
       "      <td>In his final known video, Adam Gadahn called f...</td>\n",
       "      <td>0.049031</td>\n",
       "      <td>2.794781</td>\n",
       "      <td>20.40</td>\n",
       "      <td>1.07</td>\n",
       "      <td>3.443882</td>\n",
       "      <td>73.974595</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.314815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>Former Liverpool players John Barnes and Jamie...</td>\n",
       "      <td>John Barnes appeared as a guest on Sky's A Lea...</td>\n",
       "      <td>0.049031</td>\n",
       "      <td>3.824438</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.78</td>\n",
       "      <td>3.443882</td>\n",
       "      <td>73.974595</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.414815</td>\n",
       "      <td>0.180451</td>\n",
       "      <td>0.237037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>John Higgins and Graeme Dott led the Scottish ...</td>\n",
       "      <td>John Higgins beats Robert Milkins 10-5 to reac...</td>\n",
       "      <td>0.049031</td>\n",
       "      <td>3.285094</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.91</td>\n",
       "      <td>3.443882</td>\n",
       "      <td>73.974595</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.327586</td>\n",
       "      <td>0.140351</td>\n",
       "      <td>0.155172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>Arsenal will wear their yellow and blue away s...</td>\n",
       "      <td>Arsenal face Aston Villa in the FA Cup final a...</td>\n",
       "      <td>0.049031</td>\n",
       "      <td>3.579281</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.84</td>\n",
       "      <td>3.443882</td>\n",
       "      <td>73.974595</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.598540</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.262774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>Liverpool advanced to the FA Cup semi-finals w...</td>\n",
       "      <td>Coutinho hit the only goal of the game as Live...</td>\n",
       "      <td>0.049031</td>\n",
       "      <td>3.285094</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.91</td>\n",
       "      <td>3.443882</td>\n",
       "      <td>73.974595</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.217687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>West Coast Shaving created a project blending ...</td>\n",
       "      <td>Members of 30 rock band members merged into on...</td>\n",
       "      <td>0.049031</td>\n",
       "      <td>3.187031</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.94</td>\n",
       "      <td>3.443882</td>\n",
       "      <td>73.974595</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>A young mother was humiliated after a dental a...</td>\n",
       "      <td>Mother-of-one Tayler Chaice Buzbee called Aspe...</td>\n",
       "      <td>0.044328</td>\n",
       "      <td>3.368903</td>\n",
       "      <td>22.56</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.125693</td>\n",
       "      <td>63.889157</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.091429</td>\n",
       "      <td>0.203390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>A top breeder in Devon, Tony Tancock, had 12 p...</td>\n",
       "      <td>Heart-broken Tony Tancock, 56, has won prizes ...</td>\n",
       "      <td>0.044328</td>\n",
       "      <td>2.482350</td>\n",
       "      <td>22.56</td>\n",
       "      <td>1.21</td>\n",
       "      <td>3.125693</td>\n",
       "      <td>63.889157</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>0.389381</td>\n",
       "      <td>0.144144</td>\n",
       "      <td>0.265487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>English clubs have not claimed the Heineken Cu...</td>\n",
       "      <td>It has been eight years since an English club ...</td>\n",
       "      <td>0.044328</td>\n",
       "      <td>3.147265</td>\n",
       "      <td>22.56</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3.125693</td>\n",
       "      <td>63.889157</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>0.318841</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.188406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>A 27-year-old man, Alex Gray, lost 160lbs over...</td>\n",
       "      <td>It took Atlanta-based writer Alex Gray 20 mont...</td>\n",
       "      <td>0.044328</td>\n",
       "      <td>2.305039</td>\n",
       "      <td>22.56</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.125693</td>\n",
       "      <td>63.889157</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>0.252632</td>\n",
       "      <td>0.086022</td>\n",
       "      <td>0.168421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>A taxi driver, Mohammed Nisar, returned a bag ...</td>\n",
       "      <td>Mohammed Nisar had been taking Adrian Quinn to...</td>\n",
       "      <td>0.044328</td>\n",
       "      <td>3.102937</td>\n",
       "      <td>22.56</td>\n",
       "      <td>0.97</td>\n",
       "      <td>3.125693</td>\n",
       "      <td>63.889157</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>0.325203</td>\n",
       "      <td>0.115702</td>\n",
       "      <td>0.243902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>Anthony Doerr's novel \"All the Light We Cannot...</td>\n",
       "      <td>Anthony Doerr's \"All the Light We Cannot See\" ...</td>\n",
       "      <td>0.044328</td>\n",
       "      <td>2.571005</td>\n",
       "      <td>22.56</td>\n",
       "      <td>1.17</td>\n",
       "      <td>3.125693</td>\n",
       "      <td>63.889157</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>0.361446</td>\n",
       "      <td>0.197531</td>\n",
       "      <td>0.361446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>Aaron Hernandez, the former NFL player, was fo...</td>\n",
       "      <td>Roxanne Jones: Jury right to find Hernandez gu...</td>\n",
       "      <td>0.044328</td>\n",
       "      <td>2.039073</td>\n",
       "      <td>22.56</td>\n",
       "      <td>1.47</td>\n",
       "      <td>3.125693</td>\n",
       "      <td>63.889157</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>A study by Brown University found that playing...</td>\n",
       "      <td>Researchers at Brown University studied how pe...</td>\n",
       "      <td>0.044328</td>\n",
       "      <td>3.634869</td>\n",
       "      <td>22.56</td>\n",
       "      <td>0.83</td>\n",
       "      <td>3.125693</td>\n",
       "      <td>63.889157</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>0.346457</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.204724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch_id                                             prompt  \\\n",
       "0          0  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "1          0  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "2          0  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "3          0  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "4          0  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "5          0  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "6          0  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "7          0  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "8          1  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "9          1  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "10         1  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "11         1  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "12         1  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "13         1  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "14         1  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "15         1  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "\n",
       "                                     generated_answer  \\\n",
       "0   President Barack Obama honored the New England...   \n",
       "1   Rangers FC has promised to investigate claims ...   \n",
       "2   Adam Gadahn, a 36-year-old American who was kn...   \n",
       "3   Former Liverpool players John Barnes and Jamie...   \n",
       "4   John Higgins and Graeme Dott led the Scottish ...   \n",
       "5   Arsenal will wear their yellow and blue away s...   \n",
       "6   Liverpool advanced to the FA Cup semi-finals w...   \n",
       "7   West Coast Shaving created a project blending ...   \n",
       "8   A young mother was humiliated after a dental a...   \n",
       "9   A top breeder in Devon, Tony Tancock, had 12 p...   \n",
       "10  English clubs have not claimed the Heineken Cu...   \n",
       "11  A 27-year-old man, Alex Gray, lost 160lbs over...   \n",
       "12  A taxi driver, Mohammed Nisar, returned a bag ...   \n",
       "13  Anthony Doerr's novel \"All the Light We Cannot...   \n",
       "14  Aaron Hernandez, the former NFL player, was fo...   \n",
       "15  A study by Brown University found that playing...   \n",
       "\n",
       "                                     reference_answer       ATL        GL  \\\n",
       "0   Brady cited 'prior family commitments' in bowi...  0.049031  3.236063   \n",
       "1   Reports emerged on social media  suggesting Mi...  0.049031  3.138000   \n",
       "2   In his final known video, Adam Gadahn called f...  0.049031  2.794781   \n",
       "3   John Barnes appeared as a guest on Sky's A Lea...  0.049031  3.824438   \n",
       "4   John Higgins beats Robert Milkins 10-5 to reac...  0.049031  3.285094   \n",
       "5   Arsenal face Aston Villa in the FA Cup final a...  0.049031  3.579281   \n",
       "6   Coutinho hit the only goal of the game as Live...  0.049031  3.285094   \n",
       "7   Members of 30 rock band members merged into on...  0.049031  3.187031   \n",
       "8   Mother-of-one Tayler Chaice Buzbee called Aspe...  0.044328  3.368903   \n",
       "9   Heart-broken Tony Tancock, 56, has won prizes ...  0.044328  2.482350   \n",
       "10  It has been eight years since an English club ...  0.044328  3.147265   \n",
       "11  It took Atlanta-based writer Alex Gray 20 mont...  0.044328  2.305039   \n",
       "12  Mohammed Nisar had been taking Adrian Quinn to...  0.044328  3.102937   \n",
       "13  Anthony Doerr's \"All the Light We Cannot See\" ...  0.044328  2.571005   \n",
       "14  Roxanne Jones: Jury right to find Hernandez gu...  0.044328  2.039073   \n",
       "15  Researchers at Brown University studied how pe...  0.044328  3.634869   \n",
       "\n",
       "      TPS   SPS  energy_per_token  energy_per_sentence  estimated_cost_usd  \\\n",
       "0   20.40  1.24          3.443882            73.974595            0.002632   \n",
       "1   20.40  0.96          3.443882            73.974595            0.002632   \n",
       "2   20.40  1.07          3.443882            73.974595            0.002632   \n",
       "3   20.40  0.78          3.443882            73.974595            0.002632   \n",
       "4   20.40  0.91          3.443882            73.974595            0.002632   \n",
       "5   20.40  0.84          3.443882            73.974595            0.002632   \n",
       "6   20.40  0.91          3.443882            73.974595            0.002632   \n",
       "7   20.40  0.94          3.443882            73.974595            0.002632   \n",
       "8   22.56  1.19          3.125693            63.889157            0.002265   \n",
       "9   22.56  1.21          3.125693            63.889157            0.002265   \n",
       "10  22.56  0.95          3.125693            63.889157            0.002265   \n",
       "11  22.56  1.30          3.125693            63.889157            0.002265   \n",
       "12  22.56  0.97          3.125693            63.889157            0.002265   \n",
       "13  22.56  1.17          3.125693            63.889157            0.002265   \n",
       "14  22.56  1.47          3.125693            63.889157            0.002265   \n",
       "15  22.56  0.83          3.125693            63.889157            0.002265   \n",
       "\n",
       "     ROUGE-1   ROUGE-2   ROUGE-L  \n",
       "0   0.208333  0.063830  0.125000  \n",
       "1   0.333333  0.094340  0.185185  \n",
       "2   0.462963  0.188679  0.314815  \n",
       "3   0.414815  0.180451  0.237037  \n",
       "4   0.327586  0.140351  0.155172  \n",
       "5   0.598540  0.266667  0.262774  \n",
       "6   0.367347  0.068966  0.217687  \n",
       "7   0.395833  0.106383  0.291667  \n",
       "8   0.440678  0.091429  0.203390  \n",
       "9   0.389381  0.144144  0.265487  \n",
       "10  0.318841  0.044118  0.188406  \n",
       "11  0.252632  0.086022  0.168421  \n",
       "12  0.325203  0.115702  0.243902  \n",
       "13  0.361446  0.197531  0.361446  \n",
       "14  0.250000  0.000000  0.159091  \n",
       "15  0.346457  0.112000  0.204724  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d390ea80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0147056fdc420cb74dcd4642d665f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m run_report2, details_df2 = \u001b[43mbm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mqa\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mserver\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_time\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequests_per_sec\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquality_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m bm.close()\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m bm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fast_llm_inference/benchmark/benchmark.py:491\u001b[39m, in \u001b[36mModelBenchmark.run\u001b[39m\u001b[34m(self, task, scenario, samples, batch_size, run_time, requests_per_sec, max_batch_size, sample_interval, quality_metric)\u001b[39m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, *, task, scenario, samples=\u001b[38;5;28;01mNone\u001b[39;00m, batch_size=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    486\u001b[39m         run_time=\u001b[38;5;28;01mNone\u001b[39;00m, requests_per_sec=\u001b[38;5;28;01mNone\u001b[39;00m, max_batch_size=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    487\u001b[39m         sample_interval=\u001b[32m0.1\u001b[39m, quality_metric=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    488\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    Public entrypoint. Dispatches to the unified _run_scenario.\u001b[39;00m\n\u001b[32m    490\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_scenario\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m=\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequests_per_sec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequests_per_sec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquality_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquality_metric\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fast_llm_inference/benchmark/benchmark.py:362\u001b[39m, in \u001b[36mModelBenchmark._run_scenario\u001b[39m\u001b[34m(self, task, scenario, samples, batch_size, run_time, requests_per_sec, max_batch_size, sample_interval, quality_metric)\u001b[39m\n\u001b[32m    359\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTask \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# ─── 2) initialization & metadata ────────────────────────────────\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m meta_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# scenario‐specific fields\u001b[39;00m\n\u001b[32m    364\u001b[39m extra = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fast_llm_inference/benchmark/benchmark.py:276\u001b[39m, in \u001b[36mModelBenchmark._initialize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28mself\u001b[39m.model_size = \u001b[38;5;28mself\u001b[39m.get_model_size_mb(\u001b[38;5;28mself\u001b[39m.model_path)\n\u001b[32m    275\u001b[39m \u001b[38;5;66;03m# 0.d) warm-up\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWarmup\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# 0.e) ttft\u001b[39;00m\n\u001b[32m    279\u001b[39m ttft = (\u001b[38;5;28mself\u001b[39m.backend_handler.measure_ttft()\n\u001b[32m    280\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.backend_handler, \u001b[33m\"\u001b[39m\u001b[33mmeasure_ttft\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fast_llm_inference/benchmark/backends/hf_backend.py:67\u001b[39m, in \u001b[36mHuggingFaceBackend.generate\u001b[39m\u001b[34m(self, prompts, task_type)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stopper:\n\u001b[32m     65\u001b[39m     pipe_kwargs[\u001b[33m\"\u001b[39m\u001b[33mstopping_criteria\u001b[39m\u001b[33m\"\u001b[39m] = stopper\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m raw_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpipe_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Extract & clean—take the first generation per prompt\u001b[39;00m\n\u001b[32m     70\u001b[39m texts = [out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m].lstrip() \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m raw_outputs]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:272\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(chats, **kwargs)\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1282\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[32m   1279\u001b[39m     final_iterator = \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   1280\u001b[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[32m   1281\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m     outputs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[32m   1284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m item = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m    125\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:125\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m    124\u001b[39m item = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1208\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1206\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1207\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1208\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1209\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1210\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:370\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    368\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m generated_sequence = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m out_b = generated_sequence.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/transformers/generation/utils.py:2252\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m   2244\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2245\u001b[39m         input_ids=input_ids,\n\u001b[32m   2246\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2247\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2248\u001b[39m         **model_kwargs,\n\u001b[32m   2249\u001b[39m     )\n\u001b[32m   2251\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2252\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2253\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2257\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2259\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2260\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2262\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2263\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2264\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2265\u001b[39m         batch_size=batch_size,\n\u001b[32m   2266\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2271\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2272\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/transformers/generation/utils.py:3254\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3252\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3254\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3256\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3257\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3258\u001b[39m     outputs,\n\u001b[32m   3259\u001b[39m     model_kwargs,\n\u001b[32m   3260\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3261\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1163\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m   1160\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m   1162\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1163\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1177\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1178\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:913\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    901\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    902\u001b[39m         decoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m    903\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         position_embeddings,\n\u001b[32m    911\u001b[39m     )\n\u001b[32m    912\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m913\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    925\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    927\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:640\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    637\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    639\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m hidden_states, self_attn_weights, present_key_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    651\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    653\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:578\u001b[39m, in \u001b[36mLlamaSdpaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    575\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n\u001b[32m    576\u001b[39m attn_output = attn_output.view(bsz, q_len, -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m578\u001b[39m attn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m, past_key_value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "run_report2, details_df2 = bm.run(\n",
    "    task=\"qa\",\n",
    "    scenario=\"server\",\n",
    "    run_time=60.0,\n",
    "    requests_per_sec=5.0,\n",
    "    max_batch_size=64,\n",
    "    sample_interval=0.1,\n",
    "    quality_metric=False\n",
    ")\n",
    "\n",
    "bm.close()\n",
    "del bm\n",
    "torch.cuda.empty_cache()  # clear GPU memory after closing the benchmark\n",
    "\n",
    "print(\"=== Server Run Report ===\")\n",
    "print(run_report2)\n",
    "print(\"\\n=== Per-Query Details ===\")\n",
    "print(details_df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bd0dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>startup_time_sec</th>\n",
       "      <th>load_model_time_sec</th>\n",
       "      <th>ttft_sec</th>\n",
       "      <th>cold_start_sec</th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_size_mb</th>\n",
       "      <th>backend</th>\n",
       "      <th>scenario</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>num_queries</th>\n",
       "      <th>...</th>\n",
       "      <th>peak_power_w</th>\n",
       "      <th>total_energy_wh</th>\n",
       "      <th>avg_scheduled_ts</th>\n",
       "      <th>avg_wait_time_s</th>\n",
       "      <th>avg_ATL</th>\n",
       "      <th>avg_GL</th>\n",
       "      <th>avg_TPS</th>\n",
       "      <th>avg_SPS</th>\n",
       "      <th>avg_energy_per_token</th>\n",
       "      <th>avg_energy_per_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5278</td>\n",
       "      <td>79.8238</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>80.4285</td>\n",
       "      <td>llama-3.1-8B-Instruct-4bit</td>\n",
       "      <td>15327.360256</td>\n",
       "      <td>vllm</td>\n",
       "      <td>server</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>72.27</td>\n",
       "      <td>0.110956</td>\n",
       "      <td>29.054802</td>\n",
       "      <td>1.748086e+09</td>\n",
       "      <td>0.028772</td>\n",
       "      <td>0.091197</td>\n",
       "      <td>34.833268</td>\n",
       "      <td>19.409052</td>\n",
       "      <td>2.015119</td>\n",
       "      <td>6.219429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   startup_time_sec  load_model_time_sec  ttft_sec  cold_start_sec  \\\n",
       "0            0.5278              79.8238    0.0769         80.4285   \n",
       "\n",
       "                   model_name  model_size_mb backend scenario batch_size  \\\n",
       "0  llama-3.1-8B-Instruct-4bit   15327.360256    vllm   server       None   \n",
       "\n",
       "  num_queries  ...  peak_power_w  total_energy_wh  avg_scheduled_ts  \\\n",
       "0        None  ...         72.27         0.110956         29.054802   \n",
       "\n",
       "   avg_wait_time_s   avg_ATL    avg_GL    avg_TPS    avg_SPS  \\\n",
       "0     1.748086e+09  0.028772  0.091197  34.833268  19.409052   \n",
       "\n",
       "   avg_energy_per_token  avg_energy_per_sentence  \n",
       "0              2.015119                 6.219429  \n",
       "\n",
       "[1 rows x 34 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_report2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3249ecfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>scheduled_ts</th>\n",
       "      <th>wait_time_s</th>\n",
       "      <th>ATL</th>\n",
       "      <th>GL</th>\n",
       "      <th>TPS</th>\n",
       "      <th>SPS</th>\n",
       "      <th>energy_per_token</th>\n",
       "      <th>energy_per_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>Lothar de Maizière</td>\n",
       "      <td>[Lothar de Maizière, Lothar de Maizière, Lotha...</td>\n",
       "      <td>0.093854</td>\n",
       "      <td>1.748086e+09</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.079514</td>\n",
       "      <td>37.73</td>\n",
       "      <td>12.58</td>\n",
       "      <td>1.767446</td>\n",
       "      <td>6.241295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>Complexity classes.</td>\n",
       "      <td>[complexity classes, complexity classes, some ...</td>\n",
       "      <td>0.695878</td>\n",
       "      <td>1.748086e+09</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.053010</td>\n",
       "      <td>37.73</td>\n",
       "      <td>18.86</td>\n",
       "      <td>1.767446</td>\n",
       "      <td>6.241295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>GTE</td>\n",
       "      <td>[Telenet was incorporated in 1973 and started ...</td>\n",
       "      <td>0.959227</td>\n",
       "      <td>1.748086e+09</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>37.73</td>\n",
       "      <td>37.73</td>\n",
       "      <td>1.767446</td>\n",
       "      <td>6.241295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>Water flow through the body cavity.</td>\n",
       "      <td>[water flow through the body cavity, κτείς kte...</td>\n",
       "      <td>1.141816</td>\n",
       "      <td>1.748086e+09</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.159029</td>\n",
       "      <td>37.73</td>\n",
       "      <td>6.29</td>\n",
       "      <td>1.767446</td>\n",
       "      <td>6.241295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>12 May 1705.</td>\n",
       "      <td>[12 May 1705, 1705, 12 May 1705]</td>\n",
       "      <td>1.175741</td>\n",
       "      <td>1.748086e+09</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.079514</td>\n",
       "      <td>37.73</td>\n",
       "      <td>12.58</td>\n",
       "      <td>1.767446</td>\n",
       "      <td>6.241295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>5</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>Jean Ribault</td>\n",
       "      <td>[Jean Ribault, Jean Ribault, Jean Ribault]</td>\n",
       "      <td>58.529722</td>\n",
       "      <td>1.748086e+09</td>\n",
       "      <td>0.028178</td>\n",
       "      <td>0.056356</td>\n",
       "      <td>35.49</td>\n",
       "      <td>17.74</td>\n",
       "      <td>1.981041</td>\n",
       "      <td>6.120001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>5</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>Pinedale</td>\n",
       "      <td>[Pinedale, Pinedale, Pinedale]</td>\n",
       "      <td>58.685304</td>\n",
       "      <td>1.748086e+09</td>\n",
       "      <td>0.028178</td>\n",
       "      <td>0.028178</td>\n",
       "      <td>35.49</td>\n",
       "      <td>35.49</td>\n",
       "      <td>1.981041</td>\n",
       "      <td>6.120001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>5</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>Spontaneous combustion.</td>\n",
       "      <td>[spontaneous, spontaneous combustion, spontane...</td>\n",
       "      <td>58.888211</td>\n",
       "      <td>1.748086e+09</td>\n",
       "      <td>0.028178</td>\n",
       "      <td>0.056356</td>\n",
       "      <td>35.49</td>\n",
       "      <td>17.74</td>\n",
       "      <td>1.981041</td>\n",
       "      <td>6.120001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>5</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>Vistula Valley</td>\n",
       "      <td>[Vistula Valley, geomorphologic, Vistula Valley]</td>\n",
       "      <td>59.147203</td>\n",
       "      <td>1.748086e+09</td>\n",
       "      <td>0.028178</td>\n",
       "      <td>0.056356</td>\n",
       "      <td>35.49</td>\n",
       "      <td>17.74</td>\n",
       "      <td>1.981041</td>\n",
       "      <td>6.120001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>5</td>\n",
       "      <td>### SYSTEM\\nYou are a question-answering assis...</td>\n",
       "      <td>1041</td>\n",
       "      <td>[1041, in 1041, 1041]</td>\n",
       "      <td>59.891915</td>\n",
       "      <td>1.748086e+09</td>\n",
       "      <td>0.028178</td>\n",
       "      <td>0.028178</td>\n",
       "      <td>35.49</td>\n",
       "      <td>35.49</td>\n",
       "      <td>1.981041</td>\n",
       "      <td>6.120001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     batch_id                                             prompt  \\\n",
       "0           1  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "1           1  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "2           1  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "3           1  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "4           1  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "..        ...                                                ...   \n",
       "301         5  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "302         5  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "303         5  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "304         5  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "305         5  ### SYSTEM\\nYou are a question-answering assis...   \n",
       "\n",
       "                        generated_answer  \\\n",
       "0                     Lothar de Maizière   \n",
       "1                    Complexity classes.   \n",
       "2                                    GTE   \n",
       "3    Water flow through the body cavity.   \n",
       "4                           12 May 1705.   \n",
       "..                                   ...   \n",
       "301                         Jean Ribault   \n",
       "302                             Pinedale   \n",
       "303              Spontaneous combustion.   \n",
       "304                       Vistula Valley   \n",
       "305                                 1041   \n",
       "\n",
       "                                      reference_answer  scheduled_ts  \\\n",
       "0    [Lothar de Maizière, Lothar de Maizière, Lotha...      0.093854   \n",
       "1    [complexity classes, complexity classes, some ...      0.695878   \n",
       "2    [Telenet was incorporated in 1973 and started ...      0.959227   \n",
       "3    [water flow through the body cavity, κτείς kte...      1.141816   \n",
       "4                     [12 May 1705, 1705, 12 May 1705]      1.175741   \n",
       "..                                                 ...           ...   \n",
       "301         [Jean Ribault, Jean Ribault, Jean Ribault]     58.529722   \n",
       "302                     [Pinedale, Pinedale, Pinedale]     58.685304   \n",
       "303  [spontaneous, spontaneous combustion, spontane...     58.888211   \n",
       "304   [Vistula Valley, geomorphologic, Vistula Valley]     59.147203   \n",
       "305                              [1041, in 1041, 1041]     59.891915   \n",
       "\n",
       "      wait_time_s       ATL        GL    TPS    SPS  energy_per_token  \\\n",
       "0    1.748086e+09  0.026505  0.079514  37.73  12.58          1.767446   \n",
       "1    1.748086e+09  0.026505  0.053010  37.73  18.86          1.767446   \n",
       "2    1.748086e+09  0.026505  0.026505  37.73  37.73          1.767446   \n",
       "3    1.748086e+09  0.026505  0.159029  37.73   6.29          1.767446   \n",
       "4    1.748086e+09  0.026505  0.079514  37.73  12.58          1.767446   \n",
       "..            ...       ...       ...    ...    ...               ...   \n",
       "301  1.748086e+09  0.028178  0.056356  35.49  17.74          1.981041   \n",
       "302  1.748086e+09  0.028178  0.028178  35.49  35.49          1.981041   \n",
       "303  1.748086e+09  0.028178  0.056356  35.49  17.74          1.981041   \n",
       "304  1.748086e+09  0.028178  0.056356  35.49  17.74          1.981041   \n",
       "305  1.748086e+09  0.028178  0.028178  35.49  35.49          1.981041   \n",
       "\n",
       "     energy_per_sentence  \n",
       "0               6.241295  \n",
       "1               6.241295  \n",
       "2               6.241295  \n",
       "3               6.241295  \n",
       "4               6.241295  \n",
       "..                   ...  \n",
       "301             6.120001  \n",
       "302             6.120001  \n",
       "303             6.120001  \n",
       "304             6.120001  \n",
       "305             6.120001  \n",
       "\n",
       "[306 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details_df2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
