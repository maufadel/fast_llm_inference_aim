{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc37054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-28 15:18:51 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 15:18:51.873981: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748445531.898733  176014 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748445531.906032  176014 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748445531.922127  176014 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748445531.922144  176014 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748445531.922146  176014 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748445531.922148  176014 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-28 15:18:51.927486: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-28 15:19:13 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'classify', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 05-28 15:19:15 [config.py:830] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-28 15:19:15 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-28 15:19:16 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', speculative_config=None, tokenizer='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-28 15:19:17 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f96e6afd350>\n",
      "INFO 05-28 15:19:19 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-28 15:19:19 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-28 15:19:19 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-28 15:19:19 [gpu_model_runner.py:1329] Starting to load model /home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit...\n",
      "INFO 05-28 15:19:19 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb022f2dfa94b35ad9c237e984079db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba2eb982d2943698eff9e56c83dff69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-28 15:19:47 [gpu_model_runner.py:1347] Model loading took 5.3132 GiB and 27.324152 seconds\n",
      "INFO 05-28 15:20:02 [backends.py:420] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/0f63e24e8b/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-28 15:20:02 [backends.py:430] Dynamo bytecode transform time: 14.88 s\n",
      "INFO 05-28 15:20:11 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 6.773 s\n",
      "INFO 05-28 15:20:14 [monitor.py:33] torch.compile takes 14.88 s in total\n",
      "INFO 05-28 15:20:17 [kv_cache_utils.py:634] GPU KV cache size: 106,880 tokens\n",
      "INFO 05-28 15:20:17 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 26.09x\n",
      "INFO 05-28 15:21:20 [gpu_model_runner.py:1686] Graph capturing finished in 64 secs, took 1.54 GiB\n",
      "INFO 05-28 15:21:21 [core.py:159] init engine (profile, create kv cache, warmup model) took 93.87 seconds\n",
      "INFO 05-28 15:21:21 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and setup\n",
    "import os\n",
    "import math\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# (Optional) adjust your model path here\n",
    "MODEL_PATH = \"/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit\"\n",
    "\n",
    "# Cell 2: Load model and define prompts\n",
    "model = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    max_model_len=4096,\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"What is the purpose of life?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09340280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b8dee6ec1345c58bd5dc5ebdcc32cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prompt 1: The quick brown fox jumps over the lazy dog. ===\n",
      "Generated: The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick\n",
      "\n",
      "       Token |  LogProb |      PPL\n",
      "----------------------------------\n",
      "        ĠThe |  -0.9727 |   2.6450\n",
      "      Ġquick |  -0.3296 |   1.3905\n",
      "      Ġbrown |  -0.0029 |   1.0029\n",
      "        Ġfox |  -0.0016 |   1.0016\n",
      "      Ġjumps |  -0.0055 |   1.0055\n",
      "       Ġover |  -0.0005 |   1.0005\n",
      "        Ġthe |  -0.0005 |   1.0005\n",
      "       Ġlazy |  -0.0005 |   1.0005\n",
      "        Ġdog |  -0.0004 |   1.0004\n",
      "           . |  -0.3360 |   1.3993\n",
      "        ĠThe |  -0.0372 |   1.0379\n",
      "      Ġquick |  -0.0015 |   1.0015\n",
      "      Ġbrown |  -0.0012 |   1.0012\n",
      "        Ġfox |  -0.0007 |   1.0007\n",
      "      Ġjumps |  -0.0013 |   1.0013\n",
      "       Ġover |  -0.0008 |   1.0008\n",
      "        Ġthe |  -0.0010 |   1.0010\n",
      "       Ġlazy |  -0.0010 |   1.0010\n",
      "        Ġdog |  -0.0005 |   1.0005\n",
      "           . |  -0.2609 |   1.2981\n",
      "        ĠThe |  -0.0465 |   1.0476\n",
      "      Ġquick |  -0.0021 |   1.0021\n",
      "      Ġbrown |  -0.0006 |   1.0006\n",
      "        Ġfox |  -0.0014 |   1.0014\n",
      "      Ġjumps |  -0.0008 |   1.0008\n",
      "       Ġover |  -0.0002 |   1.0002\n",
      "        Ġthe |  -0.0008 |   1.0008\n",
      "       Ġlazy |  -0.0007 |   1.0007\n",
      "        Ġdog |  -0.0004 |   1.0004\n",
      "           . |  -0.1384 |   1.1484\n",
      "        ĠThe |  -0.0204 |   1.0206\n",
      "      Ġquick |  -0.0011 |   1.0011\n",
      "\n",
      "Sequence-level Perplexity: 1.0702\n",
      "\n",
      "=== Prompt 2: What is the purpose of life? ===\n",
      "Generated: Is it to find happiness, to achieve success, or to make a difference in the world? The answer to this question is subjective and can vary greatly from person\n",
      "\n",
      "       Token |  LogProb |      PPL\n",
      "----------------------------------\n",
      "         ĠIs |  -1.4004 |   4.0569\n",
      "         Ġit |  -0.1798 |   1.1970\n",
      "         Ġto |  -0.0869 |   1.0908\n",
      "       Ġfind |  -0.9659 |   2.6272\n",
      "  Ġhappiness |  -0.0480 |   1.0492\n",
      "           , |  -0.0587 |   1.0604\n",
      "         Ġto |  -0.8425 |   2.3222\n",
      "    Ġachieve |  -0.7216 |   2.0577\n",
      "    Ġsuccess |  -0.0438 |   1.0448\n",
      "           , |  -0.0013 |   1.0013\n",
      "         Ġto |  -0.6181 |   1.8554\n",
      "         Ġto |  -0.0315 |   1.0320\n",
      "       Ġmake |  -0.7800 |   2.1815\n",
      "          Ġa |  -0.0186 |   1.0188\n",
      " Ġdifference |  -0.1679 |   1.1829\n",
      "         Ġin |  -0.1038 |   1.1094\n",
      "        Ġthe |  -0.0010 |   1.0010\n",
      "      Ġworld |  -0.0022 |   1.0022\n",
      "           ? |  -0.1106 |   1.1170\n",
      "        ĠThe |  -1.3354 |   3.8015\n",
      "     Ġanswer |  -0.2585 |   1.2950\n",
      "         Ġto |  -0.9216 |   2.5133\n",
      "       Ġthis |  -0.0481 |   1.0493\n",
      "   Ġquestion |  -0.0216 |   1.0219\n",
      "         Ġis |  -0.5206 |   1.6830\n",
      " Ġsubjective |  -1.2614 |   3.5304\n",
      "        Ġand |  -0.0497 |   1.0509\n",
      "        Ġcan |  -0.4695 |   1.5992\n",
      "       Ġvary |  -0.0099 |   1.0099\n",
      "    Ġgreatly |  -0.3733 |   1.4525\n",
      "       Ġfrom |  -0.1455 |   1.1567\n",
      "     Ġperson |  -0.0396 |   1.0404\n",
      "\n",
      "Sequence-level Perplexity: 1.4386\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Configure SamplingParams for logprobs & perplexity\n",
    "params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    max_tokens=32,\n",
    "    logprobs=1,\n",
    "    prompt_logprobs=1\n",
    ")\n",
    "\n",
    "# Cell 4: Run generation and display results in a table plus sequence PPL\n",
    "outputs = model.generate(prompts, params)\n",
    "\n",
    "for i, gen_out in enumerate(outputs):\n",
    "    sample    = gen_out.outputs[0]\n",
    "    text      = sample.text.lstrip()\n",
    "    lp_list   = sample.logprobs            # list of dicts\n",
    "    token_ids = sample.token_ids\n",
    "\n",
    "    # 1) Extract the chosen-token strings & logprobs\n",
    "    tokens, logps = [], []\n",
    "    for entry in lp_list:\n",
    "        # each entry is {token_id: Logprob(...), ...}\n",
    "        for tid, lp_obj in entry.items():\n",
    "            if lp_obj.rank == 1:\n",
    "                tokens.append(lp_obj.decoded_token)\n",
    "                logps.append(lp_obj.logprob)\n",
    "                break\n",
    "\n",
    "    # 2) Compute per-token perplexity\n",
    "    ppl = [math.exp(-lp) for lp in logps]\n",
    "\n",
    "    # 3) Print per-token table\n",
    "    print(f\"\\n=== Prompt {i+1}: {prompts[i]} ===\")\n",
    "    print(f\"Generated: {text}\\n\")\n",
    "    print(f\"{'Token':>12} | {'LogProb':>8} | {'PPL':>8}\")\n",
    "    print(\"-\" * 34)\n",
    "    for tok, lp, p in zip(tokens, logps, ppl):\n",
    "        print(f\"{tok:>12} | {lp:8.4f} | {p:8.4f}\")\n",
    "\n",
    "    # 4) Compute sequence-level perplexity\n",
    "    ppl_seq = math.exp(- sum(logps) / len(logps))\n",
    "    print(f\"\\nSequence-level Perplexity: {ppl_seq:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d64be",
   "metadata": {},
   "source": [
    "Using the official Docker images to launch the inference engines:\n",
    "\n",
    "\n",
    "##### TGI (check)\n",
    "\n",
    "docker run --rm \\\n",
    "  --gpus all \\\n",
    "  -v \"$HOME/.cache/huggingface:/data\" \\\n",
    "  -v \"$HOME/.cache/huggingface:/root/.cache/huggingface\" \\\n",
    "  -e HF_TOKEN=\"$HF_TOKEN\" \\\n",
    "  -p 127.0.0.1:23333:23333 \\\n",
    "  ghcr.io/huggingface/text-generation-inference:3.3.1 \\\n",
    "    --model-id mistralai/Mistral-7B-Instruct-v0.3 \\\n",
    "    --trust-remote-code \\\n",
    "    --port 23333 \\\n",
    "    --max-client-batch-size 128\n",
    "\n",
    "\n",
    "##### vLLM (check)\n",
    "\n",
    "docker run --rm \\\n",
    "  --runtime=nvidia --gpus all \\\n",
    "  -v \"$HOME/.cache/huggingface:/root/.cache/huggingface\" \\\n",
    "  -e HUGGING_FACE_HUB_TOKEN=\"$HF_TOKEN\" \\\n",
    "  -p 127.0.0.1:23333:23333 \\\n",
    "  --ipc=host \\\n",
    "  vllm/vllm-openai:latest \\\n",
    "    --model mistralai/Mistral-7B-Instruct-v0.3 \\\n",
    "    --port 23333\n",
    "\n",
    "\n",
    "##### LMDeploy (check)\n",
    "\n",
    "docker run --rm \\\n",
    "  --runtime=nvidia --gpus all \\\n",
    "  -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\n",
    "  -e HUGGING_FACE_HUB_TOKEN=$HF_TOKEN \\\n",
    "  -p 127.0.0.1:23333:23333 \\\n",
    "  --ipc=host \\\n",
    "  openmmlab/lmdeploy:latest \\\n",
    "    lmdeploy serve api_server mistralai/Mistral-7B-Instruct-v0.3 \\\n",
    "    --server-port 23333                               \n",
    "\n",
    "\n",
    "##### SGLang (check)\n",
    "\n",
    "docker run --gpus all \\\n",
    "  -p 127.0.0.1:23333:23333 \\\n",
    "  -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    "  --ipc=host \\\n",
    "  lmsysorg/sglang:latest \\\n",
    "  bash -c \"\\\n",
    "    pip install --no-cache-dir protobuf sentencepiece && \\\n",
    "    python3 -m sglang.launch_server \\\n",
    "      --model-path mistralai/Mistral-7B-Instruct-v0.3 \\\n",
    "      --host 0.0.0.0 \\\n",
    "      --port 23333 \\\n",
    "  \"\n",
    "\n",
    "\n",
    "##### Deepspeed-MII (check)\n",
    "\n",
    "docker run --runtime=nvidia --gpus all \\\n",
    "  -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\n",
    "  -e HUGGING_FACE_HUB_TOKEN=$HF_TOKEN \\\n",
    "  -p 127.0.0.1:23333:23333 \\\n",
    "  --ipc=host \\\n",
    "  slinusc/deepspeed-mii:latest \\\n",
    "  --model mistralai/Mistral-7B-Instruct-v0.3 \\\n",
    "  --port 23333\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d55bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "class InferenceEngineClient:\n",
    "    \"\"\"\n",
    "    A simple wrapper for an OpenAI‐compatible server,\n",
    "    defaulting to Mistral-7B-Instruct-v0.3.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_url=\"http://localhost:23333/v1\", api_key=\"none\"):\n",
    "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.default_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "    def completion(self,\n",
    "                   prompt,\n",
    "                   model: str | None = None,\n",
    "                   temperature: float = 0.7,\n",
    "                   max_tokens: int = 512,\n",
    "                   top_p: float = 0.9,\n",
    "                   stream: bool = False):\n",
    "        \"\"\"\n",
    "        Send one or more prompts.\n",
    "        :param prompt: a single string or a list of strings\n",
    "        :return: if single prompt, returns str; if list, returns List[str]\n",
    "        \"\"\"\n",
    "        model = model or self.default_model\n",
    "        is_batch = isinstance(prompt, (list, tuple))\n",
    "\n",
    "        resp = self.client.completions.create(\n",
    "            model=model,\n",
    "            prompt=prompt,        # can be str or list[str]\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=top_p,\n",
    "            stream=stream\n",
    "        )\n",
    "\n",
    "        if stream:\n",
    "            # streaming with batching is a bit more involved; you’ll get interleaved chunks\n",
    "            return resp\n",
    "\n",
    "        # non‐streaming: choices is a list with one entry per prompt\n",
    "        texts = [c.text for c in resp.choices]\n",
    "        return texts if is_batch else texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "923325d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# Insert the parent directory of the current file/notebook\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "from benchmark.tasks.qa import QATask\n",
    "from benchmark.tasks.summarization import SummarizationTask\n",
    "\n",
    "sum_task = SummarizationTask()\n",
    "qa_task = QATask()\n",
    "\n",
    "qa_queries = qa_task.generate_prompts(64)\n",
    "sum_queries = sum_task.generate_prompts(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d241d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nLothar de Maizière',\n",
       " '\\nComplexity classes',\n",
       " '\\nGTE',\n",
       " '\\nWater flow through the body cavity',\n",
       " '\\n12 May 1705',\n",
       " '\\nIsraeli poet',\n",
       " '\\nStrengthening the role of backbenchers in their scrutiny of the government.',\n",
       " '\\nNP-complete',\n",
       " '\\nca. 2 million',\n",
       " '\\nDestruction of the forest',\n",
       " '\\nTwo poles: revolution/invasion and reformist strategy.',\n",
       " '\\nBoolean circuits',\n",
       " '\\nDuisburg',\n",
       " '\\nArticles 106 and 107',\n",
       " '\\n\"Time derivative of the changing momentum\"',\n",
       " '\\nCuba',\n",
       " '\\nUnderground',\n",
       " '\\nTrio Tribe',\n",
       " '\\nOutcome of most votes',\n",
       " '\\nUsers via leased lines and the public PAD service Telepad',\n",
       " '\\nLower levels of inequality',\n",
       " '\\nExodus',\n",
       " '\\n2001 study found 1 square kilometer (247 acres)',\n",
       " '\\nSemantical problems and grammatical niceties',\n",
       " '\\nGranted the Huguenots equality and a degree of religious and political freedom within their domains.',\n",
       " '\\nIslamists',\n",
       " '\\nStore and forward switching',\n",
       " '\\nAlta California',\n",
       " '\\nPaul Samuelson',\n",
       " '\\n9%',\n",
       " '\\nEast-west',\n",
       " '\\nAn attorney',\n",
       " '\\nA citizen or company can invoke a Directive, not just in a dispute with a public authority, but in a dispute with another citizen or company.',\n",
       " '\\nCurving parabolic path in the same direction as the motion of the vehicle.',\n",
       " '\\nUrban areas',\n",
       " '\\nMay no longer exist.',\n",
       " '\\nColloblasts are specialized mushroom-shaped cells in the outer layer of the epidermis, densely covering the tentacles and tentilla of cydippid ctenophores, that capture prey by sticking to it.',\n",
       " '\\nUniversity Athletic Association (UAA)',\n",
       " '\\nPermanent pulmonary fibrosis',\n",
       " '\\ntransportation, sewer, hazardous waste, water',\n",
       " '\\nLast Glacial Maximum (LGM)',\n",
       " '\\nSir Isaac Newton',\n",
       " '\\nHe informed British merchants and fur-traders of French claims and told them to leave.',\n",
       " '\\nMughal state',\n",
       " '\\nUnit',\n",
       " '\\nGreat Yuan',\n",
       " '\\nWestward',\n",
       " '\\nOpposed',\n",
       " '\\nLaszlo Babai and Eugene Luks',\n",
       " '\\n182 million tons',\n",
       " '\\nArchipelago-like estuary',\n",
       " '\\nFrancisco de Orellana',\n",
       " '\\n260 miles east of Berlin',\n",
       " '\\nJohn M. Grunsfeld',\n",
       " '\\nThe Ruhr provides the region with drinking water.',\n",
       " '\\nElectronic Frontier Foundation',\n",
       " '\\nAsynchronously using first-in, first-out buffering',\n",
       " '\\nThe MSPs elect one MSP to serve as Presiding Officer.',\n",
       " '\\nHomicides definition',\n",
       " '\\nCourtyard adjoining the Assembly Hall']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Insert the parent directory of the current file/notebook\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "from benchmark.inference_engine_client import InferenceEngineClient\n",
    "model = \"mistralai/Mistral-7B-Instruct-v0.3\"  # or any other model you want to use\n",
    "backend = \"sglang\"  # or \"mistral\" for Mistral-7B-Instruct-v0.3\n",
    "\n",
    "cli = InferenceEngineClient()\n",
    "\n",
    "cli.launch(backend=backend, model_path=model)\n",
    "\n",
    "batch_results = cli.completion(qa_queries[0], temperature=0.1)\n",
    "\n",
    "batch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c6bbf20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lothar de Maizière',\n",
       " 'Complexity classes',\n",
       " 'GTE',\n",
       " 'Water flow through the body cavity',\n",
       " '12 May 1705',\n",
       " 'Israeli poet',\n",
       " 'Strengthening the role of backbenchers in their scrutiny of the government',\n",
       " 'NP-complete',\n",
       " 'ca. 2 million',\n",
       " 'Destruction of the forest',\n",
       " 'Two poles: revolution/invasion and reformist strategy',\n",
       " 'Boolean circuits',\n",
       " 'Duisburg',\n",
       " 'Articles 106 and 107',\n",
       " '\"Time derivative of the changing momentum\"',\n",
       " 'Cuba',\n",
       " 'Underground',\n",
       " 'Trio Tribe',\n",
       " 'Outcome of most votes',\n",
       " 'Users via leased lines and the public PAD service Telepad',\n",
       " 'Lower levels of inequality',\n",
       " 'Exodus',\n",
       " '2001 study found 1 square kilometer (247 acres)',\n",
       " 'Semantical problems and grammatical niceties',\n",
       " 'Granted the Huguenots equality and a degree of religious and political freedom within their domains',\n",
       " 'Islamists',\n",
       " 'Store and forward switching',\n",
       " 'Alta California',\n",
       " 'Paul Samuelson',\n",
       " '9%',\n",
       " 'East-west',\n",
       " 'An attorney',\n",
       " 'A citizen or company can invoke a Directive, not just in a dispute with a public authority, but in a dispute with another citizen or company',\n",
       " 'Curving parabolic path in the same direction as the motion of the vehicle',\n",
       " 'Urban areas',\n",
       " 'May no longer exist',\n",
       " 'Colloblasts are specialized mushroom-shaped cells in the outer layer of the epidermis, densely covering the tentacles and tentilla of cydippid ctenophores, that capture prey by sticking to it',\n",
       " 'University Athletic Association (UAA)',\n",
       " 'Permanent pulmonary fibrosis',\n",
       " 'transportation, sewer, hazardous waste, water',\n",
       " 'Last Glacial Maximum (LGM)',\n",
       " 'Sir Isaac Newton',\n",
       " 'He informed British merchants and fur-traders of French claims and told them to leave',\n",
       " 'Mughal state',\n",
       " 'Unit',\n",
       " 'Great Yuan',\n",
       " 'Westward',\n",
       " 'Opposed',\n",
       " 'Laszlo Babai and Eugene Luks',\n",
       " '182 million tons',\n",
       " 'Archipelago-like estuary',\n",
       " 'Francisco de Orellana',\n",
       " '260 miles east of Berlin',\n",
       " 'John M. Grunsfeld',\n",
       " 'The Ruhr provides the region with drinking water',\n",
       " 'Electronic Frontier Foundation',\n",
       " 'Asynchronously using first-in, first-out buffering',\n",
       " 'The MSPs elect one MSP to serve as Presiding Officer',\n",
       " 'Homicides definition',\n",
       " 'Courtyard adjoining the Assembly Hall']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_prediction(prediction: list[str]) -> list[str]:\n",
    "    cleaned = []\n",
    "    for raw in prediction:\n",
    "        # 1) Remove anything after the first '###'\n",
    "        ans = raw.split(\"###\", 1)[0]\n",
    "\n",
    "        # 2) Strip whitespace (including newlines) from both ends\n",
    "        ans = ans.strip()\n",
    "\n",
    "        # 3) Remove anything after the first newline (in the stripped string)\n",
    "        ans = ans.split(\"\\n\", 1)[0]\n",
    "\n",
    "        # 4) Strip again and remove any trailing periods\n",
    "        ans = ans.strip().rstrip(\".\")\n",
    "\n",
    "        cleaned.append(ans)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "clean_prediction(batch_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74041248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "\n",
    "bm = ModelBenchmark(\n",
    "    backend=\"sglang\",\n",
    "    model_path= \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    model_name= \"Mistral-7B-Instruct-v0.3\",\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a3a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.iec.launch(    backend=\"sglang\",\n",
    "    model_path= \"mistralai/Mistral-7B-Instruct-v0.3\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a5d449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = bm.generate([\"What is the capital of France?\", \"What is the largest mammal?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6e988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw, time = resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f42fba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.utils import clean_prediction\n",
    "cleaned = clean_prediction(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a309aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The capital of France is Paris. It is located in the north-central part of the country. Paris is one of the most famous cities in the world, known for its art, culture, and landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. It is also the political, economic, and cultural center of France',\n",
       " \"The blue whale is the largest mammal. It can reach lengths of up to 100 feet (30 meters) and weigh as much as 200 tons (181 metric tonnes). The blue whale's heart alone can weigh as much as a car!\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4242145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "\n",
    "bm = ModelBenchmark(\n",
    "    backend=\"sglang\",\n",
    "    model_path=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    model_name=\"Mistral-7B-Instruct-v0.3\",\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf3dbfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QATask' object has no attribute 'get_n_prompts_refs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mqa\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mserver\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_time\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcurrent_users\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequests_per_user_per_min\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fast_llm_inference/benchmark/benchmark.py:517\u001b[39m, in \u001b[36mModelBenchmark.run\u001b[39m\u001b[34m(self, task, scenario, samples, batch_size, run_time, concurrent_users, requests_per_user_per_min, sample_interval, quality_metric)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\n\u001b[32m    505\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    506\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    515\u001b[39m     quality_metric: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    516\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_scenario\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m=\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconcurrent_users\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconcurrent_users\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequests_per_user_per_min\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequests_per_user_per_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquality_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquality_metric\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fast_llm_inference/benchmark/benchmark.py:357\u001b[39m, in \u001b[36mModelBenchmark._run_scenario\u001b[39m\u001b[34m(self, task, scenario, run_time, concurrent_users, requests_per_user_per_min, batch_size, samples, sample_interval, quality_metric)\u001b[39m\n\u001b[32m    355\u001b[39m \u001b[38;5;66;03m# 5) prepare prompts/schedules\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scenario == \u001b[33m\"\u001b[39m\u001b[33mserver\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m     prompts, refs, sched_ts, user_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_prompts_per_user\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcurrent_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequests_per_user_per_min\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     events = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sched_ts, prompts, refs, user_ids))\n\u001b[32m    361\u001b[39m     events.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fast_llm_inference/benchmark/benchmark.py:269\u001b[39m, in \u001b[36mModelBenchmark._prepare_prompts_per_user\u001b[39m\u001b[34m(self, task_obj, run_time, num_users, requests_per_user_per_min)\u001b[39m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [], [], [], []\n\u001b[32m    268\u001b[39m \u001b[38;5;66;03m# Ensure get_n_prompts_refs returns exactly n_arrivals items\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m prompts, refs = \u001b[43mtask_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_n_prompts_refs\u001b[49m(n_arrivals)\n\u001b[32m    270\u001b[39m sched_ts = [e[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m events]\n\u001b[32m    271\u001b[39m user_ids = [e[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m events]\n",
      "\u001b[31mAttributeError\u001b[39m: 'QATask' object has no attribute 'get_n_prompts_refs'"
     ]
    }
   ],
   "source": [
    "bm.run(task=\"qa\", scenario=\"server\", run_time=60, concurrent_users=10, requests_per_user_per_min=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
