{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e256ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "def quantize_hf(model_name, out_dir, bit=8):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_{bit}bit=True,  # load_in_8bit or load_in_4bit\n",
    "        llm_int8_threshold=6.0  # only for 8‑bit\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "    model.save_pretrained(out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808373cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, QuantizationConfig\n",
    "\n",
    "def quantize_vllm(model_name, out_dir, bit=4):\n",
    "    qconfig = QuantizationConfig(bits=bit, backend=\"bitsandbytes\")\n",
    "    llm = LLM.from_pretrained(model_name, quantization_config=qconfig)\n",
    "    llm.save_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4fc700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming llama.cpp built and in PATH\n",
    "# bit ∈ {4, 8}\n",
    "!./quantize models/orig.gguf models/quantized_{bit}bit.gguf {bit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ba131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from deepspeed import init_inference\n",
    "\n",
    "def quantize_ds(model_name, out_dir, bit=8):\n",
    "    dtype = torch.int8 if bit==8 else torch.int4\n",
    "    ds_model = init_inference(\n",
    "        model_name,\n",
    "        mp_size=1,\n",
    "        dtype=dtype,\n",
    "        replace_method=\"auto\",\n",
    "    )\n",
    "    ds_model.save_pretrained(out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957d2d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for backend in [\"hf\", \"vllm\", \"llamacpp\", \"ds\"]:\n",
    "    for bit in [16, 8, 4]:\n",
    "        out = f\"quantized/{backend}_{bit}bit\"\n",
    "        if backend == \"hf\":\n",
    "            quantize_hf(\"facebook/llama-7b\", out, bit=bit)\n",
    "        elif backend == \"vllm\":\n",
    "            quantize_vllm(\"facebook/llama-7b\", out, bit=bit)\n",
    "        elif backend == \"llamacpp\":\n",
    "            subprocess.run([\"quantize\", \"models/llama-7b.gguf\", f\"{out}.gguf\", str(bit)])\n",
    "        elif backend == \"ds\":\n",
    "            quantize_ds(\"facebook/llama-7b\", out, bit=bit)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
