{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f0a517",
   "metadata": {},
   "source": [
    "### Creating evaluating pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9195802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"llama-3.1-8B-Instruct\", \"llama-3.1-8B-Instruct-4bit\", \"llama-3.1-8B-Instruct-8bit\"]\n",
    "backends = [\"vllm\", \"huggingface\", \"llama-cpp\"]\n",
    "task = [\"qa\", \"sql\", \"summarization\"]\n",
    "use_cases = [\"batch\", \"server\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d912d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from benchmark.benchmark import ModelBenchmark\n",
    "\n",
    "def run_benchmark(backend, model_name, task):\n",
    "    print(f\"Running benchmark for {model_name} with {backend} on {task}\")\n",
    "    bm = ModelBenchmark(\n",
    "        backend=backend,\n",
    "        model_name=model_name,\n",
    "        model_path=f\"/home/ubuntu/fast_llm_inference/models/{model_name}\",\n",
    "        task=task,\n",
    "        verbose=True,\n",
    "    )\n",
    "    bm.run(samples=5)\n",
    "    del bm  # make sure to dereference\n",
    "    import torch\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model_names = [\"llama-3.1-8B-Instruct-4bit\", \"llama-3.1-8B-Instruct-8bit\", \"llama-3.1-8B-Instruct\"]\n",
    "backends = [\"huggingface\"]\n",
    "tasks = [\"qa\", \"sql\", \"summarization\"]\n",
    "\n",
    "for backend in backends:\n",
    "    for model_name in model_names:\n",
    "        for task in tasks:\n",
    "            p = multiprocessing.Process(target=run_benchmark, args=(backend, model_name, task))\n",
    "            p.start()\n",
    "            p.join()  # wait until finished before next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7593d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.backends.backend_factory import get_backend\n",
    "\n",
    "be = get_backend(\n",
    "    name=\"vllm\",\n",
    "    model_path=\"/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit\",\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "be.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a0eaaf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52237cffd9cf400ab4dfc617a8142d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' authority in the Catholic Church?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be.generate(prompt=\"Is the pope the highest \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecbe1c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 12:57:32.618729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745585852.636204  490515 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745585852.641550  490515 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745585852.657139  490515 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745585852.657157  490515 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745585852.657159  490515 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745585852.657161  490515 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-25 12:57:32.662176: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 12:57:39,395] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "INFO 04-25 12:57:42 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e489682dd844df960e3a19b3c7f663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "\n",
    "backends = [\"huggingface\", \"vllm\", \"deepspeed_mii\", \"llama-cpp\"]\n",
    "\n",
    "bm = ModelBenchmark(\n",
    "    backend=backends[0],\n",
    "    model_name=\"llama-3.1-8B-Instruct\",\n",
    "    model_path=\"/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct\",\n",
    "    task=\"qa\",\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "449a2e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "test_prompt_qa = (\n",
    "            \"You are a question answering assistant. Given the context, answer the question. \"\n",
    "            \"If the answer isn't in the context, respond 'I don't know'. Provide the answer in a single line.\\n\\n\"\n",
    "\n",
    "            \"Here is an example:\\n\"\n",
    "            \"Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni)...\\n\"\n",
    "            \"Question: What is the name of the region the Normans gave their name to?\\n\"\n",
    "            \"Answer: Normandy\\n\\n\"\n",
    "\n",
    "            \"Context: At the end of the 19th century, the United States was a nation of farmers. \"\n",
    "            \"By 1920, more Americans lived in cities than on farms. The industrial revolution \"\n",
    "            \"had changed the way people lived and worked. The United States was becoming a modern nation of cities and factories. The most important change was the rise of the automobile. \"\n",
    "            \"The automobile changed the way people lived. It changed the way people worked. It changed the way people traveled. \"\n",
    "            \"It changed the way people thought about themselves and their country.\\n\"\n",
    "            \"Question: What was the most important change in the United States at the end of the 19th century?\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "\n",
    "test_prompt_sql = (\n",
    "            \"You are a SQL query generation assistant. Given a natural language question, generate the corresponding SQL query.\\n\"\n",
    "            \"Only generate valid SQL statements, no explanations or extra text.\\n\\n\"\n",
    "\n",
    "            \"Here is an example:\\n\\n\"\n",
    "            \"Question: How many heads of the departments are older than 56?\\n\\n\"\n",
    "            \"Tables in the database:\\n\"\n",
    "            \"Table 'department': columns = Department_ID, Name, Creation, Ranking, Budget_in_Billions, Num_Employees\\n\"\n",
    "            \"Table 'head': columns = head_ID, name, born_state, age\\n\"\n",
    "            \"Table 'management': columns = department_ID, head_ID, temporary_acting\\n\\n\"\n",
    "            \"SQL: SELECT count(*) FROM head WHERE age > 56\\n\\n\"\n",
    "\n",
    "            \"Question: How many departments have a budget greater than 5 billion?\\n\\n\"\n",
    "            \"Tables in the database:\\n\"\n",
    "            \"Table 'department': columns = Department_ID, Name, Creation, Ranking, Budget_in_Billions, Num_Employees\\n\"\n",
    "            \"Table 'head': columns = head_ID, name, born_state, age\\n\"\n",
    "            \"Table 'management': columns = department_ID, head_ID, temporary_acting\\n\\n\"\n",
    "            \"SQL:\"\n",
    "        )\n",
    "\n",
    "test_prompt_summarization = (\n",
    "            \"You are a news summarization assistant. Given a full news article, produce a concise and informative summary in 2–3 sentences.\\n\\n\"\n",
    "\n",
    "            \"Example:\\n\\n\"\n",
    "\n",
    "            \"Article:\\n\"\n",
    "            \"(CNN) -- The partnership started as a single shop on Oxford Street in London, opened in 1864 by John Lewis. \"\n",
    "            \"Today the partnership is an organization with bases throughout the UK, with supermarkets and department stores, \"\n",
    "            \"employing approximately 67,100 people. All 67,100 permanent staff are Partners who own 26 John Lewis department stores, \"\n",
    "            \"183 Waitrose supermarkets, an online and catalogue business, John Lewis Direct a direct services company - Greenbee, \"\n",
    "            \"three production units and a farm. Every Partner receives the same scale of bonus, based on a fixed percentage of their annual wage. \"\n",
    "            \"The bonus for 2006 was 18% equivalent to 9 weeks pay, which was rolled out for every employee. \"\n",
    "            \"Chairman Sir Stuart Hampson retired at the end of March 2007, his successor is Charlie Mayfield. Hampson's salary for January 26, \"\n",
    "            \"2006 to January 26, 2007 was $1.66 million which included the partnership bonus of $250,000.\"\n",
    "\n",
    "            \"Summary:\\n\"\n",
    "            \"John Lewis Partnership began as a shop on London's Oxford street in 1864 .\\n\"\n",
    "            \"All 67,100 employees are partners in the organization and own shares .\"\n",
    "\n",
    "            \"Now summarize the following article:\\n\\n\"\n",
    "\n",
    "            \"Article: The newly released iPhone 14 has a larger screen and improved camera features. \"\n",
    "            \"It is available in multiple colors and storage options. The battery life has also been extended, \"\n",
    "            \"making it more efficient for daily use. The price starts at $799, and pre-orders are available now. The phone is expected to be \"\n",
    "            \"a popular choice among consumers, especially those looking to upgrade from older models. The hardware improvements include a faster processor, \"\n",
    "            \"better graphics performance, and enhanced security features. The iPhone 14 also supports 5G connectivity, allowing for faster internet speeds. \"\n",
    "            \"Overall, the iPhone 14 is a significant upgrade over its predecessor, the iPhone 13, and is expected to be a top seller this holiday season.\\n\\n\"\n",
    "            \"Summary:\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "qa = bm.generate_single(prompt=test_prompt_qa, task=\"qa\")\n",
    "sql = bm.generate_single(prompt=test_prompt_sql, task=\"sql\")\n",
    "summarization = bm.generate_single(prompt=test_prompt_summarization, task=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af7f23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The rise of the automobile.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "728b4ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT count(*) FROM department WHERE Budget_in_Billions > 5'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f28a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The iPhone 14 has a larger screen, improved camera features, and extended battery life, starting at $799. It also has a faster processor, better graphics, and enhanced security, and supports 5G connectivity. The phone is expected to be a popular choice among consumers, especially those looking to upgrade from older models.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarization[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
