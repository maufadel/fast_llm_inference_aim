{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e1c4c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 09:39:36.436412: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745314776.454859  135795 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745314776.460469  135795 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745314776.474838  135795 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745314776.474856  135795 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745314776.474857  135795 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745314776.474859  135795 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-22 09:39:36.479446: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-22 09:39:40,849] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "\n",
    "def load_bnb_model(model_path: str, quant_bits: int = 4, compute_dtype: str = \"bfloat16\"):\n",
    "    \"\"\"\n",
    "    Load a pre-quantized BitsAndBytes model for inference.\n",
    "\n",
    "    Parameters:\n",
    "        model_path (str): Path to the quantized model directory.\n",
    "        quant_bits (int): 4 or 8 depending on the quantization type used during save.\n",
    "        compute_dtype (str): Compute type for 4-bit (\"bfloat16\" or \"float16\").\n",
    "\n",
    "    Returns:\n",
    "        pipeline: Hugging Face text-generation pipeline.\n",
    "    \"\"\"\n",
    "    assert quant_bits in [4, 8], \"Only 4-bit and 8-bit quantization supported.\"\n",
    "\n",
    "    if quant_bits == 4:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=compute_dtype\n",
    "        )\n",
    "    else:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True\n",
    "        )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6adff092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b917a5c5e37450b8323ba4d423dcdcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = load_bnb_model(\"/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct-quantizised/llama-3.1-8B-8bit\", quant_bits=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7615e536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is quantum computing? - Part 2: Quantum algorithms\n",
      "In the previous post, we discussed the basics of quantum computing and the principles that underlie it. In this post, we'll explore the concept of quantum algorithms and their potential to solve complex problems that are currently unsolvable or inefficiently solvable using classical computers.\n",
      "Quantum algorithms are designed to take advantage of the unique properties of quantum mechanics, such as superposition, entanglement, and interference. These algorithms are typically designed to solve specific problems, such\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "output = pipe(\"What is quantum computing?\", max_new_tokens=100)[0][\"generated_text\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431d1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Load test data\n",
    "\n",
    "dataset = load_dataset(\"gigaword\", split=\"test[:3]\")  # Limit to 100 for fast eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e9c922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a headline generation assistant. Given a news article, produce a concise and informative headline.\n",
      "\n",
      "Here are some examples:\n",
      "News: Scientists have discovered a new exoplanet that appears to have water on its surface, raising hopes it may be habitable.\n",
      "Headline: New exoplanet may support life\n",
      "\n",
      "News: The stock market experienced a significant downturn today, with major indices falling sharply amid economic uncertainty.\n",
      "Headline: Stock market plunges amid economic fears\n",
      "\n",
      "Generate just the answer, without repeating the question. Now it's your turn:\n",
      "\n",
      "News: japan 's nec corp. and UNK computer corp. of the united states said wednesday they had agreed to join forces in supercomputer sales .\n",
      "Headline:\n"
     ]
    }
   ],
   "source": [
    "def sum_prompt(document):\n",
    "    \"\"\"\n",
    "    Summarize the given `document` into a concise headline using a few-shot prompt.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a headline generation assistant. Given a news article, produce a concise and informative headline.\\n\\n\"\n",
    "\n",
    "        \"Here are some examples:\\n\"\n",
    "\n",
    "        \"News: Scientists have discovered a new exoplanet that appears to have water on its surface, raising hopes it may be habitable.\\n\"\n",
    "        \"Headline: New exoplanet may support life\\n\\n\"\n",
    "\n",
    "        \"News: The stock market experienced a significant downturn today, with major indices falling sharply amid economic uncertainty.\\n\"\n",
    "        \"Headline: Stock market plunges amid economic fears\\n\\n\"\n",
    "\n",
    "        \"Generate just the answer, without repeating the question. Now it's your turn:\\n\\n\"\n",
    "\n",
    "        f\"News: {document}\\n\"\n",
    "        \"Headline:\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "print(sum_prompt(dataset[0][\"document\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d912d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "\n",
    "model_name = \"Teuken-7B-instruct-research-v0.4\"\n",
    "\n",
    "bm = ModelBenchmark(\n",
    "    backend=\"vllm\",\n",
    "    model_path=f\"/home/ubuntu/fast_llm_inference/{model_name}\",\n",
    "    task=\"summarization\"\n",
    ")\n",
    "\n",
    "# Run the benchmark\n",
    "\n",
    "results = bm.run(samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c111a3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics (mean ± std) for llama-3.1-8B-Instruct:\n",
      "prompt_length                       4143.850000 ± 2039.573492\n",
      "TTFT                                      0.069163 ± 0.001972\n",
      "ATL                                       0.104923 ± 0.031886\n",
      "GL                                        1.144052 ± 0.138419\n",
      "TPS                                      11.054800 ± 2.240038\n",
      "SPS                                       1.015800 ± 0.445172\n",
      "Memory Usage (MB)                     21341.880000 ± 0.000000\n",
      "Model Size (MB)                       15327.360256 ± 0.000000\n",
      "Overhead (MB)                          6014.519744 ± 0.000000\n",
      "GPU_Utilization (%)                      95.810000 ± 7.636561\n",
      "Total Energy (Wh)                         0.021285 ± 0.004518\n",
      "Energy per Token (J/token)                6.274305 ± 1.621546\n",
      "Energy per Sentence (J/sentence)        72.789601 ± 20.600268\n",
      "Energy per Second (W)                    66.472600 ± 9.277863\n",
      "ROUGE-1                                   0.250233 ± 0.133198\n",
      "ROUGE-L                                   0.204199 ± 0.118236\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "results.to_csv(f\"vLLM_results/{model_name}_summarization.csv\", index=False)\n",
    "\n",
    "# Compute statistics\n",
    "numeric_results = results.select_dtypes(include='number')\n",
    "averages = numeric_results.mean()\n",
    "stds = numeric_results.std()\n",
    "\n",
    "# Combine mean ± std into a formatted string\n",
    "summary = averages.combine(stds, lambda mean, std: f\"{mean:.6f} ± {std:.6f}\")\n",
    "\n",
    "# Print formatted summary\n",
    "print(f\"Statistics (mean ± std) for {model_name}:\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
