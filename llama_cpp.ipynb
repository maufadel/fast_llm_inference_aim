{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 08:41:59.448302: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744274519.477011   93301 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744274519.486238   93301 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744274519.515865   93301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744274519.515883   93301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744274519.515885   93301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744274519.515888   93301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-10 08:41:59.523022: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import llama_cpp\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import requests\n",
    "import json\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter Notebook\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "dataset = load_dataset(\"gigaword\", split=\"test[:100]\")  # Limit to 100 for fast eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory released and NVML shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def empty_GPU_cache():\n",
    "    \"\"\"\n",
    "    Clear GPU memory cache.\n",
    "    \"\"\"\n",
    "    del benchmark.model\n",
    "    del benchmark.tokenizer\n",
    "    del benchmark.llm  # If using llama.cpp\n",
    "    torch.cuda.empty_cache()\n",
    "    benchmark.close()  # Shutdown NVML\n",
    "\n",
    "    print(\"GPU memory released and NVML shutdown complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a headline generation assistant. Given a news article, produce a concise and informative headline.\n",
      "\n",
      "Here are some examples:\n",
      "News: Scientists have discovered a new exoplanet that appears to have water on its surface, raising hopes it may be habitable.\n",
      "Headline: New exoplanet may support life\n",
      "\n",
      "News: The stock market experienced a significant downturn today, with major indices falling sharply amid economic uncertainty.\n",
      "Headline: Stock market plunges amid economic fears\n",
      "\n",
      "News: japan 's nec corp. and UNK computer corp. of the united states said wednesday they had agreed to join forces in supercomputer sales .\n",
      "Headline:\n"
     ]
    }
   ],
   "source": [
    "def sum_prompt(document):\n",
    "    \"\"\"\n",
    "    Summarize the given `document` into a concise headline using a few-shot prompt.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a headline generation assistant. Given a news article, produce a concise and informative headline.\\n\\n\"\n",
    "\n",
    "        \"Here are some examples:\\n\"\n",
    "\n",
    "        \"News: Scientists have discovered a new exoplanet that appears to have water on its surface, raising hopes it may be habitable.\\n\"\n",
    "        \"Headline: New exoplanet may support life\\n\\n\"\n",
    "\n",
    "        \"News: The stock market experienced a significant downturn today, with major indices falling sharply amid economic uncertainty.\\n\"\n",
    "        \"Headline: Stock market plunges amid economic fears\\n\\n\"\n",
    "\n",
    "        f\"News: {document}\\n\"\n",
    "        \"Headline:\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "print(sum_prompt(dataset[0][\"document\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization - Gemma3-12B q4_k_m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory released and NVML shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import os\n",
    "\n",
    "model_name = \"google_gemma-3-12b-it-q4_k_m\"\n",
    "\n",
    "llama_model_path = f\"/home/ubuntu/fast_llm_inference/gemma-3-gguf/{model_name}.gguf\"\n",
    "\n",
    "benchmark = ModelBenchmark(\n",
    "    backend=\"llama.cpp\",\n",
    "    task=\"summarization\",\n",
    "    llama_model_path=llama_model_path,\n",
    "    llama_gpu_layers=-1,\n",
    "    max_tokens=20,\n",
    "    model_size= os.path.getsize(llama_model_path) / 1e6, # in MB\n",
    ")\n",
    "\n",
    "results = benchmark.benchmark(prompts=[sum_prompt(i) for i in dataset[\"document\"]], \n",
    "                              references=[i for i in dataset[\"summary\"]])\n",
    "\n",
    "empty_GPU_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages for google_gemma-3-12b-it-q4_k_m:\n",
      "prompt_length                         671.680000\n",
      "ATL                                     0.104720\n",
      "GL                                      0.754810\n",
      "TPS                                     9.670200\n",
      "SPS                                     1.394400\n",
      "Memory Usage (MB)                   10005.360000\n",
      "Model Size (MB)                      7300.575232\n",
      "Total Energy (Wh)                       0.013258\n",
      "Energy per Token (J/token)              7.563756\n",
      "Energy per Sentence (J/sentence)       46.861596\n",
      "Energy per Second (W)                  63.259000\n",
      "ROUGE-1                                 0.360688\n",
      "ROUGE-L                                 0.331437\n",
      "BERTScore                               0.865379\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a CSV file\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "results.to_csv(f\"llama_cpp_results/{model_name}_summarization.csv\", index=False)\n",
    "\n",
    "averages = results.select_dtypes(include='number').mean()\n",
    "\n",
    "print(f\"Averages for {model_name}:\")\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization - Gemma-3-12B q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory released and NVML shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import os\n",
    "\n",
    "model_name = \"google_gemma-3-12b-it-q8\"\n",
    "\n",
    "llama_model_path = f\"/home/ubuntu/fast_llm_inference/gemma-3-gguf/{model_name}.gguf\"\n",
    "\n",
    "benchmark = ModelBenchmark(\n",
    "    backend=\"llama.cpp\",\n",
    "    task=\"summarization\",\n",
    "    llama_model_path=llama_model_path,\n",
    "    llama_gpu_layers=-1,\n",
    "    max_tokens=20,\n",
    "    model_size= os.path.getsize(llama_model_path) / 1e6, # in MB\n",
    ")\n",
    "\n",
    "results = benchmark.benchmark(prompts=[sum_prompt(i) for i in dataset[\"document\"]], \n",
    "                              references=[i for i in dataset[\"summary\"]])\n",
    "\n",
    "empty_GPU_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages for google_gemma-3-12b-it-q8:\n",
      "prompt_length                         671.680000\n",
      "ATL                                     0.168661\n",
      "GL                                      1.229040\n",
      "TPS                                     5.961300\n",
      "SPS                                     0.856300\n",
      "Memory Usage (MB)                   15871.480000\n",
      "Model Size (MB)                     13453.668352\n",
      "Total Energy (Wh)                       0.024536\n",
      "Energy per Token (J/token)             13.864794\n",
      "Energy per Sentence (J/sentence)       86.717467\n",
      "Energy per Second (W)                  71.866500\n",
      "ROUGE-1                                 0.350702\n",
      "ROUGE-L                                 0.321152\n",
      "BERTScore                               0.863860\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "results.to_csv(f\"llama_cpp_results/{model_name}_summarization.csv\", index=False)\n",
    "\n",
    "averages = results.select_dtypes(include='number').mean()\n",
    "\n",
    "print(f\"Averages for {model_name}:\")\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization - llama-3.1-8B-Instruct-Q4_K_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory released and NVML shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import os\n",
    "\n",
    "model_name = \"llama-3.1-8B-Instruct-Q4_K_M\"\n",
    "\n",
    "llama_model_path = f\"/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct-gguf/{model_name}.gguf\"\n",
    "\n",
    "benchmark = ModelBenchmark(\n",
    "    backend=\"llama.cpp\",\n",
    "    task=\"summarization\",\n",
    "    llama_model_path=llama_model_path,\n",
    "    llama_gpu_layers=-1,\n",
    "    max_tokens=20,\n",
    "    model_size= os.path.getsize(llama_model_path) / 1e6, # in MB\n",
    ")\n",
    "\n",
    "results = benchmark.benchmark(prompts=[sum_prompt(i) for i in dataset[\"document\"]], \n",
    "                              references=[i for i in dataset[\"summary\"]])\n",
    "\n",
    "empty_GPU_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages for llama-3.1-8B-Instruct-Q4_K_M:\n",
      "prompt_length                        671.680000\n",
      "ATL                                    0.062315\n",
      "GL                                     0.455436\n",
      "TPS                                   16.308800\n",
      "SPS                                    2.218200\n",
      "Memory Usage (MB)                   7697.880000\n",
      "Model Size (MB)                     4920.734368\n",
      "Total Energy (Wh)                      0.008088\n",
      "Energy per Token (J/token)             4.541464\n",
      "Energy per Sentence (J/sentence)      28.959549\n",
      "Energy per Second (W)                 63.964800\n",
      "ROUGE-1                                0.351636\n",
      "ROUGE-L                                0.322978\n",
      "BERTScore                              0.864453\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a CSV file\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "results.to_csv(f\"llama_cpp_results/{model_name}_summarization.csv\", index=False)\n",
    "\n",
    "averages = results.select_dtypes(include='number').mean()\n",
    "\n",
    "print(f\"Averages for {model_name}:\")\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization - llama-3.1-8B-Instruct-Q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory released and NVML shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import os\n",
    "\n",
    "model_name = \"llama-3.1-8B-Instruct-Q8_0\"\n",
    "\n",
    "llama_model_path = f\"/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct-gguf/{model_name}.gguf\"\n",
    "\n",
    "benchmark = ModelBenchmark(\n",
    "    backend=\"llama.cpp\",\n",
    "    task=\"summarization\",\n",
    "    llama_model_path=llama_model_path,\n",
    "    llama_gpu_layers=-1,\n",
    "    max_tokens=20,\n",
    "    model_size= os.path.getsize(llama_model_path) / 1e6, # in MB\n",
    ")\n",
    "\n",
    "results = benchmark.benchmark(prompts=[sum_prompt(i) for i in dataset[\"document\"]], \n",
    "                              references=[i for i in dataset[\"summary\"]])\n",
    "\n",
    "empty_GPU_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages for llama-3.1-8B-Instruct-Q8_0:\n",
      "prompt_length                         671.680000\n",
      "ATL                                     0.103335\n",
      "GL                                      0.717430\n",
      "TPS                                     9.909000\n",
      "SPS                                     1.407600\n",
      "Memory Usage (MB)                   11919.860000\n",
      "Model Size (MB)                      8540.770976\n",
      "Total Energy (Wh)                       0.012569\n",
      "Energy per Token (J/token)              7.424708\n",
      "Energy per Sentence (J/sentence)       45.000434\n",
      "Energy per Second (W)                  63.087200\n",
      "ROUGE-1                                 0.344395\n",
      "ROUGE-L                                 0.317881\n",
      "BERTScore                               0.864206\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a CSV file\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "results.to_csv(f\"llama_cpp_results/{model_name}_summarization.csv\", index=False)\n",
    "\n",
    "averages = results.select_dtypes(include='number').mean()\n",
    "\n",
    "print(f\"Averages for {model_name}:\")\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization - llama-3.1-8B-Instruct-f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory released and NVML shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import os\n",
    "\n",
    "model_name = \"llama-3.1-8B-Instruct-f16\"\n",
    "\n",
    "llama_model_path = f\"/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct-gguf/{model_name}.gguf\"\n",
    "\n",
    "benchmark = ModelBenchmark(\n",
    "    backend=\"llama.cpp\",\n",
    "    task=\"summarization\",\n",
    "    llama_model_path=llama_model_path,\n",
    "    llama_gpu_layers=-1,\n",
    "    max_tokens=20,\n",
    "    model_size= os.path.getsize(llama_model_path) / 1e6, # in MB\n",
    ")\n",
    "\n",
    "results = benchmark.benchmark(prompts=[sum_prompt(i) for i in dataset[\"document\"]], \n",
    "                              references=[i for i in dataset[\"summary\"]])\n",
    "\n",
    "empty_GPU_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages for llama-3.1-8B-Instruct-f16:\n",
      "prompt_length                         671.680000\n",
      "ATL                                     0.177835\n",
      "GL                                      1.236803\n",
      "TPS                                     5.763100\n",
      "SPS                                     0.815900\n",
      "Memory Usage (MB)                   19667.680000\n",
      "Model Size (MB)                     16068.891296\n",
      "Total Energy (Wh)                       0.024831\n",
      "Energy per Token (J/token)             14.631968\n",
      "Energy per Sentence (J/sentence)       88.945786\n",
      "Energy per Second (W)                  72.276300\n",
      "ROUGE-1                                 0.348157\n",
      "ROUGE-L                                 0.321643\n",
      "BERTScore                               0.864405\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a CSV file\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "results.to_csv(f\"llama_cpp_results/{model_name}_summarization.csv\", index=False)\n",
    "\n",
    "averages = results.select_dtypes(include='number').mean()\n",
    "\n",
    "print(f\"Averages for {model_name}:\")\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt Length</th>\n",
       "      <th>Question (Prompt)</th>\n",
       "      <th>Generated Answer</th>\n",
       "      <th>Reference</th>\n",
       "      <th>ROUGE_avg</th>\n",
       "      <th>BERTScore</th>\n",
       "      <th>FTL (s)</th>\n",
       "      <th>ATL (s)</th>\n",
       "      <th>GL (s)</th>\n",
       "      <th>TPS (tokens/s)</th>\n",
       "      <th>SPS (sentences/s)</th>\n",
       "      <th>Memory Usage (MB)</th>\n",
       "      <th>Model Size (MB)</th>\n",
       "      <th>KV-Cache Size Estimation (MB)</th>\n",
       "      <th>Total Energy (Wh)</th>\n",
       "      <th>Energy per Token (J/token)</th>\n",
       "      <th>Energy per Sentence (J/sentence)</th>\n",
       "      <th>Energy per Second (W)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>499</td>\n",
       "      <td>You are a headline generation assistant. Given...</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>1.3255</td>\n",
       "      <td>11.32</td>\n",
       "      <td>0.75</td>\n",
       "      <td>16922.06</td>\n",
       "      <td>16068.891296</td>\n",
       "      <td>853.168704</td>\n",
       "      <td>0.026255</td>\n",
       "      <td>6.301306</td>\n",
       "      <td>94.519587</td>\n",
       "      <td>71.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>659</td>\n",
       "      <td>You are a headline generation assistant. Given...</td>\n",
       "      <td>Man sentenced to years in jail for murdering h...</td>\n",
       "      <td>a court here thursday sentenced a ##-year-old ...</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.900535</td>\n",
       "      <td>0.0784</td>\n",
       "      <td>0.0784</td>\n",
       "      <td>1.3326</td>\n",
       "      <td>12.76</td>\n",
       "      <td>0.75</td>\n",
       "      <td>16922.06</td>\n",
       "      <td>16068.891296</td>\n",
       "      <td>853.168704</td>\n",
       "      <td>0.026885</td>\n",
       "      <td>5.693390</td>\n",
       "      <td>96.787630</td>\n",
       "      <td>72.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>499</td>\n",
       "      <td>You are a headline generation assistant. Given...</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>1.3436</td>\n",
       "      <td>11.16</td>\n",
       "      <td>0.74</td>\n",
       "      <td>16922.06</td>\n",
       "      <td>16068.891296</td>\n",
       "      <td>853.168704</td>\n",
       "      <td>0.027342</td>\n",
       "      <td>6.561967</td>\n",
       "      <td>98.429501</td>\n",
       "      <td>73.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>635</td>\n",
       "      <td>You are a headline generation assistant. Given...</td>\n",
       "      <td>India wins toss, chooses to bat in Antigua Test</td>\n",
       "      <td>india won the toss and chose to bat on the ope...</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.905336</td>\n",
       "      <td>0.0888</td>\n",
       "      <td>0.0888</td>\n",
       "      <td>1.3316</td>\n",
       "      <td>11.26</td>\n",
       "      <td>0.75</td>\n",
       "      <td>16922.06</td>\n",
       "      <td>16068.891296</td>\n",
       "      <td>853.168704</td>\n",
       "      <td>0.027609</td>\n",
       "      <td>6.626269</td>\n",
       "      <td>99.394037</td>\n",
       "      <td>74.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Prompt Length                                  Question (Prompt)  \\\n",
       "9             499  You are a headline generation assistant. Given...   \n",
       "29            659  You are a headline generation assistant. Given...   \n",
       "43            499  You are a headline generation assistant. Given...   \n",
       "62            635  You are a headline generation assistant. Given...   \n",
       "\n",
       "                                     Generated Answer  \\\n",
       "9                                                 UNK   \n",
       "29  Man sentenced to years in jail for murdering h...   \n",
       "43                                                UNK   \n",
       "62    India wins toss, chooses to bat in Antigua Test   \n",
       "\n",
       "                                            Reference  ROUGE_avg  BERTScore  \\\n",
       "9                                                 UNK   1.000000   1.000000   \n",
       "29  a court here thursday sentenced a ##-year-old ...   0.435897   0.900535   \n",
       "43                                                UNK   1.000000   1.000000   \n",
       "62  india won the toss and chose to bat on the ope...   0.371429   0.905336   \n",
       "\n",
       "    FTL (s)  ATL (s)  GL (s)  TPS (tokens/s)  SPS (sentences/s)  \\\n",
       "9    0.0884   0.0884  1.3255           11.32               0.75   \n",
       "29   0.0784   0.0784  1.3326           12.76               0.75   \n",
       "43   0.0896   0.0896  1.3436           11.16               0.74   \n",
       "62   0.0888   0.0888  1.3316           11.26               0.75   \n",
       "\n",
       "    Memory Usage (MB)  Model Size (MB)  KV-Cache Size Estimation (MB)  \\\n",
       "9            16922.06     16068.891296                     853.168704   \n",
       "29           16922.06     16068.891296                     853.168704   \n",
       "43           16922.06     16068.891296                     853.168704   \n",
       "62           16922.06     16068.891296                     853.168704   \n",
       "\n",
       "    Total Energy (Wh)  Energy per Token (J/token)  \\\n",
       "9            0.026255                    6.301306   \n",
       "29           0.026885                    5.693390   \n",
       "43           0.027342                    6.561967   \n",
       "62           0.027609                    6.626269   \n",
       "\n",
       "    Energy per Sentence (J/sentence)  Energy per Second (W)  \n",
       "9                          94.519587                  71.31  \n",
       "29                         96.787630                  72.63  \n",
       "43                         98.429501                  73.26  \n",
       "62                         99.394037                  74.64  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find best bertscore\n",
    "\n",
    "results.loc[results['BERTScore'] > 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt Length</th>\n",
       "      <th>Question (Prompt)</th>\n",
       "      <th>Generated Answer</th>\n",
       "      <th>Reference</th>\n",
       "      <th>ROUGE_avg</th>\n",
       "      <th>BERTScore</th>\n",
       "      <th>FTL (s)</th>\n",
       "      <th>ATL (s)</th>\n",
       "      <th>GL (s)</th>\n",
       "      <th>TPS (tokens/s)</th>\n",
       "      <th>SPS (sentences/s)</th>\n",
       "      <th>Memory Usage (MB)</th>\n",
       "      <th>Model Size (MB)</th>\n",
       "      <th>KV-Cache Size Estimation (MB)</th>\n",
       "      <th>Total Energy (Wh)</th>\n",
       "      <th>Energy per Token (J/token)</th>\n",
       "      <th>Energy per Sentence (J/sentence)</th>\n",
       "      <th>Energy per Second (W)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>610</td>\n",
       "      <td>You are a headline generation assistant. Given...</td>\n",
       "      <td>Factory orders rise in September</td>\n",
       "      <td>factory orders for manufactured goods rose #.#...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.877697</td>\n",
       "      <td>0.0747</td>\n",
       "      <td>0.0747</td>\n",
       "      <td>1.3450</td>\n",
       "      <td>13.38</td>\n",
       "      <td>0.74</td>\n",
       "      <td>16922.06</td>\n",
       "      <td>16068.891296</td>\n",
       "      <td>853.168704</td>\n",
       "      <td>0.026055</td>\n",
       "      <td>5.211036</td>\n",
       "      <td>93.798656</td>\n",
       "      <td>69.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>499</td>\n",
       "      <td>You are a headline generation assistant. Given...</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>1.3255</td>\n",
       "      <td>11.32</td>\n",
       "      <td>0.75</td>\n",
       "      <td>16922.06</td>\n",
       "      <td>16068.891296</td>\n",
       "      <td>853.168704</td>\n",
       "      <td>0.026255</td>\n",
       "      <td>6.301306</td>\n",
       "      <td>94.519587</td>\n",
       "      <td>71.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>635</td>\n",
       "      <td>You are a headline generation assistant. Given...</td>\n",
       "      <td>France names unchanged team for second test</td>\n",
       "      <td>france , still high after their convincing ##-...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.866549</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>1.3371</td>\n",
       "      <td>13.46</td>\n",
       "      <td>0.75</td>\n",
       "      <td>16922.06</td>\n",
       "      <td>16068.891296</td>\n",
       "      <td>853.168704</td>\n",
       "      <td>0.026208</td>\n",
       "      <td>5.241527</td>\n",
       "      <td>94.347494</td>\n",
       "      <td>70.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>659</td>\n",
       "      <td>You are a headline generation assistant. Given...</td>\n",
       "      <td>Man sentenced to years in jail for murdering h...</td>\n",
       "      <td>a court here thursday sentenced a ##-year-old ...</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.900535</td>\n",
       "      <td>0.0784</td>\n",
       "      <td>0.0784</td>\n",
       "      <td>1.3326</td>\n",
       "      <td>12.76</td>\n",
       "      <td>0.75</td>\n",
       "      <td>16922.06</td>\n",
       "      <td>16068.891296</td>\n",
       "      <td>853.168704</td>\n",
       "      <td>0.026885</td>\n",
       "      <td>5.693390</td>\n",
       "      <td>96.787630</td>\n",
       "      <td>72.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>688</td>\n",
       "      <td>You are a headline generation assistant. Given...</td>\n",
       "      <td>Japan's largest credit union collapses, incurr...</td>\n",
       "      <td>japan 's collapsed kizu credit union , the lar...</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.871471</td>\n",
       "      <td>0.0956</td>\n",
       "      <td>0.0956</td>\n",
       "      <td>1.3383</td>\n",
       "      <td>10.46</td>\n",
       "      <td>0.75</td>\n",
       "      <td>16922.06</td>\n",
       "      <td>16068.891296</td>\n",
       "      <td>853.168704</td>\n",
       "      <td>0.027635</td>\n",
       "      <td>7.106145</td>\n",
       "      <td>99.486031</td>\n",
       "      <td>74.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>499</td>\n",
       "      <td>You are a headline generation assistant. Given...</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>1.3436</td>\n",
       "      <td>11.16</td>\n",
       "      <td>0.74</td>\n",
       "      <td>16922.06</td>\n",
       "      <td>16068.891296</td>\n",
       "      <td>853.168704</td>\n",
       "      <td>0.027342</td>\n",
       "      <td>6.561967</td>\n",
       "      <td>98.429501</td>\n",
       "      <td>73.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>664</td>\n",
       "      <td>You are a headline generation assistant. Given...</td>\n",
       "      <td>Ericsson reaches deal to sell relay production...</td>\n",
       "      <td>swedish telecommunications giant ericsson has ...</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.885304</td>\n",
       "      <td>0.0832</td>\n",
       "      <td>0.0832</td>\n",
       "      <td>1.3316</td>\n",
       "      <td>12.02</td>\n",
       "      <td>0.75</td>\n",
       "      <td>16922.06</td>\n",
       "      <td>16068.891296</td>\n",
       "      <td>853.168704</td>\n",
       "      <td>0.026225</td>\n",
       "      <td>5.900665</td>\n",
       "      <td>94.410639</td>\n",
       "      <td>70.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>693</td>\n",
       "      <td>You are a headline generation assistant. Given...</td>\n",
       "      <td>Prince Philip marks 60 years of marriage to Qu...</td>\n",
       "      <td>britain 's prince philip celebrates his ##th b...</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.847985</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>1.3418</td>\n",
       "      <td>12.67</td>\n",
       "      <td>0.75</td>\n",
       "      <td>16922.06</td>\n",
       "      <td>16068.891296</td>\n",
       "      <td>853.168704</td>\n",
       "      <td>0.026613</td>\n",
       "      <td>5.635722</td>\n",
       "      <td>95.807276</td>\n",
       "      <td>71.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>654</td>\n",
       "      <td>You are a headline generation assistant. Given...</td>\n",
       "      <td>Notre Dame square to be renamed after Pope Joh...</td>\n",
       "      <td>the open square in front of the french capital...</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.859504</td>\n",
       "      <td>0.0835</td>\n",
       "      <td>0.0835</td>\n",
       "      <td>1.3364</td>\n",
       "      <td>11.97</td>\n",
       "      <td>0.75</td>\n",
       "      <td>16922.06</td>\n",
       "      <td>16068.891296</td>\n",
       "      <td>853.168704</td>\n",
       "      <td>0.026276</td>\n",
       "      <td>5.912092</td>\n",
       "      <td>94.593466</td>\n",
       "      <td>70.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Prompt Length                                  Question (Prompt)  \\\n",
       "3             610  You are a headline generation assistant. Given...   \n",
       "9             499  You are a headline generation assistant. Given...   \n",
       "19            635  You are a headline generation assistant. Given...   \n",
       "29            659  You are a headline generation assistant. Given...   \n",
       "41            688  You are a headline generation assistant. Given...   \n",
       "43            499  You are a headline generation assistant. Given...   \n",
       "56            664  You are a headline generation assistant. Given...   \n",
       "81            693  You are a headline generation assistant. Given...   \n",
       "95            654  You are a headline generation assistant. Given...   \n",
       "\n",
       "                                     Generated Answer  \\\n",
       "3                    Factory orders rise in September   \n",
       "9                                                 UNK   \n",
       "19        France names unchanged team for second test   \n",
       "29  Man sentenced to years in jail for murdering h...   \n",
       "41  Japan's largest credit union collapses, incurr...   \n",
       "43                                                UNK   \n",
       "56  Ericsson reaches deal to sell relay production...   \n",
       "81  Prince Philip marks 60 years of marriage to Qu...   \n",
       "95  Notre Dame square to be renamed after Pope Joh...   \n",
       "\n",
       "                                            Reference  ROUGE_avg  BERTScore  \\\n",
       "3   factory orders for manufactured goods rose #.#...   0.400000   0.877697   \n",
       "9                                                 UNK   1.000000   1.000000   \n",
       "19  france , still high after their convincing ##-...   0.400000   0.866549   \n",
       "29  a court here thursday sentenced a ##-year-old ...   0.435897   0.900535   \n",
       "41  japan 's collapsed kizu credit union , the lar...   0.410256   0.871471   \n",
       "43                                                UNK   1.000000   1.000000   \n",
       "56  swedish telecommunications giant ericsson has ...   0.545455   0.885304   \n",
       "81  britain 's prince philip celebrates his ##th b...   0.409091   0.847985   \n",
       "95  the open square in front of the french capital...   0.487179   0.859504   \n",
       "\n",
       "    FTL (s)  ATL (s)  GL (s)  TPS (tokens/s)  SPS (sentences/s)  \\\n",
       "3    0.0747   0.0747  1.3450           13.38               0.74   \n",
       "9    0.0884   0.0884  1.3255           11.32               0.75   \n",
       "19   0.0743   0.0743  1.3371           13.46               0.75   \n",
       "29   0.0784   0.0784  1.3326           12.76               0.75   \n",
       "41   0.0956   0.0956  1.3383           10.46               0.75   \n",
       "43   0.0896   0.0896  1.3436           11.16               0.74   \n",
       "56   0.0832   0.0832  1.3316           12.02               0.75   \n",
       "81   0.0789   0.0789  1.3418           12.67               0.75   \n",
       "95   0.0835   0.0835  1.3364           11.97               0.75   \n",
       "\n",
       "    Memory Usage (MB)  Model Size (MB)  KV-Cache Size Estimation (MB)  \\\n",
       "3            16922.06     16068.891296                     853.168704   \n",
       "9            16922.06     16068.891296                     853.168704   \n",
       "19           16922.06     16068.891296                     853.168704   \n",
       "29           16922.06     16068.891296                     853.168704   \n",
       "41           16922.06     16068.891296                     853.168704   \n",
       "43           16922.06     16068.891296                     853.168704   \n",
       "56           16922.06     16068.891296                     853.168704   \n",
       "81           16922.06     16068.891296                     853.168704   \n",
       "95           16922.06     16068.891296                     853.168704   \n",
       "\n",
       "    Total Energy (Wh)  Energy per Token (J/token)  \\\n",
       "3            0.026055                    5.211036   \n",
       "9            0.026255                    6.301306   \n",
       "19           0.026208                    5.241527   \n",
       "29           0.026885                    5.693390   \n",
       "41           0.027635                    7.106145   \n",
       "43           0.027342                    6.561967   \n",
       "56           0.026225                    5.900665   \n",
       "81           0.026613                    5.635722   \n",
       "95           0.026276                    5.912092   \n",
       "\n",
       "    Energy per Sentence (J/sentence)  Energy per Second (W)  \n",
       "3                          93.798656                  69.74  \n",
       "9                          94.519587                  71.31  \n",
       "19                         94.347494                  70.56  \n",
       "29                         96.787630                  72.63  \n",
       "41                         99.486031                  74.34  \n",
       "43                         98.429501                  73.26  \n",
       "56                         94.410639                  70.90  \n",
       "81                         95.807276                  71.40  \n",
       "95                         94.593466                  70.78  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find best ROUGE\n",
    "\n",
    "results.loc[results['ROUGE_avg'].between(0.4, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA 3.1 8B Summarization Benchmark\n",
    "\n",
    "<small>\n",
    "\n",
    "#### 📝 Tested Models\n",
    "| **Model File**                      | **Precision** | **Quantization Scheme** | **Notes**                                     |\n",
    "|-------------------------------------|---------------|-------------------------|-----------------------------------------------|\n",
    "| llama-3.1-8B-f16.gguf               | float16       | Full Precision          | Largest model, highest theoretical accuracy, slowest inference |\n",
    "| llama-3.1-8B-Q8_0.gguf              | 8-bit         | Q8_0                   | Reduced size, good balance of speed and quality |\n",
    "| llama-3.1-8B-Q4_K_M.gguf            | 4-bit         | Q4_K_M                 | Highly optimized 4-bit quantization, best summarization quality in tests |\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Summarization Benchmark – Quality, Latency, Efficiency\n",
    "\n",
    "<small>\n",
    "\n",
    "| **Metric**                           | **llama-3.1-8B-Instruct-f16** | **llama-3.1-8B-Instruct-Q8_0** | **llama-3.1-8B-Instruct-Q4_K_M** | **google_gemma-3-12b-it-Q4_K_M** | **google_gemma-3-12b-it-Q8** |\n",
    "|--------------------------------------|-------------------------------|--------------------------------|----------------------------------|----------------------------------|------------------------------|\n",
    "| **Number of Samples**                | 100                           | 100                            | 100                              | 100                              | 100                          |\n",
    "| **Prompt Length (avg)**              | 671.6800 tokens               | 671.6800 tokens                | 671.6800 tokens                  | 671.6800 tokens                  | 671.6800 tokens              |\n",
    "| **ROUGE-1**                          | 0.3482                        | 0.3444                         | 0.3516                           | **0.3607**                       | 0.3507                       |\n",
    "| **ROUGE-L**                          | 0.3216                        | 0.3179                         | 0.3230                           | **0.3314**                       | 0.3212                       |\n",
    "| **BERTScore**                        | 0.8644                        | 0.8642                         | 0.8645                           | **0.8654**                       | 0.8639                       |\n",
    "| **ATL (s)**                          | 0.1778                        | 0.1033                         | **0.0623**                       | 0.1047                           | 0.1687                       |\n",
    "| **GL (s)**                           | 1.2368                        | 0.7174                         | **0.4554**                       | 0.7548                           | 1.2290                       |\n",
    "| **TPS (tokens/s)**                   | 5.7631                        | 9.9090                         | **16.3088**                      | 9.6702                           | 5.9613                       |\n",
    "| **SPS (sentences/s)**                | 0.8159                        | 1.4076                         | **2.2182**                       | 1.3944                           | 0.8563                       |\n",
    "| **Memory Usage (MB)**                | 19667.68 MB                   | 11919.86 MB                    | **7697.88 MB**                   | 10005.36 MB                      | 15871.48 MB                  |\n",
    "| **Model Size (MB)**                  | 16068.89 MB                   | 8540.77 MB                     | **4920.73 MB**                   | 7300.58 MB                       | 13453.67 MB                  |\n",
    "| **Total Energy (Wh)**                | 0.024831                      | 0.012569                       | **0.008088**                     | 0.013258                         | 0.024536                     |\n",
    "| **Energy per Token (J/token)**       | 14.6320                       | 7.4247                         | **4.5415**                       | 7.5638                           | 13.8648                      |\n",
    "| **Energy per Sentence (J/sentence)** | 88.9458                       | 45.0004                        | **28.9595**                      | 46.8616                          | 86.7175                      |\n",
    "| **Energy per Second (W)**            | 72.2763 W                     | **63.0872 W**                  | 63.9648 W                        | 63.2590 W                        | 71.8665 W                    |\n",
    "\n",
    "</small>\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔍 Summary of Insights\n",
    "\n",
    "- **4-bit quantized models (Q4_K_M)** outperformed both 8-bit and float16 variants across nearly all benchmark metrics, including **ROUGE**, **BERTScore**, **inference latency** (FTL/GL), and **energy efficiency**.\n",
    "- Despite using the lowest numerical precision, **Q4_K_M** achieved the **highest summarization quality**, **fastest generation speed**, and **lowest memory and energy usage**, making it the best overall performer in this benchmark.\n",
    "- Surprisingly, the more precise **8-bit (Q8_0)** and **float16** models scored lower in **ROUGE** and **BERTScore**, while also being slower and less efficient. Possible explanations include:\n",
    "  - Differences in **prompt formatting** or few-shot setup\n",
    "  - Variations in **sampling parameters** (e.g., temperature, top-p)\n",
    "  - Potential mismatches in **model variants** (e.g., base vs. instruction-tuned)\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚡ Why Quantized Models Are Faster\n",
    "\n",
    "- **Smaller memory footprint**  \n",
    "  ➤ Less data to move = faster memory access (critical for LLMs).\n",
    "\n",
    "- **Faster matrix multiplications**  \n",
    "  ➤ Quantized operations use low-bit integer math, which is much faster on both CPUs and GPUs.\n",
    "\n",
    "- **Reduced KV cache size**  \n",
    "  ➤ Speeds up attention mechanisms and reduces memory bandwidth usage during generation.\n",
    "\n",
    "- **Lower VRAM/RAM pressure**  \n",
    "  ➤ Enables more efficient layer-wise parallelism and fewer memory stalls.\n",
    "\n",
    "- **Hardware-accelerated low-bit operations**  \n",
    "  ➤ Modern CPUs and GPUs (e.g., Apple Silicon, NVIDIA Tensor Cores) are optimized for INT4/INT8 computation.\n",
    "\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 Q4_K_M vs Q8_0 Quantization Comparison\n",
    "\n",
    "<small>\n",
    "\n",
    "Both **Q4_K_M** and **Q8_0** are quantization methods used to compress model weights for faster inference and lower memory usage.\n",
    "\n",
    "---\n",
    "\n",
    "#### Q4_K_M (4-bit Quantization, Optimized)\n",
    "\n",
    "| Feature          | Description |\n",
    "|------------------|-------------|\n",
    "| **Precision**    | 4-bit |\n",
    "| **Quantization Type** | \"K\" series, specifically **K_M** (multi-purpose optimized) |\n",
    "| **Compression**  | Very high (significantly smaller than 8-bit) |\n",
    "| **Speed**        | Extremely fast, ideal for CPU/GPU |\n",
    "| **Memory Usage** | Very low (fits on smaller GPUs like 6-8GB VRAM) |\n",
    "| **Accuracy**     | Preserves high accuracy in **instruction-tuned tasks** like **summarization**, **chat**, and **QA** |\n",
    "| **Best Use Cases** | Chatbots, summarization, reasoning tasks |\n",
    "| **Notes**        | Uses **groupwise quantization** and **per-channel scaling** for better accuracy retention despite low precision |\n",
    "\n",
    "---\n",
    "\n",
    "#### Q8_0 (8-bit Quantization, General Purpose)\n",
    "\n",
    "| Feature          | Description |\n",
    "|------------------|-------------|\n",
    "| **Precision**    | 8-bit |\n",
    "| **Quantization Type** | Uniform 8-bit |\n",
    "| **Compression**  | Moderate (smaller than float16 but larger than 4-bit) |\n",
    "| **Speed**        | Faster than float16, but slower than Q4_K_M |\n",
    "| **Memory Usage** | Moderate (needs more VRAM, typically 12GB+) |\n",
    "| **Accuracy**     | Higher precision retention in general, but not optimized for specific tasks |\n",
    "| **Best Use Cases** | Complex reasoning, precision-sensitive tasks |\n",
    "| **Notes**        | General-purpose quantization without task-specific optimizations |\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Why Q4_K_M Outperformed Q8_0 in Summarization\n",
    "- **Q4_K_M** is optimized for **task-specific performance**, often giving better results for **instruction-tuned models**, **summarization**, and **chat** tasks.\n",
    "- **Q8_0** retains more raw precision but isn't tuned for these tasks, leading to lower scores in ROUGE evaluation.\n",
    "- **Q4_K_M** also runs significantly faster with less resource usage.\n",
    "\n",
    "---\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 How Q4_K_M Is Optimized\n",
    "\n",
    "<small>\n",
    "\n",
    "Q4_K_M is part of the advanced **K series** quantization schemes, designed to balance **speed**, **size**, and **accuracy**. It introduces several optimizations to maintain high task performance despite being a 4-bit quantization.\n",
    "\n",
    "#### Key Optimizations\n",
    "\n",
    "| Optimization                   | Description |\n",
    "|--------------------------------|-------------|\n",
    "| **Groupwise Quantization**     | Weights are divided into small groups (e.g., 32 or 64) and quantized individually, improving precision retention. |\n",
    "| **Per-Channel Scaling**        | Each group or channel has its own scale factor, ensuring finer control over the quantization process. |\n",
    "| **Mixed Weight Packing (M)**   | Uses different packing strategies optimized for different layers (e.g., attention vs MLP layers). |\n",
    "| **Dynamic Zero Points**        | Zero points are dynamically computed within groups, reducing quantization bias. |\n",
    "| **Efficient SIMD Utilization** | The packed format is optimized for vectorized operations on CPU and GPU, increasing inference speed. |\n",
    "\n",
    "#### Why It Works Well\n",
    "- Optimized for **instruction-following**, **summarization**, and **chat** tasks.\n",
    "- Preserves task-critical accuracy despite aggressive compression.\n",
    "- Runs **very efficiently** on both CPU and GPU.\n",
    "\n",
    "#### Q4_K_M vs Q8_0\n",
    "\n",
    "| Feature            | Q4_K_M                   | Q8_0                 |\n",
    "|--------------------|--------------------------|----------------------|\n",
    "| Precision          | 4-bit                    | 8-bit               |\n",
    "| Compression        | High                     | Medium              |\n",
    "| Accuracy Retention | High (task-optimized)    | High (general)      |\n",
    "| Speed              | Very fast                | Fast                |\n",
    "| Memory Usage       | Very low                 | Medium              |\n",
    "| Task Tuning        | Summarization, Chat, QA  | General-purpose     |\n",
    "| Ollama Use         | ✅ Often used (default)  | ❌ Less common       |\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Load SQuAD v2 dataset (validation split)\n",
    "squad_v2 = load_dataset(\"squad_v2\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Convert the validation split to a list and sample 200 random questions\n",
    "validation_list = list(squad_v2[\"validation\"])\n",
    "sampled_questions = random.sample(validation_list, 200)\n",
    "\n",
    "questions_with_answers = [i for i in sampled_questions if len(i['answers']['text']) > 0]\n",
    "\n",
    "len(questions_with_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_prompt(example):\n",
    "    context = example['context']\n",
    "    question = example['question']\n",
    "\n",
    "    prompt_template = (\n",
    "        \"You are a question answering assistant. Given the context, answer the question. \"\n",
    "        \"If the answer isn't in the context, respond 'I don't know'.\\n\\n\"\n",
    "\n",
    "        \"Here is an example:\\n\"\n",
    "        \"Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni)...\\n\"\n",
    "        \"Question: What is the name of the region the Normans gave their name to?\\n\"\n",
    "        \"Answer: Normandy\\n\\n\"\n",
    "\n",
    "        \"Context: {context}\\n\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    return prompt_template.format(context=context, question=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemma-3-12B Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import os\n",
    "\n",
    "model_name = \"google_gemma-3-12b-it-q4_k_m\"\n",
    "\n",
    "llama_model_path = f\"/home/ubuntu/fast_llm_inference/gemma-3-gguf/{model_name}.gguf\"\n",
    "\n",
    "benchmark = ModelBenchmark(\n",
    "    backend=\"llama.cpp\",\n",
    "    task=\"qa\",\n",
    "    llama_model_path=llama_model_path,\n",
    "    llama_gpu_layers=-1,\n",
    "    max_tokens=20,\n",
    "    model_size= os.path.getsize(llama_model_path) / 1e6, # in MB\n",
    ")\n",
    "\n",
    "results = benchmark.benchmark(prompts=[qa_prompt(i) for i in questions_with_answers], \n",
    "                              references=[quest['answers']['text'] for quest in questions_with_answers])\n",
    "\n",
    "empty_GPU_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages for google_gemma-3-12b-it-q4_k_m:\n",
      "prompt_length                       1248.297030\n",
      "ATL                                    0.079040\n",
      "GL                                     0.438619\n",
      "TPS                                   10.692970\n",
      "SPS                                    3.764950\n",
      "Memory Usage (MB)                   8965.881782\n",
      "Model Size (MB)                     7300.575232\n",
      "Total Energy (Wh)                      0.008629\n",
      "Energy per Token (J/token)             8.988606\n",
      "Energy per Sentence (J/sentence)      23.808748\n",
      "Energy per Second (W)                 70.761386\n",
      "exact_match                            0.613861\n",
      "F1_score                               0.828935\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a CSV file\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "results.to_csv(f\"llama_cpp_results/{model_name}_Q&A.csv\", index=False)\n",
    "\n",
    "averages = results.select_dtypes(include='number').mean()\n",
    "\n",
    "print(f\"Averages for {model_name}:\")\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory released and NVML shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import os\n",
    "\n",
    "model_name = \"google_gemma-3-12b-it-q8\"\n",
    "\n",
    "llama_model_path = f\"/home/ubuntu/fast_llm_inference/gemma-3-gguf/{model_name}.gguf\"\n",
    "\n",
    "benchmark = ModelBenchmark(\n",
    "    backend=\"llama.cpp\",\n",
    "    task=\"qa\",\n",
    "    llama_model_path=llama_model_path,\n",
    "    llama_gpu_layers=-1,\n",
    "    max_tokens=20,\n",
    "    model_size= os.path.getsize(llama_model_path) / 1e6, # in MB\n",
    ")\n",
    "\n",
    "results = benchmark.benchmark(prompts=[qa_prompt(i) for i in questions_with_answers], \n",
    "                              references=[quest['answers']['text'] for quest in questions_with_answers])\n",
    "\n",
    "empty_GPU_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages for google_gemma-3-12b-it-q8:\n",
      "prompt_length                        1248.297030\n",
      "ATL                                     0.118337\n",
      "GL                                      0.612650\n",
      "TPS                                     7.068416\n",
      "SPS                                     2.837030\n",
      "Memory Usage (MB)                   14836.515446\n",
      "Model Size (MB)                     13453.668352\n",
      "Total Energy (Wh)                       0.012074\n",
      "Energy per Token (J/token)             12.978702\n",
      "Energy per Sentence (J/sentence)       33.038025\n",
      "Energy per Second (W)                  70.939703\n",
      "exact_match                             0.663366\n",
      "F1_score                                0.855050\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a CSV file\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "results.to_csv(f\"llama_cpp_results/{model_name}_Q&A.csv\", index=False)\n",
    "\n",
    "averages = results.select_dtypes(include='number').mean()\n",
    "\n",
    "print(f\"Averages for {model_name}:\")\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama3.1-8B Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import os\n",
    "\n",
    "model_name = \"llama-3.1-8B-Instruct-Q4_K_M\"\n",
    "\n",
    "llama_model_path = f\"/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct-gguf/{model_name}.gguf\"\n",
    "\n",
    "benchmark = ModelBenchmark(\n",
    "    backend=\"llama.cpp\",\n",
    "    llama_model_path=llama_model_path,\n",
    "    llama_gpu_layers=-1,\n",
    "    max_tokens=10,\n",
    "    model_size= os.path.getsize(llama_model_path) / 1e6, # in MB\n",
    ")\n",
    "\n",
    "answers = [x['answers']['text'] for x in questions_with_answers]\n",
    "\n",
    "results = benchmark.benchmark([qa_prompt(i) for i in questions_with_answers], answers, task=\"qa\")\n",
    "\n",
    "empty_GPU_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_length                       1248.297030\n",
      "ATL                                    0.059390\n",
      "GL                                     0.414821\n",
      "TPS                                   18.133267\n",
      "SPS                                    3.017030\n",
      "Memory Usage (MB)                   5933.545149\n",
      "Model Size (MB)                     4920.734368\n",
      "Total Energy (Wh)                      0.004908\n",
      "Energy per Token (J/token)             2.533958\n",
      "Energy per Sentence (J/sentence)      15.793119\n",
      "Energy per Second (W)                 42.552673\n",
      "exact_match                            0.663366\n",
      "F1_score                               0.853918\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a CSV file\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "results.to_csv(f\"llama_cpp_results/{model_name}_Q&A.csv\", index=False)\n",
    "\n",
    "averages = results.select_dtypes(include='number').mean()\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA 3.1 8B Instruct - Quantization Benchmark (SQuAD v2 Q&A)\n",
    "\n",
    "<small>\n",
    "\n",
    "| **Metric**                        | **4-bit (Q4_K_M)**              | **8-bit (Q8_0)**              | **fp16**                      | **GEMMA Q4_K_M**             | **GEMMA Q8**               |\n",
    "|-----------------------------------|---------------------------------|-------------------------------|-------------------------------|-------------------------------|-----------------------------|\n",
    "| **Quantization Technique**        | 4-bit Group Quantization (Q4_K_M) | 8-bit Quantization (Q8_0)    | FP16 (Half-Precision Float)   | 4-bit Group Quantization (Q4_K_M) | 8-bit Quantization (Q8) |\n",
    "| **Number of Examples**            | 101                             | 101                           | 101                           | 101                           | 101                         |\n",
    "| **Prompt Length (avg)**           | 1248.2970 tokens                | 1248.2970 tokens              | 1248.2970 tokens              | 1248.2970 tokens              | 1248.2970 tokens            |\n",
    "| **ATL (Average Token Latency)**   | **0.0539 s/token**              | 0.0736 s/token                | 0.1077 s/token                | 0.0790 s/token                | 0.1183 s/token              |\n",
    "| **GL (Generation Latency)**       | **0.3762 s**                    | 0.5008 s                      | 0.7518 s                      | 0.4386 s                      | 0.6127 s                    |\n",
    "| **TPS (Tokens/sec)**              | **19.8857**                     | 14.5372                       | 9.7952                        | 10.6930                       | 7.0684                      |\n",
    "| **SPS (Sentences/sec)**           | 3.3143                          | 2.6915                        | 1.7902                        | **3.7649**                    | 2.8370                      |\n",
    "| **Memory Usage (MB)**             | **5933.5451 MB**                | 9135.4461 MB                  | 15912.1392 MB                 | 8965.8818 MB                  | 14836.5154 MB               |\n",
    "| **Model Size (MB)**               | **4920.7344 MB**                | 8540.7710 MB                  | 16068.8913 MB                 | 7300.5752 MB                  | 13453.6684 MB               |\n",
    "| **Total Energy (Wh)**             | **0.0072 Wh**                   | 0.0095 Wh                     | 0.0147 Wh                     | 0.0086 Wh                     | 0.0121 Wh                   |\n",
    "| **Energy per Token (J/token)**    | **3.6804**                      | 5.0064                        | 7.5682                        | 8.9886                        | 12.9787                     |\n",
    "| **Energy per Sentence (J/sentence)** | **22.9896**                   | 29.2293                       | 44.9827                       | 23.8087                       | 33.0380                     |\n",
    "| **Energy per Second (W)**         | **68.7679 W**                   | 68.5086 W                     | 70.4359 W                     | 70.7614 W                     | 70.9397 W                   |\n",
    "| **Exact Match (EM)**              | 0.6634                          | **0.6733**                    | 0.6634                        | 0.6139                        | 0.6634                      |\n",
    "| **F1 Score**                      | 0.8539                          | 0.8498                        | 0.8399                        | 0.8289                        | **0.8551**                  |\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-to-SQL - Quantization Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How many heads of the departments are older than 56 ?\n",
      "SQL: SELECT count(*) FROM head WHERE age  >  56\n",
      "Number of samples: 100\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "spider = load_dataset(\"spider\", split=\"train[:100]\")\n",
    "\n",
    "# Sample\n",
    "print(\"Q:\", spider[0][\"question\"])\n",
    "print(\"SQL:\", spider[0][\"query\"])\n",
    "\n",
    "print(\"Number of samples:\", len(spider))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a SQL query generation assistant. Given a natural language question, generate the corresponding SQL query. Only generate valid SQL statements, no text!\n",
      "\n",
      "Here is an example:\n",
      "\n",
      "Question: How many heads of the departments are older than 56?\n",
      "\n",
      "Tables in the database:\n",
      "Table 'department': columns = Department_ID, Name, Creation, Ranking, Budget_in_Billions, Num_Employees\n",
      "Table 'head': columns = head_ID, name, born_state, age\n",
      "Table 'management': columns = department_ID, head_ID, temporary_acting\n",
      "\n",
      "SQL: SELECT count(*) FROM head WHERE age > 56\n",
      "\n",
      "Question: How many heads of the departments are older than 56 ?\n",
      "\n",
      "Tables in the database:\n",
      "Table 'department': columns = Department_ID, Name, Creation, Ranking, Budget_in_Billions, Num_Employees\n",
      "Table 'head': columns = head_ID, name, born_state, age\n",
      "Table 'management': columns = department_ID, head_ID, temporary_acting\n",
      "\n",
      "SQL:\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to extract and format columns grouped by table\n",
    "def get_table_columns(db_id: str) -> str:\n",
    "\n",
    "    # Load tables.json\n",
    "    with open(\"/home/ubuntu/fast_llm_inference/tables.json\", \"r\") as f:\n",
    "        tables_json = json.load(f)\n",
    "\n",
    "    for db in tables_json:\n",
    "        if db[\"db_id\"] == db_id:\n",
    "            table_names = db[\"table_names_original\"]\n",
    "            column_info = db[\"column_names_original\"]\n",
    "            \n",
    "            table_columns = {table: [] for table in table_names}\n",
    "            for table_idx, col_name in column_info:\n",
    "                if col_name == \"*\":\n",
    "                    continue  # skip wildcard\n",
    "                table = table_names[table_idx]\n",
    "                table_columns[table].append(col_name)\n",
    "            \n",
    "            # Now build the formatted string\n",
    "            lines = []\n",
    "            for table, cols in table_columns.items():\n",
    "                lines.append(f\"Table '{table}': columns = {', '.join(cols)}\")\n",
    "            return \"\\n\".join(lines)\n",
    "    \n",
    "    return \"No schema found for that db_id.\"\n",
    "\n",
    "def sql_prompt(dataset, index):\n",
    "    question = dataset[\"question\"][index]\n",
    "    db_id = dataset[\"db_id\"][index]\n",
    "    columns_str = get_table_columns(db_id)\n",
    "\n",
    "    prompt_template = (\n",
    "        \"You are a SQL query generation assistant. Given a natural language question, generate the corresponding SQL query. Only generate valid SQL statements, no text!\\n\\n\"\n",
    "        \n",
    "        \"Here is an example:\\n\\n\"\n",
    "\n",
    "        \"Question: How many heads of the departments are older than 56?\\n\\n\"\n",
    "\n",
    "        \"Tables in the database:\\n\"\n",
    "        \"Table 'department': columns = Department_ID, Name, Creation, Ranking, Budget_in_Billions, Num_Employees\\n\"\n",
    "        \"Table 'head': columns = head_ID, name, born_state, age\\n\"\n",
    "        \"Table 'management': columns = department_ID, head_ID, temporary_acting\\n\\n\"\n",
    "        \"SQL: SELECT count(*) FROM head WHERE age > 56\\n\\n\"\n",
    "        \n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \n",
    "        \"Tables in the database:\\n\"\n",
    "        \"{column_context}\\n\\n\"\n",
    "        \"SQL:\"\n",
    "    )\n",
    "    \n",
    "    return prompt_template.format(\n",
    "        question=question,\n",
    "        column_context=columns_str\n",
    "    )\n",
    "\n",
    "prompt = sql_prompt(spider, 0)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-to-SQL Gemma3 12B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import os\n",
    "\n",
    "model_name = \"google_gemma-3-12b-it-q4_k_m\"\n",
    "\n",
    "llama_model_path = f\"/home/ubuntu/fast_llm_inference/gemma-3-gguf/{model_name}.gguf\"\n",
    "\n",
    "benchmark = ModelBenchmark(\n",
    "    backend=\"llama.cpp\",\n",
    "    task=\"sql\",\n",
    "    llama_model_path=llama_model_path,\n",
    "    llama_gpu_layers=-1,\n",
    "    max_tokens=70,\n",
    "    model_size= os.path.getsize(llama_model_path) / 1e6, # in MB\n",
    ")\n",
    "\n",
    "dataset = spider\n",
    "n = len(dataset['query'])\n",
    "\n",
    "results = benchmark.benchmark([sql_prompt(dataset, i) for i in range(n)], [dataset['query'][i] for i in range(n)])\n",
    "\n",
    "empty_GPU_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages for google_gemma-3-12b-it-q4_k_m:\n",
      "prompt_length                       1206.040000\n",
      "ATL                                    0.274330\n",
      "GL                                     2.596460\n",
      "TPS                                    4.943200\n",
      "SPS                                    0.784500\n",
      "Memory Usage (MB)                   8960.040000\n",
      "Model Size (MB)                     7300.575232\n",
      "Total Energy (Wh)                      0.051605\n",
      "Energy per Token (J/token)            19.617788\n",
      "Energy per Sentence (J/sentence)     147.776944\n",
      "Energy per Second (W)                 71.542000\n",
      "AST_equal                              0.440000\n",
      "Normalized_equal                       0.450000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a CSV file\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "results.to_csv(f\"llama_cpp_results/{model_name}_sql.csv\", index=False)\n",
    "\n",
    "averages = results.select_dtypes(include='number').mean()\n",
    "\n",
    "print(f\"Averages for {model_name}:\")\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import os\n",
    "\n",
    "model_name = \"google_gemma-3-12b-it-q8\"\n",
    "\n",
    "llama_model_path = f\"/home/ubuntu/fast_llm_inference/gemma-3-gguf/{model_name}.gguf\"\n",
    "\n",
    "benchmark = ModelBenchmark(\n",
    "    backend=\"llama.cpp\",\n",
    "    task=\"sql\",\n",
    "    llama_model_path=llama_model_path,\n",
    "    llama_gpu_layers=-1,\n",
    "    max_tokens=70,\n",
    "    model_size= os.path.getsize(llama_model_path) / 1e6, # in MB\n",
    ")\n",
    "\n",
    "dataset = spider\n",
    "n = len(dataset['query'])\n",
    "\n",
    "results = benchmark.benchmark([sql_prompt(dataset, i) for i in range(n)], [dataset['query'][i] for i in range(n)])\n",
    "\n",
    "empty_GPU_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages for google_gemma-3-12b-it-q8:\n",
      "prompt_length                        1206.040000\n",
      "ATL                                     0.420400\n",
      "GL                                      4.020178\n",
      "TPS                                     3.446400\n",
      "SPS                                     0.565400\n",
      "Memory Usage (MB)                   14827.740000\n",
      "Model Size (MB)                     13453.668352\n",
      "Total Energy (Wh)                       0.080359\n",
      "Energy per Token (J/token)             30.256650\n",
      "Energy per Sentence (J/sentence)      221.414288\n",
      "Energy per Second (W)                  71.959600\n",
      "AST_equal                               0.390000\n",
      "Normalized_equal                        0.390000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a CSV file\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "results.to_csv(f\"llama_cpp_results/{model_name}_sql.csv\", index=False)\n",
    "\n",
    "averages = results.select_dtypes(include='number').mean()\n",
    "\n",
    "print(f\"Averages for {model_name}:\")\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-to-SQL LLama3.1 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import os\n",
    "import torch\n",
    "\n",
    "model_name = \"llama-3.1-8B-Instruct-f16\"\n",
    "\n",
    "llama_model_path = f\"/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct-gguf/{model_name}.gguf\"\n",
    "\n",
    "benchmark = ModelBenchmark(\n",
    "    backend=\"llama.cpp\",\n",
    "    task=\"sql\",\n",
    "    llama_model_path=llama_model_path,\n",
    "    llama_gpu_layers=-1,\n",
    "    max_tokens=70,\n",
    "    model_size= os.path.getsize(llama_model_path) / 1e6, # in MB\n",
    ")\n",
    "\n",
    "dataset = spider\n",
    "n = len(dataset['query'])\n",
    "\n",
    "results = benchmark.benchmark([sql_prompt(dataset, i) for i in range(n)], [dataset['query'][i] for i in range(n)])\n",
    "\n",
    "empty_GPU_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages for llama-3.1-8B-Instruct-f16:\n",
      "prompt_length                        1206.040000\n",
      "ATL                                     0.442202\n",
      "GL                                      4.346079\n",
      "TPS                                     2.947900\n",
      "SPS                                     0.370000\n",
      "Memory Usage (MB)                   15514.060000\n",
      "Model Size (MB)                     16068.891296\n",
      "Total Energy (Wh)                       0.086791\n",
      "Energy per Token (J/token)             31.787949\n",
      "Energy per Sentence (J/sentence)      278.841249\n",
      "Energy per Second (W)                  71.892400\n",
      "AST_equal                               0.330000\n",
      "Normalized_equal                        0.320000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a CSV file\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "results.to_csv(f\"llama_cpp_results/{model_name}_sql.csv\", index=False)\n",
    "\n",
    "averages = results.select_dtypes(include='number').mean()\n",
    "\n",
    "print(f\"Averages for {model_name}:\")\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages for llama-3.1-8B-Instruct-Q8_0:\n",
      "prompt_length                       1160.040000\n",
      "ATL                                    0.291190\n",
      "GL                                     2.809894\n",
      "TPS                                    4.425600\n",
      "SPS                                    0.551400\n",
      "Memory Usage (MB)                   9124.060000\n",
      "Model Size (MB)                     8540.770976\n",
      "Total Energy (Wh)                      0.055664\n",
      "Energy per Token (J/token)            20.767205\n",
      "Energy per Sentence (J/sentence)     178.564317\n",
      "Energy per Second (W)                 71.318700\n",
      "AST_equal                              0.320000\n",
      "Normalized_equal                       0.320000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a CSV file\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "results.to_csv(f\"llama_cpp_results/{model_name}_sql.csv\", index=False)\n",
    "\n",
    "averages = results.select_dtypes(include='number').mean()\n",
    "\n",
    "print(f\"Averages for {model_name}:\")\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages for llama-3.1-8B-Instruct-Q4_K_M:\n",
      "prompt_length                       1160.040000\n",
      "ATL                                    0.140297\n",
      "GL                                     1.367014\n",
      "TPS                                    8.920900\n",
      "SPS                                    0.961000\n",
      "Memory Usage (MB)                   5922.060000\n",
      "Model Size (MB)                     4920.734368\n",
      "Total Energy (Wh)                      0.027230\n",
      "Energy per Token (J/token)            10.062473\n",
      "Energy per Sentence (J/sentence)      91.429658\n",
      "Energy per Second (W)                 71.713700\n",
      "AST_equal                              0.300000\n",
      "Normalized_equal                       0.300000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a CSV file\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "results.to_csv(f\"llama_cpp_results/{model_name}_sql.csv\", index=False)\n",
    "\n",
    "averages = results.select_dtypes(include='number').mean()\n",
    "\n",
    "print(f\"Averages for {model_name}:\")\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(index):\n",
    "    print(\"AST:\", results['AST_equal'][index])\n",
    "    print(\"Prompt:\", results['prompt'][index], \"\\n\\n\")\n",
    "    print(\"Prediction:\", results['generated_answer'][index])\n",
    "    print(\"Generated query:\", results['reference_answer'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AST: 0\n",
      "Prompt: You are a SQL query generation assistant. Given a natural language question, generate the corresponding SQL query.\n",
      "\n",
      "Here is an example:\n",
      "\n",
      "Question: How many heads of the departments are older than 56?\n",
      "\n",
      "Tables in the database:\n",
      "Table 'department': columns = Department_ID, Name, Creation, Ranking, Budget_in_Billions, Num_Employees\n",
      "Table 'head': columns = head_ID, name, born_state, age\n",
      "Table 'management': columns = department_ID, head_ID, temporary_acting\n",
      "SQL: SELECT count(*) FROM head WHERE age > 56\n",
      "\n",
      "Question: Find distinct cities of addresses of people?\n",
      "\n",
      "Tables in the database:\n",
      "Table 'Addresses': columns = address_id, line_1, line_2, city, zip_postcode, state_province_county, country\n",
      "Table 'People': columns = person_id, first_name, middle_name, last_name, cell_mobile_number, email_address, login_name, password\n",
      "Table 'Students': columns = student_id, student_details\n",
      "Table 'Courses': columns = course_id, course_name, course_description, other_details\n",
      "Table 'People_Addresses': columns = person_address_id, person_id, address_id, date_from, date_to\n",
      "Table 'Student_Course_Registrations': columns = student_id, course_id, registration_date\n",
      "Table 'Student_Course_Attendance': columns = student_id, course_id, date_of_attendance\n",
      "Table 'Candidates': columns = candidate_id, candidate_details\n",
      "Table 'Candidate_Assessments': columns = candidate_id, qualification, assessment_date, asessment_outcome_code\n",
      "\n",
      "SQL: \n",
      "\n",
      "\n",
      "Prediction: SELECT DISTINCT city FROM Addresses\n",
      "Generated query: SELECT DISTINCT T1.city FROM addresses AS T1 JOIN people_addresses AS T2 ON T1.address_id = T2.address_id\n"
     ]
    }
   ],
   "source": [
    "check_answer(91)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔬 Quality vs Speed vs Efficiency Trade-Off Summary\n",
    "\n",
    "<small>\n",
    "\n",
    "| Metric                          | LLaMA f16               | LLaMA Q8_0             | LLaMA Q4_K_M           | GEMMA Q4_K_M           | GEMMA Q8               |\n",
    "|---------------------------------|--------------------------|------------------------|------------------------|------------------------|------------------------|\n",
    "| **Prompt Length**              | 1160.04                 | 1160.04               | 1160.04               | 1160.04               | 1206.04               |\n",
    "| **Avg Token Latency (ATL)**    | 0.485                   | 0.291                 | **0.140**             | 0.287                 | 0.420                 |\n",
    "| **Generation Latency (GL)**    | 4.633                   | 2.810                 | **1.367**             | 2.708                 | 4.020                 |\n",
    "| **Tokens/sec (TPS)**           | 2.673                   | 4.426                 | **8.921**             | 4.337                 | 3.446                 |\n",
    "| **Sentences/sec (SPS)**        | 0.331                   | 0.551                 | **0.961**             | 0.717                 | 0.565                 |\n",
    "| **Memory Usage (MB)**          | 15878.66                | 9124.06               | **5922.06**           | 8960.04               | 14827.74              |\n",
    "| **Model Size (MB)**            | 16068.89                | 8540.77               | **4920.73**           | 7300.58               | 13453.67              |\n",
    "| **Total Energy (Wh)**          | 0.092                   | 0.056                 | **0.027**             | 0.054                 | 0.080                 |\n",
    "| **Energy per Token (J/token)** | 34.822                  | 20.767                | **10.062**            | 20.640                | 30.257                |\n",
    "| **Energy per Sentence (J/s)**  | 298.992                 | 178.564               | **91.430**            | 156.619               | 221.414               |\n",
    "| **Energy per Second (W)**      | 71.862                  | **71.319**            | 71.714                | 71.731                | 71.960                |\n",
    "| **AST_equal**                  | 0.320                   | 0.320                 | 0.300                 | **0.430**             | 0.390                 |\n",
    "| **Normalized_equal**           | 0.320                   | 0.320                 | 0.300                 | **0.430**             | 0.390                 |\n",
    "\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
