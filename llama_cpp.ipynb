{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import llama_cpp\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import requests\n",
    "import json\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter Notebook\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU offload supported: True\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU offload supported:\", llama_cpp.llama_supports_gpu_offload())\n",
    "\n",
    "# Path to your GGUF model (adjust path if needed)\n",
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-gguf/llama-3.1-8B-Q8_0.gguf\"\n",
    "\n",
    "# Initialize Llama with GPU layers offloaded\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,\n",
    "    n_gpu_layers=80,     # Offload to GPU! Adjust as needed based on your VRAM\n",
    "    verbose=False         # Prints backend info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "dataset = load_dataset(\"gigaword\", split=\"test[:100]\")  # Limit to 100 for fast eval\n",
    "\n",
    "# Initialize ROUGE metric\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_with_llama_cpp(document, max_tokens=50):\n",
    "    prompt_template = (\n",
    "        \"You are an AI assistant specialized in summarizing news articles. \"\n",
    "        \"Summarize the following news sentence into a concise headline.\\n\\n\"\n",
    "\n",
    "        \"Here is an example:\\n\"\n",
    "        \"News: Japan 's nec corp. and UNK computer corp. of the united states said wednesday they had agreed to join forces in supercomputer sales.\\n\"\n",
    "        \"Headline: Nec UNK in computer sales tie-up\\n\\n\"\n",
    "\n",
    "        \"Now summarize the following news:\\n\\n\"\n",
    "\n",
    "        \"News: {document}\\n\\n\"\n",
    "        \"Headline:\"\n",
    "    )\n",
    "    \n",
    "    prompt = prompt_template.format(document=document)\n",
    "\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.3,\n",
    "        \"stream\": False,\n",
    "        \"seed\": 0,\n",
    "    }\n",
    "\n",
    "    response = llm(**payload)\n",
    "\n",
    "    summary = response['choices'][0]['text'].strip()\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries and evaluate\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for item in dataset:\n",
    "\n",
    "    doc = item['document']\n",
    "    ref_summary = item['summary']\n",
    "\n",
    "    pred_summary = summarize_with_llama_cpp(doc)\n",
    "\n",
    "    if pred_summary:\n",
    "        references.append(ref_summary)\n",
    "        predictions.append(pred_summary)\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B) float16 Summarization Results:\n",
      "\n",
      "Number of examples: 100\n",
      "\n",
      "Elapsed time: 162.28 s\n",
      "\n",
      "ROUGE Results:\n",
      "rouge1: 0.1792\n",
      "rouge2: 0.0601\n",
      "rougeL: 0.1577\n",
      "rougeLsum: 0.1629\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with ROUGE\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"llama.cpp (Llama-3.1-8B) float16 Summarization Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {len(references)}\")\n",
    "print(f\"\\nElapsed time: {end - start:.2f} s\")\n",
    "\n",
    "print(\"\\nROUGE Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now trying the 8bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-gguf/llama-3.1-8B-Q8_0.gguf\"\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,         # 8K tokens context is fine unless you need more\n",
    "    n_gpu_layers=80,    # L4 GPU 24 GB can usually handle **60-80 layers** for 8B models\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries and evaluate\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for item in dataset:\n",
    "\n",
    "    doc = item['document']\n",
    "    ref_summary = item['summary']\n",
    "\n",
    "    pred_summary = summarize_with_llama_cpp(doc)\n",
    "\n",
    "    if pred_summary:\n",
    "        references.append(ref_summary)\n",
    "        predictions.append(pred_summary)\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B) 8Bit Summarization Results:\n",
      "\n",
      "Number of examples: 100\n",
      "\n",
      "Elapsed time: 161.84 s\n",
      "\n",
      "ROUGE Results:\n",
      "rouge1: 0.1788\n",
      "rouge2: 0.0608\n",
      "rougeL: 0.1580\n",
      "rougeLsum: 0.1631\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with ROUGE\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"llama.cpp (Llama-3.1-8B) 8Bit Summarization Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {len(references)}\")\n",
    "print(f\"\\nElapsed time: {end - start:.2f} s\")\n",
    "\n",
    "print(\"\\nROUGE Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now 4Bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-gguf/llama-3.1-8B-Q4_K_M.gguf\"\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,         # 8K tokens context is fine unless you need more\n",
    "    n_gpu_layers=80,    # L4 GPU 24 GB can usually handle **60-80 layers** for 8B models\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries and evaluate\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for item in dataset:\n",
    "\n",
    "    doc = item['document']\n",
    "    ref_summary = item['summary']\n",
    "\n",
    "    pred_summary = summarize_with_llama_cpp(doc)\n",
    "\n",
    "    if pred_summary:\n",
    "        references.append(ref_summary)\n",
    "        predictions.append(pred_summary)\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B) 4Bit Summarization Results:\n",
      "\n",
      "Number of examples: 98\n",
      "\n",
      "Elapsed time: 64.42 s\n",
      "\n",
      "ROUGE Results:\n",
      "rouge1: 0.2699\n",
      "rouge2: 0.1160\n",
      "rougeL: 0.2491\n",
      "rougeLsum: 0.2501\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with ROUGE\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"llama.cpp (Llama-3.1-8B) 4Bit Summarization Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {len(references)}\")\n",
    "print(f\"\\nElapsed time: {end - start:.2f} s\")\n",
    "\n",
    "print(\"\\nROUGE Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA 3.1 8B Summarization Benchmark\n",
    "\n",
    "<small>\n",
    "\n",
    "#### üìù Tested Models\n",
    "| **Model File**                      | **Precision** | **Quantization Scheme** | **Notes**                                     |\n",
    "|-------------------------------------|---------------|-------------------------|-----------------------------------------------|\n",
    "| llama-3.1-8B-f16.gguf               | float16       | Full Precision          | Largest model, highest theoretical accuracy, slowest inference |\n",
    "| llama-3.1-8B-Q8_0.gguf              | 8-bit         | Q8_0                   | Reduced size, good balance of speed and quality |\n",
    "| llama-3.1-8B-Q4_K_M.gguf            | 4-bit         | Q4_K_M                 | Highly optimized 4-bit quantization, best summarization quality in tests |\n",
    "\n",
    "---\n",
    "\n",
    "#### üìä Summarization Results\n",
    "\n",
    "| **Platform / Model**               | **Elapsed Time (s)** | **ROUGE-1** | **ROUGE-2** | **ROUGE-L** | **ROUGE-Lsum** |\n",
    "|------------------------------------|----------------------|-------------|-------------|-------------|----------------|\n",
    "| Ollama (LLaMA 3.1 8B Q4_K_M)       | 49.06                | 0.2886      | 0.1040      | 0.2632      | 0.2658         |\n",
    "| llama.cpp (Q4_K_M)                 | 64.42                | 0.2699      | 0.1160      | 0.2491      | 0.2501         |\n",
    "| llama.cpp (Q8_0)                   | 161.84               | 0.1788      | 0.0608      | 0.1580      | 0.1631         |\n",
    "| llama.cpp (float16)                | 161.65               | 0.1801      | 0.0599      | 0.1591      | 0.1629         |\n",
    "\n",
    "---\n",
    "\n",
    "#### üîç Summary of Insights\n",
    "\n",
    "- **4-bit quantized models (Q4_K_M)** in both **Ollama** and **llama.cpp** delivered **better summarization quality** and **faster inference** than higher-precision models.\n",
    "- The **Q4_K_M quantization scheme** preserves summarization performance surprisingly well and matches Ollama's results.\n",
    "- **8-bit (Q8_0)** and **float16** models performed worse in ROUGE scores, despite having more precision. This may be due to:\n",
    "  - Differences in **prompt formatting**\n",
    "  - **Sampling parameters**\n",
    "  - Potential model variant differences (instruction-tuned vs base models)\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Recommendations\n",
    "\n",
    "1. Use **Q4_K_M quantized models** in llama.cpp for comparable performance to Ollama.\n",
    "2. Match **prompt templates** used in Ollama:\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Q4_K_M vs Q8_0 Quantization Comparison\n",
    "\n",
    "<small>\n",
    "\n",
    "#### What Are They?\n",
    "Both **Q4_K_M** and **Q8_0** are quantization methods used to compress model weights for faster inference and lower memory usage.\n",
    "\n",
    "---\n",
    "\n",
    "#### Q4_K_M (4-bit Quantization, Optimized)\n",
    "\n",
    "| Feature          | Description |\n",
    "|------------------|-------------|\n",
    "| **Precision**    | 4-bit |\n",
    "| **Quantization Type** | \"K\" series, specifically **K_M** (multi-purpose optimized) |\n",
    "| **Compression**  | Very high (significantly smaller than 8-bit) |\n",
    "| **Speed**        | Extremely fast, ideal for CPU/GPU |\n",
    "| **Memory Usage** | Very low (fits on smaller GPUs like 6-8GB VRAM) |\n",
    "| **Accuracy**     | Preserves high accuracy in **instruction-tuned tasks** like **summarization**, **chat**, and **QA** |\n",
    "| **Best Use Cases** | Chatbots, summarization, reasoning tasks |\n",
    "| **Notes**        | Uses **groupwise quantization** and **per-channel scaling** for better accuracy retention despite low precision |\n",
    "\n",
    "---\n",
    "\n",
    "#### Q8_0 (8-bit Quantization, General Purpose)\n",
    "\n",
    "| Feature          | Description |\n",
    "|------------------|-------------|\n",
    "| **Precision**    | 8-bit |\n",
    "| **Quantization Type** | Uniform 8-bit |\n",
    "| **Compression**  | Moderate (smaller than float16 but larger than 4-bit) |\n",
    "| **Speed**        | Faster than float16, but slower than Q4_K_M |\n",
    "| **Memory Usage** | Moderate (needs more VRAM, typically 12GB+) |\n",
    "| **Accuracy**     | Higher precision retention in general, but not optimized for specific tasks |\n",
    "| **Best Use Cases** | Complex reasoning, precision-sensitive tasks |\n",
    "| **Notes**        | General-purpose quantization without task-specific optimizations |\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öñÔ∏è Comparison Table: Q4_K_M vs Q8_0\n",
    "\n",
    "| Feature          | **Q4_K_M**                  | **Q8_0**                  |\n",
    "|------------------|-----------------------------|---------------------------|\n",
    "| **Precision**    | 4-bit                       | 8-bit                    |\n",
    "| **Size**         | Very small                  | Medium                   |\n",
    "| **Speed**        | Very fast (low latency)     | Fast (higher latency than 4-bit) |\n",
    "| **VRAM/Memory**  | Very low usage (fits on smaller GPUs/CPUs) | Medium (requires more VRAM) |\n",
    "| **Accuracy**     | High for summarization, chat, reasoning (optimized quantization) | General higher precision (not task-optimized) |\n",
    "| **Task Tuning**  | Task-specific optimizations (instruction-following, summarization) | General-purpose |\n",
    "| **Best Use**     | Chatbots, summarization, QA tasks with constrained resources | Complex reasoning or precision-sensitive tasks |\n",
    "| **Ollama Default?** | ‚úÖ Frequently used (Q4_K_M or Q4_K_S) | ‚ùå Usually not used |\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Why Q4_K_M Outperformed Q8_0 in Summarization\n",
    "- **Q4_K_M** is optimized for **task-specific performance**, often giving better results for **instruction-tuned models**, **summarization**, and **chat** tasks.\n",
    "- **Q8_0** retains more raw precision but isn't tuned for these tasks, leading to lower scores in ROUGE evaluation.\n",
    "- **Q4_K_M** also runs significantly faster with less resource usage.\n",
    "\n",
    "---\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† How Q4_K_M Is Optimized\n",
    "\n",
    "<small>\n",
    "\n",
    "Q4_K_M is part of the advanced **K series** quantization schemes, designed to balance **speed**, **size**, and **accuracy**. It introduces several optimizations to maintain high task performance despite being a 4-bit quantization.\n",
    "\n",
    "#### Key Optimizations\n",
    "\n",
    "| Optimization                   | Description |\n",
    "|--------------------------------|-------------|\n",
    "| **Groupwise Quantization**     | Weights are divided into small groups (e.g., 32 or 64) and quantized individually, improving precision retention. |\n",
    "| **Per-Channel Scaling**        | Each group or channel has its own scale factor, ensuring finer control over the quantization process. |\n",
    "| **Mixed Weight Packing (M)**   | Uses different packing strategies optimized for different layers (e.g., attention vs MLP layers). |\n",
    "| **Dynamic Zero Points**        | Zero points are dynamically computed within groups, reducing quantization bias. |\n",
    "| **Efficient SIMD Utilization** | The packed format is optimized for vectorized operations on CPU and GPU, increasing inference speed. |\n",
    "\n",
    "#### Why It Works Well\n",
    "- Optimized for **instruction-following**, **summarization**, and **chat** tasks.\n",
    "- Preserves task-critical accuracy despite aggressive compression.\n",
    "- Runs **very efficiently** on both CPU and GPU.\n",
    "\n",
    "#### Q4_K_M vs Q8_0\n",
    "\n",
    "| Feature            | Q4_K_M                   | Q8_0                 |\n",
    "|--------------------|--------------------------|----------------------|\n",
    "| Precision          | 4-bit                    | 8-bit               |\n",
    "| Compression        | High                     | Medium              |\n",
    "| Accuracy Retention | High (task-optimized)    | High (general)      |\n",
    "| Speed              | Very fast                | Fast                |\n",
    "| Memory Usage       | Very low                 | Medium              |\n",
    "| Task Tuning        | Summarization, Chat, QA  | General-purpose     |\n",
    "| Ollama Use         | ‚úÖ Often used (default)  | ‚ùå Less common       |\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import requests\n",
    "import json\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter Notebook\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '57111b95a58dae1900cd6c53', 'title': 'Huguenot', 'context': 'Frederick William, Elector of Brandenburg, invited Huguenots to settle in his realms, and a number of their descendants rose to positions of prominence in Prussia. Several prominent German military, cultural, and political figures were ethnic Huguenot, including poet Theodor Fontane, General Hermann von Fran√ßois, the hero of the First World War Battle of Tannenberg, Luftwaffe General and fighter ace Adolf Galland, Luftwaffe flying ace Hans-Joachim Marseille, and famed U-boat captain Lothar von Arnauld de la Peri√®re. The last Prime Minister of the (East) German Democratic Republic, Lothar de Maizi√®re, is also a descendant of a Huguenot family, as is the German Federal Minister of the Interior, Thomas de Maizi√®re.', 'question': 'Who was the final Prime Minister of East Germany?', 'answers': {'text': ['Lothar de Maizi√®re', 'Lothar de Maizi√®re', 'Lothar de Maizi√®re'], 'answer_start': [588, 588, 588]}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Load SQuAD v2 dataset (validation split)\n",
    "squad_v2 = load_dataset(\"squad_v2\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Convert the validation split to a list and sample 100 random questions\n",
    "validation_list = list(squad_v2[\"validation\"])\n",
    "sampled_questions = random.sample(validation_list, 100)\n",
    "\n",
    "print(sampled_questions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sampled questions: 100\n",
      "Answerable questions: 48\n",
      "Unanswerable questions: 52\n"
     ]
    }
   ],
   "source": [
    "# Count how many sampled questions are answerable\n",
    "answerable_count = sum(1 for q in sampled_questions if len(q['answers']['text']) > 0)\n",
    "\n",
    "# Optionally, count unanswerable too\n",
    "unanswerable_count = len(sampled_questions) - answerable_count\n",
    "\n",
    "# Print results\n",
    "print(f\"Total sampled questions: {len(sampled_questions)}\")\n",
    "print(f\"Answerable questions: {answerable_count}\")\n",
    "print(f\"Unanswerable questions: {unanswerable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load the SQuAD evaluation metric\n",
    "squad_metric = load(\"squad\")\n",
    "\n",
    "def qa_with_llama_cpp(example, max_tokens=50, verbose=True):\n",
    "\n",
    "    context = example['context']\n",
    "    question = example['question']\n",
    "    ground_truth_answers = example['answers']\n",
    "\n",
    "    prompt_template = (\n",
    "        \"You are a question answering assistant. Given the context, answer the question. \"\n",
    "        \"If the answer isn't in the context, say 'I don't know'.\\n\\n\"\n",
    "\n",
    "        \"Here is an example:\\n\"\n",
    "        \"Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France.\\n\"\n",
    "        \"Question: What is the name of the region the Normans gave their name to?\\n\"\n",
    "        \"Answer: Normandy\\n\\n\"\n",
    "\n",
    "        \"Context: {context}\\n\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    prompt = prompt_template.format(context=context, question=question)\n",
    "\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.0,\n",
    "        \"stream\": False,\n",
    "        \"seed\": 0,\n",
    "    }\n",
    "\n",
    "    # Send to llama.cpp\n",
    "    response = llm(**payload)\n",
    "    prediction = response['choices'][0]['text'].strip()\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"prediction\": prediction,\n",
    "        \"ground_truths\": ground_truth_answers['text'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_qa_with_llama_cpp(dataset, qa_function, skip_unanswerable=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluates a QA function using the SQuAD metric (Exact Match and F1).\n",
    "\n",
    "    Returns:\n",
    "        results: Dictionary with exact_match and f1 score.\n",
    "        num_references: Number of evaluated references.\n",
    "        elapsed_time: Total time taken.\n",
    "    \"\"\"\n",
    "    from evaluate import load\n",
    "    import time\n",
    "\n",
    "    # Load the SQuAD evaluation metric\n",
    "    squad_metric = load(\"squad\")\n",
    "\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for example in dataset:\n",
    "        if skip_unanswerable and len(example['answers']['text']) == 0:\n",
    "            continue\n",
    "\n",
    "        result = qa_function(example)\n",
    "\n",
    "        if result['prediction'] is not None:\n",
    "            predictions.append({\n",
    "                \"id\": example['id'],\n",
    "                \"prediction_text\": result['prediction']\n",
    "            })\n",
    "\n",
    "            references.append({\n",
    "                \"id\": example['id'],\n",
    "                \"answers\": {\n",
    "                    \"text\": result['ground_truths']\n",
    "                }\n",
    "            })\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute EM and F1 scores\n",
    "    results = squad_metric.compute(predictions=predictions, references=references)\n",
    "    elapsed_time = end - start\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nllama.cpp QA Evaluation Results\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Number of evaluated examples: {len(references)}\")\n",
    "        print(f\"Elapsed time: {elapsed_time:.2f} seconds\\n\")\n",
    "        for key, value in results.items():\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "    return results, len(references), elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA L4, compute capability 8.9, VMM: yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU offload supported: True\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import llama_cpp\n",
    "\n",
    "print(\"GPU offload supported:\", llama_cpp.llama_supports_gpu_offload())\n",
    "\n",
    "# Path to your GGUF model (adjust path if needed)\n",
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-gguf/llama-3.1-8B-f16.gguf\"\n",
    "\n",
    "# Initialize Llama with GPU layers offloaded\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,\n",
    "    n_gpu_layers=80,     # Offload to GPU! Adjust as needed based on your VRAM\n",
    "    verbose=False         # Prints backend info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B) float16 QA Results:\n",
      "\n",
      "Number of examples: 48\n",
      "Elapsed time: 150.84 seconds\n",
      "\n",
      "QA Evaluation Results:\n",
      "exact_match: 0.0000\n",
      "f1: 20.5061\n"
     ]
    }
   ],
   "source": [
    "results, num_examples, elapsed_time = evaluate_qa_with_llama_cpp(\n",
    "    sampled_questions,\n",
    "    qa_with_llama_cpp,\n",
    "    skip_unanswerable=True,\n",
    "    verbose=False   # We print it manually below\n",
    ")\n",
    "\n",
    "# Print final report\n",
    "print(\"llama.cpp (Llama-3.1-8B) float16 QA Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {num_examples}\")\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nQA Evaluation Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA L4, compute capability 8.9, VMM: yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU offload supported: True\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import llama_cpp\n",
    "\n",
    "print(\"GPU offload supported:\", llama_cpp.llama_supports_gpu_offload())\n",
    "\n",
    "# Path to your GGUF model (adjust path if needed)\n",
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-gguf/llama-3.1-8B-Q8_0.gguf\"\n",
    "\n",
    "# Initialize Llama with GPU layers offloaded\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,\n",
    "    n_gpu_layers=80,     # Offload to GPU! Adjust as needed based on your VRAM\n",
    "    verbose=False         # Prints backend info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B) 8Bit QA Results:\n",
      "\n",
      "Number of examples: 48\n",
      "Elapsed time: 89.03 seconds\n",
      "\n",
      "QA Evaluation Results:\n",
      "exact_match: 0.0000\n",
      "f1: 20.4193\n"
     ]
    }
   ],
   "source": [
    "results, num_examples, elapsed_time = evaluate_qa_with_llama_cpp(\n",
    "    sampled_questions,\n",
    "    qa_with_llama_cpp,\n",
    "    skip_unanswerable=True,\n",
    "    verbose=False   # We print it manually below\n",
    ")\n",
    "\n",
    "# Print final report\n",
    "print(\"llama.cpp (Llama-3.1-8B) 8Bit QA Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {num_examples}\")\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nQA Evaluation Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA L4, compute capability 8.9, VMM: yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU offload supported: True\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import llama_cpp\n",
    "\n",
    "print(\"GPU offload supported:\", llama_cpp.llama_supports_gpu_offload())\n",
    "\n",
    "# Path to your GGUF model (adjust path if needed)\n",
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-gguf/llama-3.1-8B-Q4_K_M.gguf\"\n",
    "\n",
    "# Initialize Llama with GPU layers offloaded\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,\n",
    "    n_gpu_layers=80,     # Offload to GPU! Adjust as needed based on your VRAM\n",
    "    verbose=False         # Prints backend info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B) 4Bit QA Results:\n",
      "\n",
      "Number of examples: 48\n",
      "Elapsed time: 58.31 seconds\n",
      "\n",
      "QA Evaluation Results:\n",
      "exact_match: 0.0000\n",
      "f1: 19.4507\n"
     ]
    }
   ],
   "source": [
    "results, num_examples, elapsed_time = evaluate_qa_with_llama_cpp(\n",
    "    sampled_questions,\n",
    "    qa_with_llama_cpp,\n",
    "    skip_unanswerable=True,\n",
    "    verbose=False   # We print it manually below\n",
    ")\n",
    "\n",
    "# Print final report\n",
    "print(\"llama.cpp (Llama-3.1-8B) 4Bit QA Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {num_examples}\")\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nQA Evaluation Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
