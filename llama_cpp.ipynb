{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import llama_cpp\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import requests\n",
    "import json\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter Notebook\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU offload supported: True\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU offload supported:\", llama_cpp.llama_supports_gpu_offload())\n",
    "\n",
    "# Path to your GGUF model (adjust path if needed)\n",
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-gguf/llama-3.1-8B-Q8_0.gguf\"\n",
    "\n",
    "# Initialize Llama with GPU layers offloaded\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,\n",
    "    n_gpu_layers=80,     # Offload to GPU! Adjust as needed based on your VRAM\n",
    "    verbose=False         # Prints backend info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "dataset = load_dataset(\"gigaword\", split=\"test[:100]\")  # Limit to 100 for fast eval\n",
    "\n",
    "# Initialize ROUGE metric\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_with_llama_cpp(document, max_tokens=50):\n",
    "    prompt_template = (\n",
    "        \"You are an AI assistant specialized in summarizing news articles. \"\n",
    "        \"Summarize the following news sentence into a concise headline.\\n\\n\"\n",
    "\n",
    "        \"Here is an example:\\n\"\n",
    "        \"News: Japan 's nec corp. and UNK computer corp. of the united states said wednesday they had agreed to join forces in supercomputer sales.\\n\"\n",
    "        \"Headline: Nec UNK in computer sales tie-up\\n\\n\"\n",
    "\n",
    "        \"Now summarize the following news:\\n\\n\"\n",
    "\n",
    "        \"News: {document}\\n\\n\"\n",
    "        \"Headline:\"\n",
    "    )\n",
    "    \n",
    "    prompt = prompt_template.format(document=document)\n",
    "\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.3,\n",
    "        \"stream\": False,\n",
    "        \"seed\": 0,\n",
    "    }\n",
    "\n",
    "    response = llm(**payload)\n",
    "\n",
    "    summary = response['choices'][0]['text'].strip()\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries and evaluate\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for item in dataset:\n",
    "\n",
    "    doc = item['document']\n",
    "    ref_summary = item['summary']\n",
    "\n",
    "    pred_summary = summarize_with_llama_cpp(doc)\n",
    "\n",
    "    if pred_summary:\n",
    "        references.append(ref_summary)\n",
    "        predictions.append(pred_summary)\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B) float16 Summarization Results:\n",
      "\n",
      "Number of examples: 100\n",
      "\n",
      "Elapsed time: 162.28 s\n",
      "\n",
      "ROUGE Results:\n",
      "rouge1: 0.1792\n",
      "rouge2: 0.0601\n",
      "rougeL: 0.1577\n",
      "rougeLsum: 0.1629\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with ROUGE\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"llama.cpp (Llama-3.1-8B) float16 Summarization Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {len(references)}\")\n",
    "print(f\"\\nElapsed time: {end - start:.2f} s\")\n",
    "\n",
    "print(\"\\nROUGE Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now trying the 8bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-gguf/llama-3.1-8B-Q8_0.gguf\"\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,         # 8K tokens context is fine unless you need more\n",
    "    n_gpu_layers=80,    # L4 GPU 24 GB can usually handle **60-80 layers** for 8B models\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries and evaluate\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for item in dataset:\n",
    "\n",
    "    doc = item['document']\n",
    "    ref_summary = item['summary']\n",
    "\n",
    "    pred_summary = summarize_with_llama_cpp(doc)\n",
    "\n",
    "    if pred_summary:\n",
    "        references.append(ref_summary)\n",
    "        predictions.append(pred_summary)\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B) 8Bit Summarization Results:\n",
      "\n",
      "Number of examples: 100\n",
      "\n",
      "Elapsed time: 161.84 s\n",
      "\n",
      "ROUGE Results:\n",
      "rouge1: 0.1788\n",
      "rouge2: 0.0608\n",
      "rougeL: 0.1580\n",
      "rougeLsum: 0.1631\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with ROUGE\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"llama.cpp (Llama-3.1-8B) 8Bit Summarization Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {len(references)}\")\n",
    "print(f\"\\nElapsed time: {end - start:.2f} s\")\n",
    "\n",
    "print(\"\\nROUGE Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now 4Bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-gguf/llama-3.1-8B-Q4_K_M.gguf\"\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,         # 8K tokens context is fine unless you need more\n",
    "    n_gpu_layers=80,    # L4 GPU 24 GB can usually handle **60-80 layers** for 8B models\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries and evaluate\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for item in dataset:\n",
    "\n",
    "    doc = item['document']\n",
    "    ref_summary = item['summary']\n",
    "\n",
    "    pred_summary = summarize_with_llama_cpp(doc)\n",
    "\n",
    "    if pred_summary:\n",
    "        references.append(ref_summary)\n",
    "        predictions.append(pred_summary)\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B) 4Bit Summarization Results:\n",
      "\n",
      "Number of examples: 98\n",
      "\n",
      "Elapsed time: 64.42 s\n",
      "\n",
      "ROUGE Results:\n",
      "rouge1: 0.2699\n",
      "rouge2: 0.1160\n",
      "rougeL: 0.2491\n",
      "rougeLsum: 0.2501\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with ROUGE\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"llama.cpp (Llama-3.1-8B) 4Bit Summarization Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {len(references)}\")\n",
    "print(f\"\\nElapsed time: {end - start:.2f} s\")\n",
    "\n",
    "print(\"\\nROUGE Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA 3.1 8B Summarization Benchmark\n",
    "\n",
    "<small>\n",
    "\n",
    "#### üìù Tested Models\n",
    "| **Model File**                      | **Precision** | **Quantization Scheme** | **Notes**                                     |\n",
    "|-------------------------------------|---------------|-------------------------|-----------------------------------------------|\n",
    "| llama-3.1-8B-f16.gguf               | float16       | Full Precision          | Largest model, highest theoretical accuracy, slowest inference |\n",
    "| llama-3.1-8B-Q8_0.gguf              | 8-bit         | Q8_0                   | Reduced size, good balance of speed and quality |\n",
    "| llama-3.1-8B-Q4_K_M.gguf            | 4-bit         | Q4_K_M                 | Highly optimized 4-bit quantization, best summarization quality in tests |\n",
    "\n",
    "---\n",
    "\n",
    "#### üìä Summarization Results\n",
    "\n",
    "| **Platform / Model**               | **Elapsed Time (s)** | **ROUGE-1** | **ROUGE-2** | **ROUGE-L** | **ROUGE-Lsum** |\n",
    "|------------------------------------|----------------------|-------------|-------------|-------------|----------------|\n",
    "| Ollama (LLaMA 3.1 8B Q4_K_M)       | 49.06                | 0.2886      | 0.1040      | 0.2632      | 0.2658         |\n",
    "| llama.cpp (Q4_K_M)                 | 64.42                | 0.2699      | 0.1160      | 0.2491      | 0.2501         |\n",
    "| llama.cpp (Q8_0)                   | 161.84               | 0.1788      | 0.0608      | 0.1580      | 0.1631         |\n",
    "| llama.cpp (float16)                | 161.65               | 0.1801      | 0.0599      | 0.1591      | 0.1629         |\n",
    "\n",
    "---\n",
    "\n",
    "#### üîç Summary of Insights\n",
    "\n",
    "- **4-bit quantized models (Q4_K_M)** in both **Ollama** and **llama.cpp** delivered **better summarization quality** and **faster inference** than higher-precision models.\n",
    "- The **Q4_K_M quantization scheme** preserves summarization performance surprisingly well and matches Ollama's results.\n",
    "- **8-bit (Q8_0)** and **float16** models performed worse in ROUGE scores, despite having more precision. This may be due to:\n",
    "  - Differences in **prompt formatting**\n",
    "  - **Sampling parameters**\n",
    "  - Potential model variant differences (instruction-tuned vs base models)\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Recommendations\n",
    "\n",
    "1. Use **Q4_K_M quantized models** in llama.cpp for comparable performance to Ollama.\n",
    "2. Match **prompt templates** used in Ollama:\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Q4_K_M vs Q8_0 Quantization Comparison\n",
    "\n",
    "<small>\n",
    "\n",
    "#### What Are They?\n",
    "Both **Q4_K_M** and **Q8_0** are quantization methods used to compress model weights for faster inference and lower memory usage.\n",
    "\n",
    "---\n",
    "\n",
    "#### Q4_K_M (4-bit Quantization, Optimized)\n",
    "\n",
    "| Feature          | Description |\n",
    "|------------------|-------------|\n",
    "| **Precision**    | 4-bit |\n",
    "| **Quantization Type** | \"K\" series, specifically **K_M** (multi-purpose optimized) |\n",
    "| **Compression**  | Very high (significantly smaller than 8-bit) |\n",
    "| **Speed**        | Extremely fast, ideal for CPU/GPU |\n",
    "| **Memory Usage** | Very low (fits on smaller GPUs like 6-8GB VRAM) |\n",
    "| **Accuracy**     | Preserves high accuracy in **instruction-tuned tasks** like **summarization**, **chat**, and **QA** |\n",
    "| **Best Use Cases** | Chatbots, summarization, reasoning tasks |\n",
    "| **Notes**        | Uses **groupwise quantization** and **per-channel scaling** for better accuracy retention despite low precision |\n",
    "\n",
    "---\n",
    "\n",
    "#### Q8_0 (8-bit Quantization, General Purpose)\n",
    "\n",
    "| Feature          | Description |\n",
    "|------------------|-------------|\n",
    "| **Precision**    | 8-bit |\n",
    "| **Quantization Type** | Uniform 8-bit |\n",
    "| **Compression**  | Moderate (smaller than float16 but larger than 4-bit) |\n",
    "| **Speed**        | Faster than float16, but slower than Q4_K_M |\n",
    "| **Memory Usage** | Moderate (needs more VRAM, typically 12GB+) |\n",
    "| **Accuracy**     | Higher precision retention in general, but not optimized for specific tasks |\n",
    "| **Best Use Cases** | Complex reasoning, precision-sensitive tasks |\n",
    "| **Notes**        | General-purpose quantization without task-specific optimizations |\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öñÔ∏è Comparison Table: Q4_K_M vs Q8_0\n",
    "\n",
    "| Feature          | **Q4_K_M**                  | **Q8_0**                  |\n",
    "|------------------|-----------------------------|---------------------------|\n",
    "| **Precision**    | 4-bit                       | 8-bit                    |\n",
    "| **Size**         | Very small                  | Medium                   |\n",
    "| **Speed**        | Very fast (low latency)     | Fast (higher latency than 4-bit) |\n",
    "| **VRAM/Memory**  | Very low usage (fits on smaller GPUs/CPUs) | Medium (requires more VRAM) |\n",
    "| **Accuracy**     | High for summarization, chat, reasoning (optimized quantization) | General higher precision (not task-optimized) |\n",
    "| **Task Tuning**  | Task-specific optimizations (instruction-following, summarization) | General-purpose |\n",
    "| **Best Use**     | Chatbots, summarization, QA tasks with constrained resources | Complex reasoning or precision-sensitive tasks |\n",
    "| **Ollama Default?** | ‚úÖ Frequently used (Q4_K_M or Q4_K_S) | ‚ùå Usually not used |\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Why Q4_K_M Outperformed Q8_0 in Summarization\n",
    "- **Q4_K_M** is optimized for **task-specific performance**, often giving better results for **instruction-tuned models**, **summarization**, and **chat** tasks.\n",
    "- **Q8_0** retains more raw precision but isn't tuned for these tasks, leading to lower scores in ROUGE evaluation.\n",
    "- **Q4_K_M** also runs significantly faster with less resource usage.\n",
    "\n",
    "---\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† How Q4_K_M Is Optimized\n",
    "\n",
    "<small>\n",
    "\n",
    "Q4_K_M is part of the advanced **K series** quantization schemes, designed to balance **speed**, **size**, and **accuracy**. It introduces several optimizations to maintain high task performance despite being a 4-bit quantization.\n",
    "\n",
    "#### Key Optimizations\n",
    "\n",
    "| Optimization                   | Description |\n",
    "|--------------------------------|-------------|\n",
    "| **Groupwise Quantization**     | Weights are divided into small groups (e.g., 32 or 64) and quantized individually, improving precision retention. |\n",
    "| **Per-Channel Scaling**        | Each group or channel has its own scale factor, ensuring finer control over the quantization process. |\n",
    "| **Mixed Weight Packing (M)**   | Uses different packing strategies optimized for different layers (e.g., attention vs MLP layers). |\n",
    "| **Dynamic Zero Points**        | Zero points are dynamically computed within groups, reducing quantization bias. |\n",
    "| **Efficient SIMD Utilization** | The packed format is optimized for vectorized operations on CPU and GPU, increasing inference speed. |\n",
    "\n",
    "#### Why It Works Well\n",
    "- Optimized for **instruction-following**, **summarization**, and **chat** tasks.\n",
    "- Preserves task-critical accuracy despite aggressive compression.\n",
    "- Runs **very efficiently** on both CPU and GPU.\n",
    "\n",
    "#### Q4_K_M vs Q8_0\n",
    "\n",
    "| Feature            | Q4_K_M                   | Q8_0                 |\n",
    "|--------------------|--------------------------|----------------------|\n",
    "| Precision          | 4-bit                    | 8-bit               |\n",
    "| Compression        | High                     | Medium              |\n",
    "| Accuracy Retention | High (task-optimized)    | High (general)      |\n",
    "| Speed              | Very fast                | Fast                |\n",
    "| Memory Usage       | Very low                 | Medium              |\n",
    "| Task Tuning        | Summarization, Chat, QA  | General-purpose     |\n",
    "| Ollama Use         | ‚úÖ Often used (default)  | ‚ùå Less common       |\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '57111b95a58dae1900cd6c53', 'title': 'Huguenot', 'context': 'Frederick William, Elector of Brandenburg, invited Huguenots to settle in his realms, and a number of their descendants rose to positions of prominence in Prussia. Several prominent German military, cultural, and political figures were ethnic Huguenot, including poet Theodor Fontane, General Hermann von Fran√ßois, the hero of the First World War Battle of Tannenberg, Luftwaffe General and fighter ace Adolf Galland, Luftwaffe flying ace Hans-Joachim Marseille, and famed U-boat captain Lothar von Arnauld de la Peri√®re. The last Prime Minister of the (East) German Democratic Republic, Lothar de Maizi√®re, is also a descendant of a Huguenot family, as is the German Federal Minister of the Interior, Thomas de Maizi√®re.', 'question': 'Who was the final Prime Minister of East Germany?', 'answers': {'text': ['Lothar de Maizi√®re', 'Lothar de Maizi√®re', 'Lothar de Maizi√®re'], 'answer_start': [588, 588, 588]}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Load SQuAD v2 dataset (validation split)\n",
    "squad_v2 = load_dataset(\"squad_v2\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Convert the validation split to a list and sample 200 random questions\n",
    "validation_list = list(squad_v2[\"validation\"])\n",
    "sampled_questions = random.sample(validation_list, 200)\n",
    "\n",
    "print(sampled_questions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sampled questions: 200\n",
      "Answerable questions: 101\n",
      "Unanswerable questions: 99\n"
     ]
    }
   ],
   "source": [
    "# Count how many sampled questions are answerable\n",
    "answerable_count = sum(1 for q in sampled_questions if len(q['answers']['text']) > 0)\n",
    "\n",
    "# Optionally, count unanswerable too\n",
    "unanswerable_count = len(sampled_questions) - answerable_count\n",
    "\n",
    "# Print results\n",
    "print(f\"Total sampled questions: {len(sampled_questions)}\")\n",
    "print(f\"Answerable questions: {answerable_count}\")\n",
    "print(f\"Unanswerable questions: {unanswerable_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from evaluate import load\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import time\n",
    "from evaluate import load\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles, and extra whitespace.\"\"\"\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punctuation(lower(s))))\n",
    "\n",
    "def clean_prediction(prediction):\n",
    "    \"\"\"\n",
    "    Cleans the raw prediction output from llama.cpp.\n",
    "    - Truncates at a new line, 'Context:', or other stop signals.\n",
    "    - Normalizes the prediction.\n",
    "    \"\"\"\n",
    "    # Split on common stop sequences\n",
    "    stop_tokens = [\"\\n\\n\", \"\\nContext:\", \"Context:\", \"Question:\"]\n",
    "    for stop in stop_tokens:\n",
    "        if stop in prediction:\n",
    "            prediction = prediction.split(stop)[0]\n",
    "\n",
    "    return normalize_answer(prediction)\n",
    "\n",
    "\n",
    "def qa_with_llama_cpp(example, max_tokens=50, verbose=True):\n",
    "    context = example['context']\n",
    "    question = example['question']\n",
    "    ground_truth_answers = example['answers']\n",
    "\n",
    "    prompt_template = (\n",
    "        \"You are a question answering assistant. Given the context, answer the question. \"\n",
    "        \"If the answer isn't in the context, say 'I don't know'.\\n\\n\"\n",
    "\n",
    "        \"Here is an example:\\n\"\n",
    "        \"Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni)...\\n\"\n",
    "        \"Question: What is the name of the region the Normans gave their name to?\\n\"\n",
    "        \"Answer: Normandy\\n\\n\"\n",
    "\n",
    "        \"Context: {context}\\n\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    prompt = prompt_template.format(context=context, question=question)\n",
    "\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.0,\n",
    "        \"stream\": False,\n",
    "        \"seed\": 0,\n",
    "    }\n",
    "\n",
    "    response = llm(**payload)\n",
    "    raw_prediction = response['choices'][0]['text'].strip()\n",
    "\n",
    "    # Clean and normalize the prediction\n",
    "    prediction = clean_prediction(raw_prediction)\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"prediction\": prediction,\n",
    "        \"ground_truths\": ground_truth_answers['text'],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def compute_exact_match(prediction, ground_truths):\n",
    "    \"\"\"Exact match: 1 if prediction is in ground_truths, else 0.\"\"\"\n",
    "    return int(prediction in ground_truths)\n",
    "\n",
    "def compute_f1(prediction, ground_truths):\n",
    "    \"\"\"Compute the maximum F1 over all ground truths.\"\"\"\n",
    "    def get_tokens(s):\n",
    "        return normalize_answer(s).split()\n",
    "\n",
    "    pred_tokens = get_tokens(prediction)\n",
    "    if not pred_tokens:\n",
    "        return int(not any(get_tokens(gt) for gt in ground_truths))\n",
    "\n",
    "    scores = []\n",
    "    for gt in ground_truths:\n",
    "        gt_tokens = get_tokens(gt)\n",
    "        common = set(pred_tokens) & set(gt_tokens)\n",
    "        num_same = len(common)\n",
    "\n",
    "        if num_same == 0:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "\n",
    "        precision = num_same / len(pred_tokens)\n",
    "        recall = num_same / len(gt_tokens)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        scores.append(f1)\n",
    "\n",
    "    return max(scores)\n",
    "\n",
    "def evaluate_qa_with_llama_cpp(dataset, qa_function, save_path=None, skip_unanswerable=True):\n",
    "    squad_metric = load(\"squad\")\n",
    "    references = []\n",
    "    predictions = []\n",
    "    per_example_results = []\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for example in dataset:\n",
    "        if skip_unanswerable and len(example['answers']['text']) == 0:\n",
    "            continue\n",
    "\n",
    "        result = qa_function(example)\n",
    "\n",
    "        if result['prediction'] is not None:\n",
    "            normalized_prediction = normalize_answer(result['prediction'])\n",
    "            normalized_ground_truths = [normalize_answer(ans) for ans in result['ground_truths']]\n",
    "\n",
    "            em = compute_exact_match(normalized_prediction, normalized_ground_truths)\n",
    "            f1 = compute_f1(normalized_prediction, normalized_ground_truths)\n",
    "\n",
    "            predictions.append({\n",
    "                \"id\": example['id'],\n",
    "                \"prediction_text\": normalized_prediction\n",
    "            })\n",
    "\n",
    "            references.append({\n",
    "                \"id\": example['id'],\n",
    "                \"answers\": {\n",
    "                    \"text\": result['ground_truths'],\n",
    "                    \"answer_start\": example['answers']['answer_start']\n",
    "                }\n",
    "            })\n",
    "\n",
    "            per_example_results.append({\n",
    "                'id': example['id'],\n",
    "                'prediction_text': normalized_prediction,\n",
    "                'ground_truth_text': \"; \".join(normalized_ground_truths),\n",
    "                'answer_start': \"; \".join(map(str, example['answers']['answer_start'])),\n",
    "                'exact_match': em,\n",
    "                'f1_score': round(f1, 4)\n",
    "            })\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute overall metrics\n",
    "    results = squad_metric.compute(predictions=predictions, references=references)\n",
    "    elapsed_time = end - start\n",
    "\n",
    "    # Save per-example results to CSV\n",
    "    if save_path:\n",
    "        with open(save_path, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['id', 'prediction_text', 'ground_truth_text', 'answer_start', 'exact_match', 'f1_score']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in per_example_results:\n",
    "                writer.writerow(row)\n",
    "\n",
    "    return results, len(references), elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU offload supported: True\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import llama_cpp\n",
    "\n",
    "print(\"GPU offload supported:\", llama_cpp.llama_supports_gpu_offload())\n",
    "\n",
    "# Path to your GGUF model (adjust path if needed)\n",
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct-gguf/llama-3.1-8B-Instruct-f16.gguf\"\n",
    "\n",
    "# Initialize Llama with GPU layers offloaded\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,\n",
    "    n_gpu_layers=-1,     # Offload to GPU! Adjust as needed based on your VRAM\n",
    "    verbose=False,         # Prints backend info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B-Instruct) float16 QA Results:\n",
      "\n",
      "Number of examples: 101\n",
      "Elapsed time: 300.17 seconds\n",
      "\n",
      "QA Evaluation Results:\n",
      "exact_match: 68.3168\n",
      "f1: 84.6438\n"
     ]
    }
   ],
   "source": [
    "results, num_examples, elapsed_time = evaluate_qa_with_llama_cpp(\n",
    "    sampled_questions,\n",
    "    qa_with_llama_cpp,\n",
    "    skip_unanswerable=True,\n",
    "    save_path=\"evaluation_with_predictions_and_references_f16.csv\"\n",
    ")\n",
    "\n",
    "# Print final report\n",
    "print(\"llama.cpp (Llama-3.1-8B-Instruct) float16 QA Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {num_examples}\")\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nQA Evaluation Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "llm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA L4, compute capability 8.9, VMM: yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU offload supported: True\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import llama_cpp\n",
    "\n",
    "print(\"GPU offload supported:\", llama_cpp.llama_supports_gpu_offload())\n",
    "\n",
    "# Path to your GGUF model (adjust path if needed)\n",
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct-gguf/llama-3.1-8B-Instruct-8bit.gguf\"\n",
    "\n",
    "# Initialize Llama with GPU layers offloaded\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,\n",
    "    n_gpu_layers=-1,     # Offload to GPU! Adjust as needed based on your VRAM\n",
    "    verbose=False         # Prints backend info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B-Instruct) 8Bit QA Results:\n",
      "\n",
      "Number of examples: 101\n",
      "Elapsed time: 178.62 seconds\n",
      "\n",
      "QA Evaluation Results:\n",
      "exact_match: 70.2970\n",
      "f1: 85.1146\n"
     ]
    }
   ],
   "source": [
    "results, num_examples, elapsed_time = evaluate_qa_with_llama_cpp(\n",
    "    sampled_questions,\n",
    "    qa_with_llama_cpp,\n",
    "    skip_unanswerable=True,\n",
    "    save_path=\"evaluation_with_predictions_and_references_8bit.csv\"\n",
    ")\n",
    "\n",
    "# Print final report\n",
    "print(\"llama.cpp (Llama-3.1-8B-Instruct) 8Bit QA Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {num_examples}\")\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nQA Evaluation Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "llm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU offload supported: True\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import llama_cpp\n",
    "\n",
    "print(\"GPU offload supported:\", llama_cpp.llama_supports_gpu_offload())\n",
    "\n",
    "# Path to your GGUF model (adjust path if needed)\n",
    "model_path = \"/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct-gguf/llama-3.1-8B-Instruct-Q4_K_M.gguf\"\n",
    "\n",
    "# Initialize Llama with GPU layers offloaded\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=8192,\n",
    "    n_gpu_layers=-1,     # Offload to GPU! Adjust as needed based on your VRAM\n",
    "    verbose=False         # Prints backend info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama.cpp (Llama-3.1-8B-Instruct) 4Bit QA Results:\n",
      "\n",
      "Number of examples: 101\n",
      "Elapsed time: 111.71 seconds\n",
      "\n",
      "QA Evaluation Results:\n",
      "exact_match: 67.3267\n",
      "f1: 84.0383\n"
     ]
    }
   ],
   "source": [
    "results, num_examples, elapsed_time = evaluate_qa_with_llama_cpp(\n",
    "    sampled_questions,\n",
    "    qa_with_llama_cpp,\n",
    "    skip_unanswerable=True,\n",
    "    save_path=\"evaluation_with_predictions_and_references_4bit.csv\"\n",
    ")\n",
    "\n",
    "# Print final report\n",
    "print(\"llama.cpp (Llama-3.1-8B-Instruct) 4Bit QA Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {num_examples}\")\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nQA Evaluation Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "llm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction_text</th>\n",
       "      <th>ground_truth_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57111b95a58dae1900cd6c53</td>\n",
       "      <td>lothar de maizi√®re</td>\n",
       "      <td>lothar de maizi√®re; lothar de maizi√®re; lothar...</td>\n",
       "      <td>588; 588; 588</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56e1c0f6cd28a01900c67b2c</td>\n",
       "      <td>complexity classes</td>\n",
       "      <td>complexity classes; complexity classes; some c...</td>\n",
       "      <td>16; 16; 11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57264228ec44d21400f3dcf9</td>\n",
       "      <td>gte</td>\n",
       "      <td>telenet was incorporated in 1973 and started o...</td>\n",
       "      <td>560; 669; 669</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57263eaa38643c19005ad373</td>\n",
       "      <td>water flow through body cavity</td>\n",
       "      <td>water flow through body cavity; Œ∫œÑŒµŒØœÇ kteis co...</td>\n",
       "      <td>801; 90; 801</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5710eca0a58dae1900cd6b3e</td>\n",
       "      <td>1705</td>\n",
       "      <td>12 may 1705; 1705; 12 may 1705</td>\n",
       "      <td>420; 427; 420</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5733a6ac4776f41900660f5b</td>\n",
       "      <td>poet</td>\n",
       "      <td>poet; poet; poet</td>\n",
       "      <td>273; 273; 273</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>572fc6f204bcaa1900d76cf7</td>\n",
       "      <td>fact that there is no revising chamber</td>\n",
       "      <td>no revising chamber; no revising chamber; take...</td>\n",
       "      <td>313; 313; 400</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>56e1d9fee3433e14004231cb</td>\n",
       "      <td>npcomplete</td>\n",
       "      <td>npcomplete; npcomplete; npcomplete</td>\n",
       "      <td>244; 244; 244</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>57108d69b654c5140001f985</td>\n",
       "      <td>2 million</td>\n",
       "      <td>ca 2 million; 2 million; 2 million</td>\n",
       "      <td>367; 371; 371</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>572a020f6aef051400155199</td>\n",
       "      <td>destruction of forest</td>\n",
       "      <td>destruction of forest; destruction of forest; ...</td>\n",
       "      <td>81; 81; 81</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>572ff626947a6a140053ce91</td>\n",
       "      <td>two poles</td>\n",
       "      <td>poles; two poles; poles</td>\n",
       "      <td>403; 399; 403</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>56e1c4fce3433e140042314f</td>\n",
       "      <td>boolean circuits</td>\n",
       "      <td>boolean; boolean; boolean circuits</td>\n",
       "      <td>150; 150; 150</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>572f6c85947a6a140053c942</td>\n",
       "      <td>duisburg</td>\n",
       "      <td>duisburg; duisburg; duisburg</td>\n",
       "      <td>298; 298; 298</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5726a7ecf1498d1400e8e656</td>\n",
       "      <td>articles 106 and 107</td>\n",
       "      <td>articles 106 and 107; articles 106 and 107; ar...</td>\n",
       "      <td>949; 949; 949</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>57378b141c456719005744a1</td>\n",
       "      <td>force</td>\n",
       "      <td>gravitational force; gravitational force; grav...</td>\n",
       "      <td>1117; 1117; 1117; 1117</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>57340d124776f419006617c3</td>\n",
       "      <td>cuba</td>\n",
       "      <td>most went to cuba; most went to cuba; cuba; cu...</td>\n",
       "      <td>831; 831; 844; 844; 844</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>572998673f37b319004784d6</td>\n",
       "      <td>underground</td>\n",
       "      <td>as grubs underground; underground; underground...</td>\n",
       "      <td>133; 142; 142; 142</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>572a07c11d046914007796d6</td>\n",
       "      <td>trio tribe</td>\n",
       "      <td>trio tribe; trio; trio</td>\n",
       "      <td>249; 249; 249</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>572fc5a1947a6a140053cc8a</td>\n",
       "      <td>outcome of most votes</td>\n",
       "      <td>votes; outcome of most votes; outcome</td>\n",
       "      <td>20; 4; 4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>57264586f1498d1400e8dac7</td>\n",
       "      <td>datanet 1 was public switched data network ope...</td>\n",
       "      <td>datanet 1 only referred to network and connect...</td>\n",
       "      <td>119; 146; 146</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5729f24baf94a219006aa6e1</td>\n",
       "      <td>lower levels of inequality</td>\n",
       "      <td>lower levels of inequality; lower levels of in...</td>\n",
       "      <td>514; 514; 514</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5728dddc2ca10214002da9d6</td>\n",
       "      <td>exodus</td>\n",
       "      <td>book of exodus; exodus; exodus; exodus; exodus</td>\n",
       "      <td>983; 991; 991; 991; 991</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5726722bdd62a815002e8528</td>\n",
       "      <td>62</td>\n",
       "      <td>62 acres; quarter square; quarter square; 62</td>\n",
       "      <td>114; 88; 88; 114</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5728d7c54b864d1900164f4e</td>\n",
       "      <td>definition of civil disobedience</td>\n",
       "      <td>semantical problems and grammatical niceties; ...</td>\n",
       "      <td>268; 260; 268; 268</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>571093aba58dae1900cd6a5e</td>\n",
       "      <td>it gave them religious and political freedom w...</td>\n",
       "      <td>granted protestants equality with catholics; g...</td>\n",
       "      <td>390; 390; 390</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>572ff932a23a5019007fcbd7</td>\n",
       "      <td>islamists</td>\n",
       "      <td>its supporters; scholars and observers; islamism</td>\n",
       "      <td>107; 238; 0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>57263677ec44d21400f3dc4c</td>\n",
       "      <td>store and forward switching</td>\n",
       "      <td>by store and forward switching; packets; store...</td>\n",
       "      <td>665; 625; 668</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5705eee952bb8914006896df</td>\n",
       "      <td>southern california</td>\n",
       "      <td>alta california; alta california; alta california</td>\n",
       "      <td>281; 281; 281</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>572870b2ff5b5019007da224</td>\n",
       "      <td>paul samuelson</td>\n",
       "      <td>paul samuelson; paul samuelson; paul samuelson</td>\n",
       "      <td>475; 475; 475</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>572756265951b619008f886f</td>\n",
       "      <td>9 per cent</td>\n",
       "      <td>9; 9; 9</td>\n",
       "      <td>529; 529; 529</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>570d391fb3d812140066d577</td>\n",
       "      <td>eastwest</td>\n",
       "      <td>eastwest; eastwest; eastwest</td>\n",
       "      <td>142; 142; 142</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>572750e8dd62a815002e9af4</td>\n",
       "      <td>attorney</td>\n",
       "      <td>attorney; attorney; attorney</td>\n",
       "      <td>517; 520; 517</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>57269e3bf1498d1400e8e517</td>\n",
       "      <td>member state cannot enforce conflicting laws a...</td>\n",
       "      <td>citizen or company can invoke directive not ju...</td>\n",
       "      <td>632; 118; 118</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5737477bc3c5551400e51ea4</td>\n",
       "      <td>parabolic path in same direction as motion of ...</td>\n",
       "      <td>parabolic; parabolic path; curving parabolic p...</td>\n",
       "      <td>411; 411; 401; 403; 403</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5705f7c875f01819005e77e0</td>\n",
       "      <td>metropolitan region</td>\n",
       "      <td>international metropolitan; international metr...</td>\n",
       "      <td>680; 677; 680</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>57264fe65951b619008f6fa3</td>\n",
       "      <td>it is thought to have died out</td>\n",
       "      <td>may no longer exist; may no longer exist; may ...</td>\n",
       "      <td>298; 298; 298</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>572658daf1498d1400e8dcad</td>\n",
       "      <td>colloblasts are specialized mushroomshaped cel...</td>\n",
       "      <td>specialized mushroomshaped cells in outer laye...</td>\n",
       "      <td>297; 297; 297</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>572864542ca10214002da2de</td>\n",
       "      <td>university athletic association</td>\n",
       "      <td>university athletic association; university at...</td>\n",
       "      <td>61; 65; 65</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>571ce3745efbb31900334e21</td>\n",
       "      <td>pulmonary fibrosis</td>\n",
       "      <td>pulmonary fibrosis; permanent pulmonary fibros...</td>\n",
       "      <td>238; 228; 228; 228; 238</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>57273e50dd62a815002e9a05</td>\n",
       "      <td>transportation sewer hazardous waste and water</td>\n",
       "      <td>transportation sewer hazardous waste and water...</td>\n",
       "      <td>652; 644; 652</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>572841772ca10214002da1a7</td>\n",
       "      <td>last glacial maximum lgm and subsequent deglac...</td>\n",
       "      <td>last glacial maximum lgm and subsequent deglac...</td>\n",
       "      <td>126; 283; 283</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>573735e8c3c5551400e51e73</td>\n",
       "      <td>sir isaac newton</td>\n",
       "      <td>sir isaac newton; sir isaac newton; sir isaac ...</td>\n",
       "      <td>654; 654; 654; 654; 654; 654</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>5733db8dd058e614000b642a</td>\n",
       "      <td>c√©lorons expedition force consisted of about 2...</td>\n",
       "      <td>british merchants or furtraders c√©loron inform...</td>\n",
       "      <td>614; 712; 590; 519; 590</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>573098f38ab72b1400f9c5d5</td>\n",
       "      <td>mughal state</td>\n",
       "      <td>mughal state; mughal state; mughal state; mugh...</td>\n",
       "      <td>345; 349; 349; 349; 349</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>57297547af94a219006aa45f</td>\n",
       "      <td>unit</td>\n",
       "      <td>its own special category as unit; unit; unit; ...</td>\n",
       "      <td>742; 773; 770; 773; 770</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>57285ed5ff5b5019007da1b7</td>\n",
       "      <td>great yuan</td>\n",
       "      <td>great yuan; great yuan; great yuan</td>\n",
       "      <td>62; 62; 62</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>572fe9b3947a6a140053cde0</td>\n",
       "      <td>westward</td>\n",
       "      <td>westward; westward; westward</td>\n",
       "      <td>55; 55; 55</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5733dd4f4776f419006613ac</td>\n",
       "      <td>william shirley felt that french were threat t...</td>\n",
       "      <td>british colonists would not be safe as long as...</td>\n",
       "      <td>532; 532; 532; 532; 509</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  \\\n",
       "0   57111b95a58dae1900cd6c53   \n",
       "1   56e1c0f6cd28a01900c67b2c   \n",
       "2   57264228ec44d21400f3dcf9   \n",
       "3   57263eaa38643c19005ad373   \n",
       "4   5710eca0a58dae1900cd6b3e   \n",
       "5   5733a6ac4776f41900660f5b   \n",
       "6   572fc6f204bcaa1900d76cf7   \n",
       "7   56e1d9fee3433e14004231cb   \n",
       "8   57108d69b654c5140001f985   \n",
       "9   572a020f6aef051400155199   \n",
       "10  572ff626947a6a140053ce91   \n",
       "11  56e1c4fce3433e140042314f   \n",
       "12  572f6c85947a6a140053c942   \n",
       "13  5726a7ecf1498d1400e8e656   \n",
       "14  57378b141c456719005744a1   \n",
       "15  57340d124776f419006617c3   \n",
       "16  572998673f37b319004784d6   \n",
       "17  572a07c11d046914007796d6   \n",
       "18  572fc5a1947a6a140053cc8a   \n",
       "19  57264586f1498d1400e8dac7   \n",
       "20  5729f24baf94a219006aa6e1   \n",
       "21  5728dddc2ca10214002da9d6   \n",
       "22  5726722bdd62a815002e8528   \n",
       "23  5728d7c54b864d1900164f4e   \n",
       "24  571093aba58dae1900cd6a5e   \n",
       "25  572ff932a23a5019007fcbd7   \n",
       "26  57263677ec44d21400f3dc4c   \n",
       "27  5705eee952bb8914006896df   \n",
       "28  572870b2ff5b5019007da224   \n",
       "29  572756265951b619008f886f   \n",
       "30  570d391fb3d812140066d577   \n",
       "31  572750e8dd62a815002e9af4   \n",
       "32  57269e3bf1498d1400e8e517   \n",
       "33  5737477bc3c5551400e51ea4   \n",
       "34  5705f7c875f01819005e77e0   \n",
       "35  57264fe65951b619008f6fa3   \n",
       "36  572658daf1498d1400e8dcad   \n",
       "37  572864542ca10214002da2de   \n",
       "38  571ce3745efbb31900334e21   \n",
       "39  57273e50dd62a815002e9a05   \n",
       "40  572841772ca10214002da1a7   \n",
       "41  573735e8c3c5551400e51e73   \n",
       "42  5733db8dd058e614000b642a   \n",
       "43  573098f38ab72b1400f9c5d5   \n",
       "44  57297547af94a219006aa45f   \n",
       "45  57285ed5ff5b5019007da1b7   \n",
       "46  572fe9b3947a6a140053cde0   \n",
       "47  5733dd4f4776f419006613ac   \n",
       "\n",
       "                                      prediction_text  \\\n",
       "0                                  lothar de maizi√®re   \n",
       "1                                  complexity classes   \n",
       "2                                                 gte   \n",
       "3                      water flow through body cavity   \n",
       "4                                                1705   \n",
       "5                                                poet   \n",
       "6              fact that there is no revising chamber   \n",
       "7                                          npcomplete   \n",
       "8                                           2 million   \n",
       "9                               destruction of forest   \n",
       "10                                          two poles   \n",
       "11                                   boolean circuits   \n",
       "12                                           duisburg   \n",
       "13                               articles 106 and 107   \n",
       "14                                              force   \n",
       "15                                               cuba   \n",
       "16                                        underground   \n",
       "17                                         trio tribe   \n",
       "18                              outcome of most votes   \n",
       "19  datanet 1 was public switched data network ope...   \n",
       "20                         lower levels of inequality   \n",
       "21                                             exodus   \n",
       "22                                                 62   \n",
       "23                   definition of civil disobedience   \n",
       "24  it gave them religious and political freedom w...   \n",
       "25                                          islamists   \n",
       "26                        store and forward switching   \n",
       "27                                southern california   \n",
       "28                                     paul samuelson   \n",
       "29                                         9 per cent   \n",
       "30                                           eastwest   \n",
       "31                                           attorney   \n",
       "32  member state cannot enforce conflicting laws a...   \n",
       "33  parabolic path in same direction as motion of ...   \n",
       "34                                metropolitan region   \n",
       "35                     it is thought to have died out   \n",
       "36  colloblasts are specialized mushroomshaped cel...   \n",
       "37                    university athletic association   \n",
       "38                                 pulmonary fibrosis   \n",
       "39     transportation sewer hazardous waste and water   \n",
       "40  last glacial maximum lgm and subsequent deglac...   \n",
       "41                                   sir isaac newton   \n",
       "42  c√©lorons expedition force consisted of about 2...   \n",
       "43                                       mughal state   \n",
       "44                                               unit   \n",
       "45                                         great yuan   \n",
       "46                                           westward   \n",
       "47  william shirley felt that french were threat t...   \n",
       "\n",
       "                                    ground_truth_text  \\\n",
       "0   lothar de maizi√®re; lothar de maizi√®re; lothar...   \n",
       "1   complexity classes; complexity classes; some c...   \n",
       "2   telenet was incorporated in 1973 and started o...   \n",
       "3   water flow through body cavity; Œ∫œÑŒµŒØœÇ kteis co...   \n",
       "4                      12 may 1705; 1705; 12 may 1705   \n",
       "5                                    poet; poet; poet   \n",
       "6   no revising chamber; no revising chamber; take...   \n",
       "7                  npcomplete; npcomplete; npcomplete   \n",
       "8                  ca 2 million; 2 million; 2 million   \n",
       "9   destruction of forest; destruction of forest; ...   \n",
       "10                            poles; two poles; poles   \n",
       "11                 boolean; boolean; boolean circuits   \n",
       "12                       duisburg; duisburg; duisburg   \n",
       "13  articles 106 and 107; articles 106 and 107; ar...   \n",
       "14  gravitational force; gravitational force; grav...   \n",
       "15  most went to cuba; most went to cuba; cuba; cu...   \n",
       "16  as grubs underground; underground; underground...   \n",
       "17                             trio tribe; trio; trio   \n",
       "18              votes; outcome of most votes; outcome   \n",
       "19  datanet 1 only referred to network and connect...   \n",
       "20  lower levels of inequality; lower levels of in...   \n",
       "21     book of exodus; exodus; exodus; exodus; exodus   \n",
       "22       62 acres; quarter square; quarter square; 62   \n",
       "23  semantical problems and grammatical niceties; ...   \n",
       "24  granted protestants equality with catholics; g...   \n",
       "25   its supporters; scholars and observers; islamism   \n",
       "26  by store and forward switching; packets; store...   \n",
       "27  alta california; alta california; alta california   \n",
       "28     paul samuelson; paul samuelson; paul samuelson   \n",
       "29                                            9; 9; 9   \n",
       "30                       eastwest; eastwest; eastwest   \n",
       "31                       attorney; attorney; attorney   \n",
       "32  citizen or company can invoke directive not ju...   \n",
       "33  parabolic; parabolic path; curving parabolic p...   \n",
       "34  international metropolitan; international metr...   \n",
       "35  may no longer exist; may no longer exist; may ...   \n",
       "36  specialized mushroomshaped cells in outer laye...   \n",
       "37  university athletic association; university at...   \n",
       "38  pulmonary fibrosis; permanent pulmonary fibros...   \n",
       "39  transportation sewer hazardous waste and water...   \n",
       "40  last glacial maximum lgm and subsequent deglac...   \n",
       "41  sir isaac newton; sir isaac newton; sir isaac ...   \n",
       "42  british merchants or furtraders c√©loron inform...   \n",
       "43  mughal state; mughal state; mughal state; mugh...   \n",
       "44  its own special category as unit; unit; unit; ...   \n",
       "45                 great yuan; great yuan; great yuan   \n",
       "46                       westward; westward; westward   \n",
       "47  british colonists would not be safe as long as...   \n",
       "\n",
       "                    answer_start  exact_match  f1_score  \n",
       "0                  588; 588; 588            1    1.0000  \n",
       "1                     16; 16; 11            1    1.0000  \n",
       "2                  560; 669; 669            1    1.0000  \n",
       "3                   801; 90; 801            1    1.0000  \n",
       "4                  420; 427; 420            1    1.0000  \n",
       "5                  273; 273; 273            1    1.0000  \n",
       "6                  313; 313; 400            0    0.6000  \n",
       "7                  244; 244; 244            1    1.0000  \n",
       "8                  367; 371; 371            1    1.0000  \n",
       "9                     81; 81; 81            1    1.0000  \n",
       "10                 403; 399; 403            1    1.0000  \n",
       "11                 150; 150; 150            1    1.0000  \n",
       "12                 298; 298; 298            1    1.0000  \n",
       "13                 949; 949; 949            1    1.0000  \n",
       "14        1117; 1117; 1117; 1117            0    0.6667  \n",
       "15       831; 831; 844; 844; 844            1    1.0000  \n",
       "16            133; 142; 142; 142            1    1.0000  \n",
       "17                 249; 249; 249            1    1.0000  \n",
       "18                      20; 4; 4            1    1.0000  \n",
       "19                 119; 146; 146            0    0.5455  \n",
       "20                 514; 514; 514            1    1.0000  \n",
       "21       983; 991; 991; 991; 991            1    1.0000  \n",
       "22              114; 88; 88; 114            1    1.0000  \n",
       "23            268; 260; 268; 268            0    0.1818  \n",
       "24                 390; 390; 390            0    0.5185  \n",
       "25                   107; 238; 0            0    0.0000  \n",
       "26                 665; 625; 668            1    1.0000  \n",
       "27                 281; 281; 281            0    0.5000  \n",
       "28                 475; 475; 475            1    1.0000  \n",
       "29                 529; 529; 529            0    0.5000  \n",
       "30                 142; 142; 142            1    1.0000  \n",
       "31                 517; 520; 517            1    1.0000  \n",
       "32                 632; 118; 118            0    0.7879  \n",
       "33       411; 411; 401; 403; 403            0    0.3636  \n",
       "34                 680; 677; 680            0    0.8000  \n",
       "35                 298; 298; 298            0    0.0000  \n",
       "36                 297; 297; 297            0    0.4324  \n",
       "37                    61; 65; 65            1    1.0000  \n",
       "38       238; 228; 228; 228; 238            1    1.0000  \n",
       "39                 652; 644; 652            1    1.0000  \n",
       "40                 126; 283; 283            1    1.0000  \n",
       "41  654; 654; 654; 654; 654; 654            1    1.0000  \n",
       "42       614; 712; 590; 519; 590            0    0.0870  \n",
       "43       345; 349; 349; 349; 349            1    1.0000  \n",
       "44       742; 773; 770; 773; 770            1    1.0000  \n",
       "45                    62; 62; 62            1    1.0000  \n",
       "46                    55; 55; 55            1    1.0000  \n",
       "47       532; 532; 532; 532; 509            0    0.2727  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csv results\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"evaluation_with_predictions_and_references.csv\")\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
