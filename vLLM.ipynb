{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 12:38:22 [__init__.py:239] Automatically detected platform cuda.\n",
      "2025-04-15 12:38:22.721132: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744720702.738654   45243 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744720702.743890   45243 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744720702.760125   45243 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744720702.760154   45243 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744720702.760157   45243 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744720702.760161   45243 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-15 12:38:22.764279: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO 04-15 12:38:27 [api_server.py:1034] vLLM API server version 0.8.4\n",
      "INFO 04-15 12:38:27 [api_server.py:1035] args: Namespace(subparser='serve', model_tag='meta-llama/Llama-3.1-8B', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='meta-llama/Llama-3.1-8B', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, load_format='auto', download_dir=None, model_loader_extra_config=None, use_tqdm_on_load=True, config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=8192, guided_decoding_backend='auto', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_chunked_mm_input=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7cbc355567a0>)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/fastllm_venv/bin/vllm\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 51, in main\n",
      "    args.dispatch_function(args)\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 27, in cmd\n",
      "    uvloop.run(run_server(args))\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/uvloop/__init__.py\", line 105, in run\n",
      "    return runner.run(wrapper())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/runners.py\", line 118, in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"uvloop/loop.pyx\", line 1512, in uvloop.loop.Loop.run_until_complete\n",
      "  File \"uvloop/loop.pyx\", line 1505, in uvloop.loop.Loop.run_until_complete\n",
      "  File \"uvloop/loop.pyx\", line 1379, in uvloop.loop.Loop.run_forever\n",
      "  File \"uvloop/loop.pyx\", line 557, in uvloop.loop.Loop._run\n",
      "  File \"uvloop/loop.pyx\", line 476, in uvloop.loop.Loop._on_idle\n",
      "  File \"uvloop/cbhandles.pyx\", line 83, in uvloop.loop.Handle._run\n",
      "  File \"uvloop/cbhandles.pyx\", line 61, in uvloop.loop.Handle._run\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/uvloop/__init__.py\", line 61, in wrapper\n",
      "    return await main\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1069, in run_server\n",
      "    async with build_async_engine_client(args) as engine_client:\n",
      "  File \"/usr/lib/python3.11/contextlib.py\", line 210, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 146, in build_async_engine_client\n",
      "    async with build_async_engine_client_from_engine_args(\n",
      "  File \"/usr/lib/python3.11/contextlib.py\", line 210, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 166, in build_async_engine_client_from_engine_args\n",
      "    vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1154, in create_engine_config\n",
      "    model_config = self.create_model_config()\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1042, in create_model_config\n",
      "    return ModelConfig(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/config.py\", line 489, in __init__\n",
      "    self.multimodal_config = self._init_multimodal_config(\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/config.py\", line 558, in _init_multimodal_config\n",
      "    if self.registry.is_multimodal_model(self.architectures):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/models/registry.py\", line 496, in is_multimodal_model\n",
      "    model_cls, _ = self.inspect_model_cls(architectures)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/models/registry.py\", line 452, in inspect_model_cls\n",
      "    model_info = self._try_inspect_model_cls(arch)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/models/registry.py\", line 425, in _try_inspect_model_cls\n",
      "    return _try_inspect_model_cls(model_arch, self.models[model_arch])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/models/registry.py\", line 344, in _try_inspect_model_cls\n",
      "    return model.inspect_model_cls()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/models/registry.py\", line 315, in inspect_model_cls\n",
      "    return _run_in_subprocess(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/models/registry.py\", line 572, in _run_in_subprocess\n",
      "    returned = subprocess.run(_SUBPROCESS_COMMAND,\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/subprocess.py\", line 550, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/subprocess.py\", line 1209, in communicate\n",
      "    stdout, stderr = self._communicate(input, endtime, timeout)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/subprocess.py\", line 2115, in _communicate\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/runners.py\", line 157, in _on_sigint\n",
      "    raise KeyboardInterrupt()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!vllm serve meta-llama/Llama-3.1-8B --max-model-len 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import requests\n",
    "import json\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter Notebook\n",
    "import time\n",
    "\n",
    "# Load test data\n",
    "dataset = load_dataset(\"gigaword\", split=\"test[:100]\")  # Limit to 100 for fast eval\n",
    "\n",
    "# Initialize ROUGE metric\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_with_vllm(document, max_tokens=50):\n",
    "    prompt_template = (\n",
    "        \"You are an AI assistant specialized in summarizing news articles. \"\n",
    "        \"Summarize the following news sentence into a concise headline.\\n\\n\"\n",
    "\n",
    "        \"Here is an example:\\n\"\n",
    "        \"News: Japan 's nec corp. and UNK computer corp. of the united states said wednesday they had agreed to join forces in supercomputer sales.\\n\"\n",
    "        \"Headline: Nec UNK in computer sales tie-up\\n\\n\"\n",
    "\n",
    "        \"Now summarize the following news:\\n\\n\"\n",
    "\n",
    "        \"News: {document}\\n\\n\"\n",
    "        \"Headline:\"\n",
    "    )\n",
    "    \n",
    "    prompt = prompt_template.format(document=document)\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.3,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"http://localhost:8000/v1/completions\", json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        summary = result['choices'][0]['text'].strip()\n",
    "        return summary\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7284356cf7e3494d991c7c931c8bc69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing:   0%|          | 0/100 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM (Llama-3.1-8B) Summarization Results:\n",
      "\n",
      "Number of examples: 98\n",
      "\n",
      "Elapsed time: 271.17 s\n",
      "\n",
      "ROUGE Results:\n",
      "rouge1: 0.1682\n",
      "rouge2: 0.0582\n",
      "rougeL: 0.1498\n",
      "rougeLsum: 0.1538\n"
     ]
    }
   ],
   "source": [
    "# Generate summaries and evaluate\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# tqdm around dataset loop with a description and progress bar\n",
    "for item in tqdm(dataset, desc=\"Summarizing\", unit=\"example\"):\n",
    "\n",
    "    doc = item['document']\n",
    "    ref_summary = item['summary']\n",
    "\n",
    "    pred_summary = summarize_with_vllm(doc)\n",
    "\n",
    "    if pred_summary:\n",
    "        references.append(ref_summary)\n",
    "        predictions.append(pred_summary)\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# Evaluate with ROUGE\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"vLLM (Llama-3.1-8B) Summarization Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {len(references)}\")\n",
    "print(f\"\\nElapsed time: {end - start:.2f} s\")\n",
    "\n",
    "print(\"\\nROUGE Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmarking vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Load test data\n",
    "dataset = load_dataset(\"gigaword\", split=\"test[:100]\")  # Limit to 100 for fast eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def empty_GPU_cache():\n",
    "    \"\"\"\n",
    "    Clear GPU memory cache.\n",
    "    \"\"\"\n",
    "    del benchmark.tokenizer\n",
    "    del benchmark.llm  # If using llama.cpp\n",
    "    torch.cuda.empty_cache()\n",
    "    benchmark.close()  # Shutdown NVML\n",
    "\n",
    "    print(\"GPU memory released and NVML shutdown complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a headline generation assistant. Given a news article, produce a concise and informative headline.\n",
      "\n",
      "Here are some examples:\n",
      "News: Scientists have discovered a new exoplanet that appears to have water on its surface, raising hopes it may be habitable.\n",
      "Headline: New exoplanet may support life\n",
      "\n",
      "News: The stock market experienced a significant downturn today, with major indices falling sharply amid economic uncertainty.\n",
      "Headline: Stock market plunges amid economic fears\n",
      "\n",
      "News: japan 's nec corp. and UNK computer corp. of the united states said wednesday they had agreed to join forces in supercomputer sales .\n",
      "Headline:\n"
     ]
    }
   ],
   "source": [
    "def sum_prompt(document):\n",
    "    \"\"\"\n",
    "    Summarize the given `document` into a concise headline using a few-shot prompt.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a headline generation assistant. Given a news article, produce a concise and informative headline.\\n\\n\"\n",
    "\n",
    "        \"Here are some examples:\\n\"\n",
    "\n",
    "        \"News: Scientists have discovered a new exoplanet that appears to have water on its surface, raising hopes it may be habitable.\\n\"\n",
    "        \"Headline: New exoplanet may support life\\n\\n\"\n",
    "\n",
    "        \"News: The stock market experienced a significant downturn today, with major indices falling sharply amid economic uncertainty.\\n\"\n",
    "        \"Headline: Stock market plunges amid economic fears\\n\\n\"\n",
    "\n",
    "        f\"News: {document}\\n\"\n",
    "        \"Headline:\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "print(sum_prompt(dataset[0][\"document\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import os\n",
    "\n",
    "model_name = \"llama-3.1-8B-Instruct\"\n",
    "\n",
    "model_path = f\"/home/ubuntu/fast_llm_inference/{model_name}\"\n",
    "\n",
    "benchmark = ModelBenchmark(\n",
    "    backend=\"vllm\",\n",
    "    task=\"summarization\",\n",
    "    model_path=model_path,\n",
    "    llama_gpu_layers=-1,\n",
    "    max_tokens=70\n",
    ")\n",
    "\n",
    "results = benchmark.benchmark(prompts=[sum_prompt(i) for i in dataset[\"document\"]], \n",
    "                              references=[i for i in dataset[\"summary\"]])\n",
    "\n",
    "empty_GPU_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>TTFT</th>\n",
       "      <th>ATL</th>\n",
       "      <th>GL</th>\n",
       "      <th>TPS</th>\n",
       "      <th>SPS</th>\n",
       "      <th>Memory Usage (MB)</th>\n",
       "      <th>Model Size (MB)</th>\n",
       "      <th>Overhead (MB)</th>\n",
       "      <th>GPU_Utilization (%)</th>\n",
       "      <th>Total Energy (Wh)</th>\n",
       "      <th>Energy per Token (J/token)</th>\n",
       "      <th>Energy per Sentence (J/sentence)</th>\n",
       "      <th>Energy per Second (W)</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>BERTScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>704</td>\n",
       "      <td>You are a headline generation assistant. Given...</td>\n",
       "      <td>World leaders gather for Rabin funeral</td>\n",
       "      <td>israel prepares jerusalem state funeral for rabin</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>0.5007</td>\n",
       "      <td>11.98</td>\n",
       "      <td>2.00</td>\n",
       "      <td>22613.19</td>\n",
       "      <td>14219.518604</td>\n",
       "      <td>8393.671396</td>\n",
       "      <td>97</td>\n",
       "      <td>0.008614</td>\n",
       "      <td>5.168482</td>\n",
       "      <td>31.010890</td>\n",
       "      <td>61.93</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.888598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>703</td>\n",
       "      <td>You are a headline generation assistant. Given...</td>\n",
       "      <td>Kashmir violence erupts over rao's autonomy plan</td>\n",
       "      <td>indian pm 's announcement on kashmir polls aut...</td>\n",
       "      <td>0.0656</td>\n",
       "      <td>0.1544</td>\n",
       "      <td>0.9266</td>\n",
       "      <td>7.55</td>\n",
       "      <td>1.08</td>\n",
       "      <td>22613.19</td>\n",
       "      <td>14219.518604</td>\n",
       "      <td>8393.671396</td>\n",
       "      <td>98</td>\n",
       "      <td>0.016995</td>\n",
       "      <td>8.740443</td>\n",
       "      <td>61.183098</td>\n",
       "      <td>66.03</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.861604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>499</td>\n",
       "      <td>You are a headline generation assistant. Given...</td>\n",
       "      <td>UNK</td>\n",
       "      <td>russian liberal party wins registration</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>5.08</td>\n",
       "      <td>5.08</td>\n",
       "      <td>22613.19</td>\n",
       "      <td>14219.518604</td>\n",
       "      <td>8393.671396</td>\n",
       "      <td>66</td>\n",
       "      <td>0.003136</td>\n",
       "      <td>11.290758</td>\n",
       "      <td>11.290758</td>\n",
       "      <td>57.33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_length                                             prompt  \\\n",
       "7            704  You are a headline generation assistant. Given...   \n",
       "8            703  You are a headline generation assistant. Given...   \n",
       "9            499  You are a headline generation assistant. Given...   \n",
       "\n",
       "                                    generated_answer  \\\n",
       "7             World leaders gather for Rabin funeral   \n",
       "8   Kashmir violence erupts over rao's autonomy plan   \n",
       "9                                                UNK   \n",
       "\n",
       "                                    reference_answer    TTFT     ATL      GL  \\\n",
       "7  israel prepares jerusalem state funeral for rabin  0.0655  0.1001  0.5007   \n",
       "8  indian pm 's announcement on kashmir polls aut...  0.0656  0.1544  0.9266   \n",
       "9            russian liberal party wins registration  0.0667  0.0000  0.1969   \n",
       "\n",
       "     TPS   SPS  Memory Usage (MB)  Model Size (MB)  Overhead (MB)  \\\n",
       "7  11.98  2.00           22613.19     14219.518604    8393.671396   \n",
       "8   7.55  1.08           22613.19     14219.518604    8393.671396   \n",
       "9   5.08  5.08           22613.19     14219.518604    8393.671396   \n",
       "\n",
       "   GPU_Utilization (%)  Total Energy (Wh)  Energy per Token (J/token)  \\\n",
       "7                   97           0.008614                    5.168482   \n",
       "8                   98           0.016995                    8.740443   \n",
       "9                   66           0.003136                   11.290758   \n",
       "\n",
       "   Energy per Sentence (J/sentence)  Energy per Second (W)   ROUGE-1  \\\n",
       "7                         31.010890                  61.93  0.461538   \n",
       "8                         61.183098                  66.03  0.333333   \n",
       "9                         11.290758                  57.33  0.000000   \n",
       "\n",
       "    ROUGE-L  BERTScore  \n",
       "7  0.307692   0.888598  \n",
       "8  0.222222   0.861604  \n",
       "9  0.000000   0.800133  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics (mean ± std) for llama-3.1-8B-Instruct:\n",
      "prompt_length                          671.680000 ± 38.617492\n",
      "TTFT                                      0.124431 ± 0.000960\n",
      "ATL                                       0.100500 ± 0.027451\n",
      "GL                                        0.613707 ± 0.176179\n",
      "TPS                                      11.723100 ± 2.083747\n",
      "SPS                                       1.800900 ± 0.659893\n",
      "Memory Usage (MB)                   22594.498700 ± 126.913000\n",
      "Model Size (MB)                       15327.360256 ± 0.000000\n",
      "Overhead (MB)                        7267.138444 ± 126.913000\n",
      "GPU_Utilization (%)                      97.450000 ± 1.274260\n",
      "Total Energy (Wh)                         0.010297 ± 0.003833\n",
      "Energy per Token (J/token)                5.313529 ± 1.565282\n",
      "Energy per Sentence (J/sentence)        36.701358 ± 13.300023\n",
      "Energy per Second (W)                    59.264100 ± 4.949606\n",
      "ROUGE-1                                   0.341666 ± 0.216097\n",
      "ROUGE-L                                   0.315891 ± 0.209783\n",
      "BERTScore                                 0.864220 ± 0.040673\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "results.to_csv(f\"vLLM_results/{model_name}_summarization.csv\", index=False)\n",
    "\n",
    "# Compute statistics\n",
    "numeric_results = results.select_dtypes(include='number')\n",
    "averages = numeric_results.mean()\n",
    "stds = numeric_results.std()\n",
    "\n",
    "# Combine mean ± std into a formatted string\n",
    "summary = averages.combine(stds, lambda mean, std: f\"{mean:.6f} ± {std:.6f}\")\n",
    "\n",
    "# Print formatted summary\n",
    "print(f\"Statistics (mean ± std) for {model_name}:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics (mean ± std) for Teuken-7B-instruct-research-v0.4:\n",
      "Number of examples: 100\n",
      "prompt_length                          671.680000 ± 38.617492\n",
      "TTFT                                      0.075690 ± 0.020008\n",
      "ATL                                       0.104796 ± 0.025269\n",
      "GL                                        0.714092 ± 0.168455\n",
      "TPS                                      10.992200 ± 1.962327\n",
      "SPS                                       1.530100 ± 0.614085\n",
      "Memory Usage (MB)                   22341.010000 ± 101.800000\n",
      "Model Size (MB)                       14219.518604 ± 0.000000\n",
      "Overhead (MB)                        8121.491396 ± 101.800000\n",
      "GPU_Utilization (%)                      96.190000 ± 9.402015\n",
      "Total Energy (Wh)                         0.012157 ± 0.003838\n",
      "Energy per Token (J/token)                5.701451 ± 1.381443\n",
      "Energy per Sentence (J/sentence)        43.567129 ± 14.014799\n",
      "Energy per Second (W)                    60.199400 ± 5.275295\n",
      "ROUGE-1                                   0.343723 ± 0.215248\n",
      "ROUGE-L                                   0.311663 ± 0.205888\n",
      "BERTScore                                 0.863958 ± 0.040808\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "# results.to_csv(f\"vLLM_results/{model_name}_summarization.csv\", index=False)\n",
    "\n",
    "# Compute statistics\n",
    "numeric_results = results.select_dtypes(include='number')\n",
    "averages = numeric_results.mean()\n",
    "stds = numeric_results.std()\n",
    "\n",
    "# Combine mean ± std into a formatted string\n",
    "summary = averages.combine(stds, lambda mean, std: f\"{mean:.6f} ± {std:.6f}\")\n",
    "\n",
    "# Print formatted summary\n",
    "print(f\"Statistics (mean ± std) for {model_name}:\")\n",
    "print(\"Number of examples:\", len(results))\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
