{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vllm serve meta-llama/Llama-3.1-8B --max-model-len 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import requests\n",
    "import json\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter Notebook\n",
    "import time\n",
    "\n",
    "# Load test data\n",
    "dataset = load_dataset(\"gigaword\", split=\"test[:100]\")  # Limit to 100 for fast eval\n",
    "\n",
    "# Initialize ROUGE metric\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_with_vllm(document, max_tokens=50):\n",
    "    prompt_template = (\n",
    "        \"You are an AI assistant specialized in summarizing news articles. \"\n",
    "        \"Summarize the following news sentence into a concise headline.\\n\\n\"\n",
    "\n",
    "        \"Here is an example:\\n\"\n",
    "        \"News: Japan 's nec corp. and UNK computer corp. of the united states said wednesday they had agreed to join forces in supercomputer sales.\\n\"\n",
    "        \"Headline: Nec UNK in computer sales tie-up\\n\\n\"\n",
    "\n",
    "        \"Now summarize the following news:\\n\\n\"\n",
    "\n",
    "        \"News: {document}\\n\\n\"\n",
    "        \"Headline:\"\n",
    "    )\n",
    "    \n",
    "    prompt = prompt_template.format(document=document)\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.3,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"http://localhost:8000/v1/completions\", json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        summary = result['choices'][0]['text'].strip()\n",
    "        return summary\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7284356cf7e3494d991c7c931c8bc69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing:   0%|          | 0/100 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM (Llama-3.1-8B) Summarization Results:\n",
      "\n",
      "Number of examples: 98\n",
      "\n",
      "Elapsed time: 271.17 s\n",
      "\n",
      "ROUGE Results:\n",
      "rouge1: 0.1682\n",
      "rouge2: 0.0582\n",
      "rougeL: 0.1498\n",
      "rougeLsum: 0.1538\n"
     ]
    }
   ],
   "source": [
    "# Generate summaries and evaluate\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# tqdm around dataset loop with a description and progress bar\n",
    "for item in tqdm(dataset, desc=\"Summarizing\", unit=\"example\"):\n",
    "\n",
    "    doc = item['document']\n",
    "    ref_summary = item['summary']\n",
    "\n",
    "    pred_summary = summarize_with_vllm(doc)\n",
    "\n",
    "    if pred_summary:\n",
    "        references.append(ref_summary)\n",
    "        predictions.append(pred_summary)\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# Evaluate with ROUGE\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"vLLM (Llama-3.1-8B) Summarization Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {len(references)}\")\n",
    "print(f\"\\nElapsed time: {end - start:.2f} s\")\n",
    "\n",
    "print(\"\\nROUGE Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 09:24:14 [config.py:689] This model supports multiple tasks: {'generate', 'score', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 04-15 09:24:14 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 04-15 09:24:17 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct', speculative_config=None, tokenizer='/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 04-15 09:24:18 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x740c824e2110>\n",
      "INFO 04-15 09:24:19 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-15 09:24:19 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 04-15 09:24:19 [gpu_model_runner.py:1276] Starting to load model /home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct...\n",
      "WARNING 04-15 09:24:19 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374056a7000341488e692c6cb7b579dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 09:24:25 [loader.py:458] Loading weights took 5.41 seconds\n",
      "INFO 04-15 09:24:25 [gpu_model_runner.py:1291] Model loading took 14.9596 GiB and 5.835499 seconds\n",
      "INFO 04-15 09:24:35 [backends.py:416] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/02b5663e4f/rank_0_0 for vLLM's torch.compile\n",
      "INFO 04-15 09:24:35 [backends.py:426] Dynamo bytecode transform time: 9.47 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:W0415 09:24:36.977000 40091 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 09:24:40 [backends.py:132] Cache the graph of shape None for later use\n",
      "INFO 04-15 09:25:15 [backends.py:144] Compiling a graph for general shape takes 38.99 s\n",
      "INFO 04-15 09:25:30 [monitor.py:33] torch.compile takes 48.46 s in total\n",
      "INFO 04-15 09:25:32 [kv_cache_utils.py:634] GPU KV cache size: 27,856 tokens\n",
      "INFO 04-15 09:25:32 [kv_cache_utils.py:637] Maximum concurrency for 8,192 tokens per request: 3.40x\n",
      "INFO 04-15 09:26:50 [gpu_model_runner.py:1626] Graph capturing finished in 78 secs, took 0.51 GiB\n",
      "INFO 04-15 09:26:50 [core.py:163] init engine (profile, create kv cache, warmup model) took 144.93 seconds\n",
      "INFO 04-15 09:26:50 [core_client.py:435] Core engine process 0 ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/ubuntu/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886 (last modified on Wed Mar 12 15:05:43 2025) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import os\n",
    "\n",
    "model_name = \"llama-3.1-8B-Instruct\"\n",
    "\n",
    "model_path = f\"/home/ubuntu/fast_llm_inference/{model_name}\"\n",
    "\n",
    "benchmark = ModelBenchmark(\n",
    "    backend=\"vllm\",\n",
    "    task=\"summarization\",\n",
    "    model_path=model_path,\n",
    "    llama_gpu_layers=-1,\n",
    "    max_tokens=70,\n",
    "    model_size= os.path.getsize(model_path) / 1e6, # in MB\n",
    ")\n",
    "\n",
    "results = benchmark.benchmark([\"\"\"You are a headline generation assistant. Given a news article, produce a concise and informative headline.\n",
    "\n",
    "Here are some examples:\n",
    "News: Scientists have discovered a new exoplanet that appears to have water on its surface, raising hopes it may be habitable.\n",
    "Headline: New exoplanet may support life\n",
    "\n",
    "News: The stock market experienced a significant downturn today, with major indices falling sharply amid economic uncertainty.\n",
    "Headline: Stock market plunges amid economic fears\n",
    "\n",
    "News: japan 's nec corp. and UNK computer corp. of the united states said wednesday they had agreed to join forces in supercomputer sales .\n",
    "Headline:\"\"\"], [\"nec corp. and UNK tie-up in supercomputer sales\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
