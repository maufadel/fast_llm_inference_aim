{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vllm serve meta-llama/Llama-3.1-8B --max-model-len 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def vLLM_llama3_stream(prompt, max_tokens=2048):\n",
    "    API_URL = \"http://localhost:8000/v1/completions\"\n",
    "\n",
    "    # Manual instruction since we're using /completions\n",
    "    system_instruction = \"You are an NLP expert and teacher.\"\n",
    "    full_prompt = f\"{system_instruction}\\n\\n{prompt}\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B\",\n",
    "        \"prompt\": full_prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "    response = requests.post(API_URL, json=payload, stream=True)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "    full_response = \"\"\n",
    "\n",
    "    print(\"üìù Generating response...\\n\")\n",
    "\n",
    "    for line in response.iter_lines():\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            decoded_line = line.decode('utf-8')\n",
    "\n",
    "            if decoded_line.startswith(\"data: \"):\n",
    "                decoded_line = decoded_line[6:]\n",
    "\n",
    "            if decoded_line.strip() == \"[DONE]\":\n",
    "                break\n",
    "\n",
    "            data = json.loads(decoded_line)\n",
    "\n",
    "            chunk_text = data.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
    "\n",
    "            print(chunk_text, end='', flush=True)\n",
    "\n",
    "            full_response += chunk_text\n",
    "        \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Skipping invalid line: {line}\\nError: {e}\")\n",
    "\n",
    "    print(\"\\n\\n‚úÖ Generation complete!\")\n",
    "\n",
    "    display(Markdown(f\"**Final Response:**\\n\\n{full_response.strip()}\"))\n",
    "\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Generating response...\n",
      "\n",
      "Transformer models are"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " very powerful and have demonstrated their ability to handle a wide range of NLP tasks. However, there are still some challenges that need to be addressed. One of the main challenges is the computation cost. The Transformer model is computationally expensive and requires a lot of memory and computational power. This can be a problem for large-scale applications. To address this issue, some researchers have proposed methods to reduce the computation cost of the Transformer model. Another challenge is the training time. The Transformer model can take a long time to train, especially for large-scale applications. To address this issue, some researchers have proposed methods to speed up the training process. Finally, there is the challenge of generalization. The Transformer model performs well on the tasks it is trained on, but it may not generalize well to other tasks. This is because the model may be overfitting to the training data. To address this issue, some researchers have proposed methods to improve the generalization ability of the Transformer model.\n",
      "## Here are some of the challenges faced by Transformer models in NLP, and how they are addressed:\n",
      "\n",
      "### Computation cost\n",
      "The Transformer model is computationally expensive and requires a lot of memory and computational power. This can be a problem for large-scale applications. To address this issue, some researchers have proposed methods to reduce the computation cost of the Transformer model. For example, they have proposed methods to reduce the number of parameters in the model, to use smaller networks, or to use less powerful hardware.\n",
      "\n",
      "### Training time\n",
      "The Transformer model can take a long time to train, especially for large-scale applications. To address this issue, some researchers have proposed methods to speed up the training process. For example, they have proposed methods to use gradient descent to optimize the model, to use data augmentation, or to use pre-trained models.\n",
      "\n",
      "### Generalization\n",
      "The Transformer model performs well on the tasks it is trained on, but it may not generalize well to other tasks. This is because the model may be overfitting to the training data. To address this issue, some researchers have proposed methods to improve the generalization ability of the Transformer model. For example, they have proposed methods to use regularization, to use dropout, or to use ensembles of models.\n",
      "\n",
      "### Memory consumption\n",
      "The Transformer model can consume a lot of memory, especially for large-scale applications. To address this issue, some researchers have proposed methods to reduce the memory consumption of the Transformer model. For example, they have proposed methods to use less powerful hardware, to use smaller networks, or to use data augmentation.\n",
      "\n",
      "‚úÖ Generation complete!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Final Response:**\n",
       "\n",
       "Transformer models are very powerful and have demonstrated their ability to handle a wide range of NLP tasks. However, there are still some challenges that need to be addressed. One of the main challenges is the computation cost. The Transformer model is computationally expensive and requires a lot of memory and computational power. This can be a problem for large-scale applications. To address this issue, some researchers have proposed methods to reduce the computation cost of the Transformer model. Another challenge is the training time. The Transformer model can take a long time to train, especially for large-scale applications. To address this issue, some researchers have proposed methods to speed up the training process. Finally, there is the challenge of generalization. The Transformer model performs well on the tasks it is trained on, but it may not generalize well to other tasks. This is because the model may be overfitting to the training data. To address this issue, some researchers have proposed methods to improve the generalization ability of the Transformer model.\n",
       "## Here are some of the challenges faced by Transformer models in NLP, and how they are addressed:\n",
       "\n",
       "### Computation cost\n",
       "The Transformer model is computationally expensive and requires a lot of memory and computational power. This can be a problem for large-scale applications. To address this issue, some researchers have proposed methods to reduce the computation cost of the Transformer model. For example, they have proposed methods to reduce the number of parameters in the model, to use smaller networks, or to use less powerful hardware.\n",
       "\n",
       "### Training time\n",
       "The Transformer model can take a long time to train, especially for large-scale applications. To address this issue, some researchers have proposed methods to speed up the training process. For example, they have proposed methods to use gradient descent to optimize the model, to use data augmentation, or to use pre-trained models.\n",
       "\n",
       "### Generalization\n",
       "The Transformer model performs well on the tasks it is trained on, but it may not generalize well to other tasks. This is because the model may be overfitting to the training data. To address this issue, some researchers have proposed methods to improve the generalization ability of the Transformer model. For example, they have proposed methods to use regularization, to use dropout, or to use ensembles of models.\n",
       "\n",
       "### Memory consumption\n",
       "The Transformer model can consume a lot of memory, especially for large-scale applications. To address this issue, some researchers have proposed methods to reduce the memory consumption of the Transformer model. For example, they have proposed methods to use less powerful hardware, to use smaller networks, or to use data augmentation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define your prompt here\n",
    "prompt = \"\"\"\n",
    "What are some of the challenges faced by Transformer models in NLP, and how are they addressed? Explain in detail.\n",
    "\"\"\"\n",
    "\n",
    "# Run the function\n",
    "response = vLLM_llama3_stream(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import requests\n",
    "import json\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter Notebook\n",
    "import time\n",
    "\n",
    "# Load test data\n",
    "dataset = load_dataset(\"gigaword\", split=\"test[:100]\")  # Limit to 100 for fast eval\n",
    "\n",
    "# Initialize ROUGE metric\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968ae0d3250b498b8e6d1d1652eb6fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing:   0%|          | 0/100 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROUGE Results:\n",
      "rouge1: 0.1827\n",
      "rouge2: 0.0696\n",
      "rougeL: 0.1696\n",
      "rougeLsum: 0.1715\n"
     ]
    }
   ],
   "source": [
    "def summarize_with_vllm(document, max_tokens=50):\n",
    "    prompt_template = (\n",
    "        \"You are an AI assistant specialized in summarizing news articles. \"\n",
    "        \"Summarize the following news sentence into a concise headline.\\n\\n\"\n",
    "\n",
    "        \"Here is an example:\\n\"\n",
    "        \"News: Japan 's nec corp. and UNK computer corp. of the united states said wednesday they had agreed to join forces in supercomputer sales.\\n\"\n",
    "        \"Headline: Nec UNK in computer sales tie-up\\n\\n\"\n",
    "\n",
    "        \"Now summarize the following news:\\n\\n\"\n",
    "\n",
    "        \"News: {document}\\n\\n\"\n",
    "        \"Headline:\"\n",
    "    )\n",
    "    \n",
    "    prompt = prompt_template.format(document=document)\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.3,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"http://localhost:8000/v1/completions\", json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        summary = result['choices'][0]['text'].strip()\n",
    "        return summary\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Generate summaries and evaluate\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# tqdm around dataset loop with a description and progress bar\n",
    "for item in tqdm(dataset, desc=\"Summarizing\", unit=\"example\"):\n",
    "\n",
    "    doc = item['document']\n",
    "    ref_summary = item['summary']\n",
    "\n",
    "    pred_summary = summarize_with_vllm(doc)\n",
    "\n",
    "    if pred_summary:\n",
    "        references.append(ref_summary)\n",
    "        predictions.append(pred_summary)\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# Evaluate with ROUGE\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"vLLM (Llama-3.1-8B) Summarization Results:\")\n",
    "\n",
    "print(f\"\\nElapsed time: {end - start:.2f} s\")\n",
    "\n",
    "print(\"\\nROUGE Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
