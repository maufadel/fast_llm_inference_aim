{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 12:38:22 [__init__.py:239] Automatically detected platform cuda.\n",
      "2025-04-15 12:38:22.721132: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744720702.738654   45243 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744720702.743890   45243 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744720702.760125   45243 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744720702.760154   45243 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744720702.760157   45243 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744720702.760161   45243 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-15 12:38:22.764279: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO 04-15 12:38:27 [api_server.py:1034] vLLM API server version 0.8.4\n",
      "INFO 04-15 12:38:27 [api_server.py:1035] args: Namespace(subparser='serve', model_tag='meta-llama/Llama-3.1-8B', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='meta-llama/Llama-3.1-8B', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, load_format='auto', download_dir=None, model_loader_extra_config=None, use_tqdm_on_load=True, config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=8192, guided_decoding_backend='auto', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_chunked_mm_input=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7cbc355567a0>)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/fastllm_venv/bin/vllm\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/entrypoints/cli/main.py\", line 51, in main\n",
      "    args.dispatch_function(args)\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/entrypoints/cli/serve.py\", line 27, in cmd\n",
      "    uvloop.run(run_server(args))\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/uvloop/__init__.py\", line 105, in run\n",
      "    return runner.run(wrapper())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/runners.py\", line 118, in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"uvloop/loop.pyx\", line 1512, in uvloop.loop.Loop.run_until_complete\n",
      "  File \"uvloop/loop.pyx\", line 1505, in uvloop.loop.Loop.run_until_complete\n",
      "  File \"uvloop/loop.pyx\", line 1379, in uvloop.loop.Loop.run_forever\n",
      "  File \"uvloop/loop.pyx\", line 557, in uvloop.loop.Loop._run\n",
      "  File \"uvloop/loop.pyx\", line 476, in uvloop.loop.Loop._on_idle\n",
      "  File \"uvloop/cbhandles.pyx\", line 83, in uvloop.loop.Handle._run\n",
      "  File \"uvloop/cbhandles.pyx\", line 61, in uvloop.loop.Handle._run\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/uvloop/__init__.py\", line 61, in wrapper\n",
      "    return await main\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 1069, in run_server\n",
      "    async with build_async_engine_client(args) as engine_client:\n",
      "  File \"/usr/lib/python3.11/contextlib.py\", line 210, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 146, in build_async_engine_client\n",
      "    async with build_async_engine_client_from_engine_args(\n",
      "  File \"/usr/lib/python3.11/contextlib.py\", line 210, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 166, in build_async_engine_client_from_engine_args\n",
      "    vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1154, in create_engine_config\n",
      "    model_config = self.create_model_config()\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 1042, in create_model_config\n",
      "    return ModelConfig(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/config.py\", line 489, in __init__\n",
      "    self.multimodal_config = self._init_multimodal_config(\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/config.py\", line 558, in _init_multimodal_config\n",
      "    if self.registry.is_multimodal_model(self.architectures):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/models/registry.py\", line 496, in is_multimodal_model\n",
      "    model_cls, _ = self.inspect_model_cls(architectures)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/models/registry.py\", line 452, in inspect_model_cls\n",
      "    model_info = self._try_inspect_model_cls(arch)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/models/registry.py\", line 425, in _try_inspect_model_cls\n",
      "    return _try_inspect_model_cls(model_arch, self.models[model_arch])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/models/registry.py\", line 344, in _try_inspect_model_cls\n",
      "    return model.inspect_model_cls()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/models/registry.py\", line 315, in inspect_model_cls\n",
      "    return _run_in_subprocess(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/models/registry.py\", line 572, in _run_in_subprocess\n",
      "    returned = subprocess.run(_SUBPROCESS_COMMAND,\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/subprocess.py\", line 550, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/subprocess.py\", line 1209, in communicate\n",
      "    stdout, stderr = self._communicate(input, endtime, timeout)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/subprocess.py\", line 2115, in _communicate\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/runners.py\", line 157, in _on_sigint\n",
      "    raise KeyboardInterrupt()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!vllm serve meta-llama/Llama-3.1-8B --max-model-len 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import requests\n",
    "import json\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter Notebook\n",
    "import time\n",
    "\n",
    "# Load test data\n",
    "dataset = load_dataset(\"gigaword\", split=\"test[:100]\")  # Limit to 100 for fast eval\n",
    "\n",
    "# Initialize ROUGE metric\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_with_vllm(document, max_tokens=50):\n",
    "    prompt_template = (\n",
    "        \"You are an AI assistant specialized in summarizing news articles. \"\n",
    "        \"Summarize the following news sentence into a concise headline.\\n\\n\"\n",
    "\n",
    "        \"Here is an example:\\n\"\n",
    "        \"News: Japan 's nec corp. and UNK computer corp. of the united states said wednesday they had agreed to join forces in supercomputer sales.\\n\"\n",
    "        \"Headline: Nec UNK in computer sales tie-up\\n\\n\"\n",
    "\n",
    "        \"Now summarize the following news:\\n\\n\"\n",
    "\n",
    "        \"News: {document}\\n\\n\"\n",
    "        \"Headline:\"\n",
    "    )\n",
    "    \n",
    "    prompt = prompt_template.format(document=document)\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.3,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"http://localhost:8000/v1/completions\", json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        summary = result['choices'][0]['text'].strip()\n",
    "        return summary\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7284356cf7e3494d991c7c931c8bc69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarizing:   0%|          | 0/100 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM (Llama-3.1-8B) Summarization Results:\n",
      "\n",
      "Number of examples: 98\n",
      "\n",
      "Elapsed time: 271.17 s\n",
      "\n",
      "ROUGE Results:\n",
      "rouge1: 0.1682\n",
      "rouge2: 0.0582\n",
      "rougeL: 0.1498\n",
      "rougeLsum: 0.1538\n"
     ]
    }
   ],
   "source": [
    "# Generate summaries and evaluate\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# tqdm around dataset loop with a description and progress bar\n",
    "for item in tqdm(dataset, desc=\"Summarizing\", unit=\"example\"):\n",
    "\n",
    "    doc = item['document']\n",
    "    ref_summary = item['summary']\n",
    "\n",
    "    pred_summary = summarize_with_vllm(doc)\n",
    "\n",
    "    if pred_summary:\n",
    "        references.append(ref_summary)\n",
    "        predictions.append(pred_summary)\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# Evaluate with ROUGE\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"vLLM (Llama-3.1-8B) Summarization Results:\")\n",
    "\n",
    "print(f\"\\nNumber of examples: {len(references)}\")\n",
    "print(f\"\\nElapsed time: {end - start:.2f} s\")\n",
    "\n",
    "print(\"\\nROUGE Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmarking vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Load test data\n",
    "dataset = load_dataset(\"gigaword\", split=\"test[:100]\")  # Limit to 100 for fast eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def empty_GPU_cache():\n",
    "    \"\"\"\n",
    "    Clear GPU memory cache.\n",
    "    \"\"\"\n",
    "    del benchmark.model\n",
    "    del benchmark.tokenizer\n",
    "    del benchmark.llm  # If using llama.cpp\n",
    "    torch.cuda.empty_cache()\n",
    "    benchmark.close()  # Shutdown NVML\n",
    "\n",
    "    print(\"GPU memory released and NVML shutdown complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a headline generation assistant. Given a news article, produce a concise and informative headline.\n",
      "\n",
      "Here are some examples:\n",
      "News: Scientists have discovered a new exoplanet that appears to have water on its surface, raising hopes it may be habitable.\n",
      "Headline: New exoplanet may support life\n",
      "\n",
      "News: The stock market experienced a significant downturn today, with major indices falling sharply amid economic uncertainty.\n",
      "Headline: Stock market plunges amid economic fears\n",
      "\n",
      "News: japan 's nec corp. and UNK computer corp. of the united states said wednesday they had agreed to join forces in supercomputer sales .\n",
      "Headline:\n"
     ]
    }
   ],
   "source": [
    "def sum_prompt(document):\n",
    "    \"\"\"\n",
    "    Summarize the given `document` into a concise headline using a few-shot prompt.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a headline generation assistant. Given a news article, produce a concise and informative headline.\\n\\n\"\n",
    "\n",
    "        \"Here are some examples:\\n\"\n",
    "\n",
    "        \"News: Scientists have discovered a new exoplanet that appears to have water on its surface, raising hopes it may be habitable.\\n\"\n",
    "        \"Headline: New exoplanet may support life\\n\\n\"\n",
    "\n",
    "        \"News: The stock market experienced a significant downturn today, with major indices falling sharply amid economic uncertainty.\\n\"\n",
    "        \"Headline: Stock market plunges amid economic fears\\n\\n\"\n",
    "\n",
    "        f\"News: {document}\\n\"\n",
    "        \"Headline:\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "print(sum_prompt(dataset[0][\"document\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 12:38:57 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 12:38:58.372471: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744720738.397761   45353 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744720738.405120   45353 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744720738.424911   45353 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744720738.424933   45353 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744720738.424936   45353 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744720738.424938   45353 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-15 12:38:58.431771: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-15 12:39:18 [config.py:689] This model supports multiple tasks: {'score', 'reward', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 04-15 12:39:18 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 04-15 12:39:20 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct', speculative_config=None, tokenizer='/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 04-15 12:39:22 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7edb52918190>\n",
      "INFO 04-15 12:39:23 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-15 12:39:23 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 04-15 12:39:23 [gpu_model_runner.py:1276] Starting to load model /home/ubuntu/fast_llm_inference/llama-3.1-8B-Instruct...\n",
      "ERROR 04-15 12:39:24 [core.py:387] EngineCore hit an exception: Traceback (most recent call last):\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 378, in run_engine_core\n",
      "ERROR 04-15 12:39:24 [core.py:387]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 04-15 12:39:24 [core.py:387]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 320, in __init__\n",
      "ERROR 04-15 12:39:24 [core.py:387]     super().__init__(vllm_config, executor_class, log_stats)\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 67, in __init__\n",
      "ERROR 04-15 12:39:24 [core.py:387]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 04-15 12:39:24 [core.py:387]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "ERROR 04-15 12:39:24 [core.py:387]     self._init_executor()\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "ERROR 04-15 12:39:24 [core.py:387]     self.collective_rpc(\"load_model\")\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 04-15 12:39:24 [core.py:387]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 04-15 12:39:24 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/utils.py\", line 2378, in run_method\n",
      "ERROR 04-15 12:39:24 [core.py:387]     return func(*args, **kwargs)\n",
      "ERROR 04-15 12:39:24 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 136, in load_model\n",
      "ERROR 04-15 12:39:24 [core.py:387]     self.model_runner.load_model()\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1279, in load_model\n",
      "ERROR 04-15 12:39:24 [core.py:387]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "ERROR 04-15 12:39:24 [core.py:387]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
      "ERROR 04-15 12:39:24 [core.py:387]     return loader.load_model(vllm_config=vllm_config)\n",
      "ERROR 04-15 12:39:24 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 452, in load_model\n",
      "ERROR 04-15 12:39:24 [core.py:387]     model = _initialize_model(vllm_config=vllm_config)\n",
      "ERROR 04-15 12:39:24 [core.py:387]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\n",
      "ERROR 04-15 12:39:24 [core.py:387]     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "ERROR 04-15 12:39:24 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 486, in __init__\n",
      "ERROR 04-15 12:39:24 [core.py:387]     self.model = self._init_model(vllm_config=vllm_config,\n",
      "ERROR 04-15 12:39:24 [core.py:387]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 527, in _init_model\n",
      "ERROR 04-15 12:39:24 [core.py:387]     return LlamaModel(vllm_config=vllm_config,\n",
      "ERROR 04-15 12:39:24 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "ERROR 04-15 12:39:24 [core.py:387]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 313, in __init__\n",
      "ERROR 04-15 12:39:24 [core.py:387]     self.embed_tokens = VocabParallelEmbedding(\n",
      "ERROR 04-15 12:39:24 [core.py:387]                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 263, in __init__\n",
      "ERROR 04-15 12:39:24 [core.py:387]     self.quant_method.create_weights(self,\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 31, in create_weights\n",
      "ERROR 04-15 12:39:24 [core.py:387]     weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "ERROR 04-15 12:39:24 [core.py:387]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-15 12:39:24 [core.py:387]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\n",
      "ERROR 04-15 12:39:24 [core.py:387]     return func(*args, **kwargs)\n",
      "ERROR 04-15 12:39:24 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-15 12:39:24 [core.py:387] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 22.05 GiB of which 142.81 MiB is free. Process 40091 has 20.38 GiB memory in use. Including non-PyTorch memory, this process has 280.00 MiB memory in use. Of the allocated memory 64.60 MiB is allocated by PyTorch, and 1.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "ERROR 04-15 12:39:24 [core.py:387] \n",
      "CRITICAL 04-15 12:39:24 [core_client.py:359] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import os\n",
    "\n",
    "model_name = \"llama-3.1-8B-Instruct\"\n",
    "\n",
    "model_path = f\"/home/ubuntu/fast_llm_inference/{model_name}\"\n",
    "\n",
    "benchmark = ModelBenchmark(\n",
    "    backend=\"vllm\",\n",
    "    task=\"summarization\",\n",
    "    model_path=model_path,\n",
    "    llama_gpu_layers=-1,\n",
    "    max_tokens=70,\n",
    "    model_size= os.path.getsize(model_path) / 1e6, # in MB\n",
    ")\n",
    "\n",
    "results = benchmark.benchmark(prompts=[sum_prompt(i) for i in dataset[\"document\"]], \n",
    "                              references=[i for i in dataset[\"summary\"]])\n",
    "\n",
    "empty_GPU_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>ATL</th>\n",
       "      <th>GL</th>\n",
       "      <th>TPS</th>\n",
       "      <th>SPS</th>\n",
       "      <th>Memory Usage (MB)</th>\n",
       "      <th>Model Size (MB)</th>\n",
       "      <th>Total Energy (Wh)</th>\n",
       "      <th>Energy per Token (J/token)</th>\n",
       "      <th>Energy per Sentence (J/sentence)</th>\n",
       "      <th>Energy per Second (W)</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>BERTScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>629</td>\n",
       "      <td>You are a headline generation assistant. Given...</td>\n",
       "      <td>Japan and US firms team up on supercomputer s...</td>\n",
       "      <td>nec corp. and UNK tie-up in supercomputer sales</td>\n",
       "      <td>0.0803</td>\n",
       "      <td>0.7224</td>\n",
       "      <td>12.46</td>\n",
       "      <td>1.38</td>\n",
       "      <td>21342.06</td>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.009122</td>\n",
       "      <td>3.64868</td>\n",
       "      <td>32.838123</td>\n",
       "      <td>45.46</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.926682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_length                                             prompt  \\\n",
       "0            629  You are a headline generation assistant. Given...   \n",
       "\n",
       "                                    generated_answer  \\\n",
       "0   Japan and US firms team up on supercomputer s...   \n",
       "\n",
       "                                  reference_answer     ATL      GL    TPS  \\\n",
       "0  nec corp. and UNK tie-up in supercomputer sales  0.0803  0.7224  12.46   \n",
       "\n",
       "    SPS  Memory Usage (MB)  Model Size (MB)  Total Energy (Wh)  \\\n",
       "0  1.38           21342.06         0.004096           0.009122   \n",
       "\n",
       "   Energy per Token (J/token)  Energy per Sentence (J/sentence)  \\\n",
       "0                     3.64868                         32.838123   \n",
       "\n",
       "   Energy per Second (W)   ROUGE-1   ROUGE-L  BERTScore  \n",
       "0                  45.46  0.444444  0.444444   0.926682  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
