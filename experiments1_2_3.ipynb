{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c8f55e",
   "metadata": {},
   "source": [
    "### RQ1 - Comparing different quantization levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef740d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-12 06:59:57,327] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "INFO 05-12 06:59:58 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-12 06:59:58 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_benchmark(backend, model_name, task, base_path, samples=500, verbose=False, batch_size=100):\n",
    "    print(f\"Running benchmark for {model_name} with {backend} on {task}\")\n",
    "    try:\n",
    "        bm = ModelBenchmark(\n",
    "            backend=backend,\n",
    "            model_name=model_name,\n",
    "            model_path=f\"{base_path}/models/{model_name}\",\n",
    "            base_path=base_path,\n",
    "            task=task,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        bm.run(samples=samples, batch_size=batch_size)\n",
    "        bm.close()\n",
    "        del bm\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"✅ Completed: {model_name} | {backend} | {task}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed: {model_name} | {backend} | {task} -- {e}\")\n",
    "        torch.cuda.empty_cache()  # ensure no memory leak on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac63b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/rag/fast_llm_inference/\"\n",
    "\n",
    "backends = [\"vllm\"] #, \"huggingface\",\"deepspeed_mii\", \"llama.cpp\"]\n",
    "models   = [\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.1-8B-Instruct-4bit\",\n",
    "    \"llama-3.1-8B-Instruct-8bit\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "    \"llama-3.2-3b-instruct-4bit\",\n",
    "    \"llama-3.2-1b-instruct-4bit\",\n",
    "    \"llama-3.2-3b-instruct-8bit\",\n",
    "    \"llama-3.2-1b-instruct-8bit\",\n",
    "   \n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-7B-Instruct-4bit\",\n",
    "    \"Qwen2.5-7B-Instruct-8bit\", # some weird error\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct-4bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-3B-Instruct-8bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-8bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-8bit\",\n",
    "\n",
    "\n",
    "    \"gemma-2-9b-it-bnb4\",\n",
    "    \"gemma-2-9b-it-8bit\",\n",
    "    \"gemma-2-9b-it\", # too large\n",
    "    \"gemma-2-2b-it-4bit\",\n",
    "    \"gemma-2-2b-it-8bit\",\n",
    "    \"gemma-2-2b-it\",\n",
    "]\n",
    "\n",
    "tasks    = [\"summarization\", \"qa\", \"sql\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb85145",
   "metadata": {},
   "source": [
    "first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for backend in backends:\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=100,\n",
    "                verbose=False,\n",
    "                batch_size=100,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c7713",
   "metadata": {},
   "source": [
    "check if anything is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a66e3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models with missing files:\n",
      "Qwen2.5-1.5B-Instruct-4bit\n",
      "Qwen2.5-3B-Instruct-4bit\n",
      "Qwen2.5-7B-Instruct-4bit\n",
      "Qwen2.5-7B-Instruct-8bit\n",
      "gemma-2-9b-it-4bit\n",
      "gemma-2-9b-it-8bit\n",
      "llama-3.1-8B-Instruct-4bit\n",
      "llama-3.2-3b-instruct-4bit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define your parameters\n",
    "backends = [\"vllm\"]\n",
    "models = [\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.1-8B-Instruct-4bit\",\n",
    "    \"llama-3.1-8B-Instruct-8bit\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "    \"llama-3.2-3b-instruct-4bit\",\n",
    "    \"llama-3.2-1b-instruct-4bit\",\n",
    "    \"llama-3.2-3b-instruct-8bit\",\n",
    "    \"llama-3.2-1b-instruct-8bit\",\n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-7B-Instruct-4bit\",\n",
    "    \"Qwen2.5-7B-Instruct-8bit\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct-4bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-3B-Instruct-8bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-8bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-8bit\",\n",
    "    \"gemma-2-9b-it-4bit\",\n",
    "    \"gemma-2-9b-it-8bit\",\n",
    "    \"gemma-2-9b-it\",\n",
    "    \"gemma-2-2b-it-4bit\",\n",
    "    \"gemma-2-2b-it-8bit\",\n",
    "    \"gemma-2-2b-it\",\n",
    "]\n",
    "tasks = [\"summarization\", \"qa\", \"sql\"]\n",
    "\n",
    "results_dir = \"./results/experiment_1/\"\n",
    "\n",
    "missing_models = set()\n",
    "\n",
    "for backend in backends:\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            filename = f\"{backend}_{model}_{task}.csv\"\n",
    "            filepath = os.path.join(results_dir, filename)\n",
    "            if not os.path.exists(filepath):\n",
    "                missing_models.add(model)\n",
    "                break  # No need to check more tasks if one is missing\n",
    "\n",
    "# Print models with missing files\n",
    "if missing_models:\n",
    "    print(\"Models with missing files:\")\n",
    "    for model in sorted(missing_models):\n",
    "        print(model)\n",
    "else:\n",
    "    print(\"✅ All models are complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e8200",
   "metadata": {},
   "source": [
    "try again with the missing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859b4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 07:04:31 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-12 07:04:34 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/home/rag/fast_llm_inference//models/Qwen2.5-7B-Instruct-8bit', speculative_config=None, tokenizer='/home/rag/fast_llm_inference//models/Qwen2.5-7B-Instruct-8bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/rag/fast_llm_inference//models/Qwen2.5-7B-Instruct-8bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "WARNING 05-12 07:04:34 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fa57513ff90>\n",
      "INFO 05-12 07:04:35 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-12 07:04:35 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-12 07:04:35 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-12 07:04:35 [gpu_model_runner.py:1329] Starting to load model /home/rag/fast_llm_inference//models/Qwen2.5-7B-Instruct-8bit...\n",
      "INFO 05-12 07:04:35 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.37it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.62it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.57it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.33it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.58it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.54it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 07:04:38 [gpu_model_runner.py:1347] Model loading took 8.1424 GiB and 2.707208 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rag/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 07:04:40 [kv_cache_utils.py:634] GPU KV cache size: 194,432 tokens\n",
      "INFO 05-12 07:04:40 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 5.93x\n",
      "INFO 05-12 07:04:40 [core.py:159] init engine (profile, create kv cache, warmup model) took 2.55 seconds\n",
      "INFO 05-12 07:04:40 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e6325f208143f091fe3f0fc2abf40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196dd6d7862145798857724498782750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578c36615112482a8caab67ce1eb632a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821e7483e3ea4f8f9cf6ea7853df29af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9663ad9b738649d0a1e90425b301ea6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2cb6832c15e42cf8aba47d9dde52994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1f194d25ee4dd785d22eab7c608e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61595326c7845e288641b3186513ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324fa37fa1864da581ec97a424262b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36b0685025746f0aa3e99accd93bc83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Qwen2.5-7B-Instruct-8bit on vllm/sql:\n",
      "prompt_length                        1201.928000 ± 296.321737\n",
      "TTFT                                      0.145320 ± 0.007999\n",
      "ATL                                       0.870983 ± 0.576903\n",
      "GL                                       10.709580 ± 0.371103\n",
      "TPS                                       1.601360 ± 0.840256\n",
      "SPS                                       0.314820 ± 0.277602\n",
      "Avg GPU Mem (MB)                    23475.730000 ± 142.954940\n",
      "Peak GPU Mem (MB)                    23511.160000 ± 92.892939\n",
      "Avg GPU Util (%)                         54.116000 ± 2.713727\n",
      "Peak GPU Util (%)                        99.000000 ± 0.000000\n",
      "Total Energy (Wh)                         0.296732 ± 0.004935\n",
      "Avg Power (W)                            99.840000 ± 3.102569\n",
      "Peak Power (W)                          177.940000 ± 2.034009\n",
      "Energy per Token (J/token)              86.825554 ± 57.317674\n",
      "Energy per Sentence (J/sentence)      691.601725 ± 443.435410\n",
      "Memory Usage (MB)                    23511.160000 ± 92.892939\n",
      "Model Size (MB)                        8323.219277 ± 0.000000\n",
      "Overhead (MB)                        15187.940723 ± 92.892939\n",
      "AST_equal                                 0.218000 ± 0.413301\n",
      "Normalized_equal                          0.272000 ± 0.445436\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W512 07:05:49.017523971 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed: Qwen2.5-7B-Instruct-8bit | vllm | sql\n",
      "Running benchmark for llama-3.2-3b-instruct-4bit with vllm on summarization\n",
      "INFO 05-12 07:05:57 [config.py:717] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 05-12 07:05:57 [config.py:830] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-12 07:05:57 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-12 07:06:01 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-12 07:06:03 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/home/rag/fast_llm_inference//models/llama-3.2-3b-instruct-4bit', speculative_config=None, tokenizer='/home/rag/fast_llm_inference//models/llama-3.2-3b-instruct-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/rag/fast_llm_inference//models/llama-3.2-3b-instruct-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-12 07:06:03 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f47e6145650>\n",
      "INFO 05-12 07:06:04 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-12 07:06:04 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-12 07:06:04 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-12 07:06:04 [gpu_model_runner.py:1329] Starting to load model /home/rag/fast_llm_inference//models/llama-3.2-3b-instruct-4bit...\n",
      "INFO 05-12 07:06:04 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.45it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.45it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.20it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.20it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 07:06:05 [gpu_model_runner.py:1347] Model loading took 2.1269 GiB and 1.199941 seconds\n",
      "INFO 05-12 07:06:13 [backends.py:420] Using cache directory: /home/rag/.cache/vllm/torch_compile_cache/802b26bcda/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-12 07:06:13 [backends.py:430] Dynamo bytecode transform time: 7.96 s\n",
      "INFO 05-12 07:06:19 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 5.730 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-10 (_metrics_monitor):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/rag/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rag/fast_llm_inference/benchmark/benchmark.py\", line 80, in _metrics_monitor\n",
      "    readings[\"power\"].append(self._get_gpu_power_usage())\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rag/fast_llm_inference/benchmark/benchmark.py\", line 62, in _get_gpu_power_usage\n",
      "    power_mw = nvmlDeviceGetPowerUsage(self.handle)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/rag/.venv/lib/python3.11/site-packages/pynvml.py\", line 3486, in nvmlDeviceGetPowerUsage\n",
      "    _nvmlCheckReturn(ret)\n",
      "  File \"/home/rag/.venv/lib/python3.11/site-packages/pynvml.py\", line 1059, in _nvmlCheckReturn\n",
      "    raise NVMLError(ret)\n",
      "pynvml.NVMLError_NotSupported: Not Supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 07:06:25 [monitor.py:33] torch.compile takes 7.96 s in total\n",
      "INFO 05-12 07:06:25 [kv_cache_utils.py:634] GPU KV cache size: 164,096 tokens\n",
      "INFO 05-12 07:06:25 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 1.25x\n",
      "INFO 05-12 07:06:53 [gpu_model_runner.py:1686] Graph capturing finished in 28 secs, took 1.21 GiB\n",
      "INFO 05-12 07:06:53 [core.py:159] init engine (profile, create kv cache, warmup model) took 47.89 seconds\n",
      "INFO 05-12 07:06:53 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06b02ed05be414d82e303aca85cefe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5541924170542039ecc29c5b4f9a902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-12 07:07:22] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:22] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:22] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:22] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:22] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:22] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:23] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:23] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:23] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:23] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:23] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:23] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:23] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:23] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:24] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:24] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:24] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:24] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:24] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:24] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:24] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:24] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:25] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:25] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:25] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:25] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:25] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:25] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:25] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:26] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:26] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:26] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:26] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:26] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:26] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:26] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:27] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:27] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:27] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:27] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:27] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:27] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:27] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:28] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:28] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:28] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:28] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:28] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:28] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:28] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:28] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:29] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:29] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:29] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:29] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:29] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:29] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:29] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:32] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:32] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:32] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:32] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:32] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:32] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:32] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:07:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:03] INFO rouge_scorer.py:83: Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66d3389f201450c90c8c71ab84cb875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b88f254c1eb4fa09eb7c9603a3f56f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-12 07:08:27] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:28] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:28] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:28] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:28] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:28] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:28] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:28] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:28] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:29] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:29] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:29] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:29] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:29] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:29] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:29] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:29] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:32] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:32] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:32] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:32] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:32] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:32] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:32] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:08:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:08] INFO rouge_scorer.py:83: Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3aa2f0974a40a38820336cdc13c3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e393c5b95c23401b8ce8f8c4c7047549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-12 07:09:32] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:33] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:34] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:35] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:36] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:37] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:09:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:13] INFO rouge_scorer.py:83: Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a1010d392d43a79ea14e7733200b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a61f11997024135938e21eb73950506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-12 07:10:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:38] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:10:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:17] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:17] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:17] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:17] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:17] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:17] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:17] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:18] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:18] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:18] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:18] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:18] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:18] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:18] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:19] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:19] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:19] INFO rouge_scorer.py:83: Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84551d139f724f32a1d2afaede22fef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed34b353f0554b4da5ab5ff808e5704f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-12 07:11:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:43] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:44] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:45] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:46] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:47] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:48] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:49] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:53] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:54] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:55] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:11:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:00] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:01] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:04] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:05] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:06] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:07] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:08] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:09] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:10] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:11] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:12] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:13] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:17] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:17] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:17] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:17] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:17] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:17] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:17] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:18] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:18] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:18] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:18] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:18] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:18] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:19] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:19] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:19] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:19] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:19] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:19] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:19] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:19] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:20] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:20] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:20] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:20] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:20] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:20] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:20] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:20] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:21] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:21] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:21] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:21] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:21] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:21] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:21] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:22] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:22] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:22] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:22] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:22] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:22] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:22] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:23] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:23] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:23] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:23] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:23] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:23] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:23] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:24] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:24] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2025-05-12 07:12:24] INFO rouge_scorer.py:83: Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for llama-3.2-3b-instruct-4bit on vllm/summarization:\n",
      "prompt_length                       5387.808000 ± 1970.082684\n",
      "TTFT                                      0.069840 ± 0.000734\n",
      "ATL                                       0.464657 ± 0.103959\n",
      "GL                                       24.034520 ± 0.334911\n",
      "TPS                                       2.266560 ± 0.595230\n",
      "SPS                                       0.159560 ± 0.029590\n",
      "Avg GPU Mem (MB)                      23435.446000 ± 0.228228\n",
      "Peak GPU Mem (MB)                     23435.560000 ± 0.000000\n",
      "Avg GPU Util (%)                         95.626000 ± 0.204518\n",
      "Peak GPU Util (%)                       100.000000 ± 0.000000\n",
      "Total Energy (Wh)                         1.048984 ± 0.018107\n",
      "Avg Power (W)                           157.116000 ± 0.951270\n",
      "Peak Power (W)                          185.734000 ± 6.375286\n",
      "Energy per Token (J/token)              73.018018 ± 16.397062\n",
      "Energy per Sentence (J/sentence)     1020.849448 ± 185.338261\n",
      "Memory Usage (MB)                     23435.560000 ± 0.000000\n",
      "Model Size (MB)                        2155.331536 ± 0.000000\n",
      "Overhead (MB)                         21280.228464 ± 0.000000\n",
      "ROUGE-1                                   0.386498 ± 0.094862\n",
      "ROUGE-2                                   0.139846 ± 0.082484\n",
      "ROUGE-L                                   0.246692 ± 0.080725\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W512 07:12:25.618541924 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed: llama-3.2-3b-instruct-4bit | vllm | summarization\n",
      "Running benchmark for llama-3.2-3b-instruct-4bit with vllm on qa\n",
      "INFO 05-12 07:12:26 [config.py:717] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 05-12 07:12:26 [config.py:830] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-12 07:12:26 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-12 07:12:30 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-12 07:12:32 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/home/rag/fast_llm_inference//models/llama-3.2-3b-instruct-4bit', speculative_config=None, tokenizer='/home/rag/fast_llm_inference//models/llama-3.2-3b-instruct-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/rag/fast_llm_inference//models/llama-3.2-3b-instruct-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-12 07:12:32 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f9bb36b5f90>\n",
      "INFO 05-12 07:12:33 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-12 07:12:33 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-12 07:12:33 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-12 07:12:33 [gpu_model_runner.py:1329] Starting to load model /home/rag/fast_llm_inference//models/llama-3.2-3b-instruct-4bit...\n",
      "INFO 05-12 07:12:33 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.54it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.54it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 07:12:35 [gpu_model_runner.py:1347] Model loading took 2.1269 GiB and 1.238727 seconds\n",
      "INFO 05-12 07:12:43 [backends.py:420] Using cache directory: /home/rag/.cache/vllm/torch_compile_cache/802b26bcda/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-12 07:12:43 [backends.py:430] Dynamo bytecode transform time: 8.73 s\n",
      "INFO 05-12 07:12:49 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 5.589 s\n",
      "INFO 05-12 07:12:54 [monitor.py:33] torch.compile takes 8.73 s in total\n",
      "INFO 05-12 07:12:54 [kv_cache_utils.py:634] GPU KV cache size: 164,096 tokens\n",
      "INFO 05-12 07:12:54 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 1.25x\n",
      "INFO 05-12 07:13:22 [gpu_model_runner.py:1686] Graph capturing finished in 28 secs, took 1.21 GiB\n",
      "INFO 05-12 07:13:22 [core.py:159] init engine (profile, create kv cache, warmup model) took 47.68 seconds\n",
      "INFO 05-12 07:13:22 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc1a6648f514fac8f91f32208ac1933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20eabf7840c44ac493932ee85b2754af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1985f5e28db04e13bcee5c8d00c87a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd3fcfc164b4956b8502c3b0172c2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daeb4a2608114e69860cd6405fa28b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9075b8c87ef84c79b54e2d2b7ffbd0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6dcd51882b941c788b8965ceabafd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37484fa89c264203920b4d6ba3b56c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80f1209fe1749d99f5e7f6df5cba535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b67e99098cd4f62924f7e78b609f208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for llama-3.2-3b-instruct-4bit on vllm/qa:\n",
      "prompt_length                       1376.792000 ± 308.648558\n",
      "TTFT                                     0.078380 ± 0.013088\n",
      "ATL                                      1.898850 ± 1.032705\n",
      "GL                                       3.060620 ± 0.400142\n",
      "TPS                                      0.861760 ± 0.980049\n",
      "SPS                                      0.505300 ± 0.188440\n",
      "Avg GPU Mem (MB)                     23435.270000 ± 0.580581\n",
      "Peak GPU Mem (MB)                    23435.560000 ± 0.000000\n",
      "Avg GPU Util (%)                        86.552000 ± 4.107886\n",
      "Peak GPU Util (%)                      100.000000 ± 0.000000\n",
      "Total Energy (Wh)                        0.126989 ± 0.017909\n",
      "Avg Power (W)                          149.112000 ± 3.313072\n",
      "Peak Power (W)                        183.004000 ± 12.252346\n",
      "Energy per Token (J/token)           283.686501 ± 155.286853\n",
      "Energy per Sentence (J/sentence)     338.252621 ± 123.123354\n",
      "Memory Usage (MB)                    23435.560000 ± 0.000000\n",
      "Model Size (MB)                       2155.331536 ± 0.000000\n",
      "Overhead (MB)                        21280.228464 ± 0.000000\n",
      "exact_match                              0.664000 ± 0.472812\n",
      "F1_score                                 0.791636 ± 0.342510\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W512 07:13:43.201558132 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed: llama-3.2-3b-instruct-4bit | vllm | qa\n",
      "Running benchmark for llama-3.2-3b-instruct-4bit with vllm on sql\n",
      "INFO 05-12 07:13:44 [config.py:717] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 05-12 07:13:44 [config.py:830] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-12 07:13:44 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-12 07:13:48 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-12 07:13:51 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/home/rag/fast_llm_inference//models/llama-3.2-3b-instruct-4bit', speculative_config=None, tokenizer='/home/rag/fast_llm_inference//models/llama-3.2-3b-instruct-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/rag/fast_llm_inference//models/llama-3.2-3b-instruct-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-12 07:13:51 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f5b101bd190>\n",
      "INFO 05-12 07:13:51 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-12 07:13:51 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-12 07:13:51 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-12 07:13:51 [gpu_model_runner.py:1329] Starting to load model /home/rag/fast_llm_inference//models/llama-3.2-3b-instruct-4bit...\n",
      "INFO 05-12 07:13:52 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.53it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.53it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.19it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.19it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 07:13:53 [gpu_model_runner.py:1347] Model loading took 2.1269 GiB and 1.300287 seconds\n",
      "INFO 05-12 07:14:00 [backends.py:420] Using cache directory: /home/rag/.cache/vllm/torch_compile_cache/802b26bcda/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-12 07:14:00 [backends.py:430] Dynamo bytecode transform time: 7.60 s\n",
      "INFO 05-12 07:14:08 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 6.753 s\n",
      "INFO 05-12 07:14:13 [monitor.py:33] torch.compile takes 7.60 s in total\n",
      "INFO 05-12 07:14:14 [kv_cache_utils.py:634] GPU KV cache size: 164,096 tokens\n",
      "INFO 05-12 07:14:14 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 1.25x\n",
      "INFO 05-12 07:14:42 [gpu_model_runner.py:1686] Graph capturing finished in 29 secs, took 1.21 GiB\n",
      "INFO 05-12 07:14:42 [core.py:159] init engine (profile, create kv cache, warmup model) took 49.57 seconds\n",
      "INFO 05-12 07:14:42 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9141ba12929c44e6b74dd8b6e250f370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614152d6845f459980b96e829999bd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4094e7606ed412a9b3379dafc65bbb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1d3f74030f478f85083ec81861cb61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d407702effaa48639390eaef227aca40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef13e0ca4984f4294201d4673e85e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff2af32b40c451992979a910855ad85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52334e0fd8a24204afb7653670408ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4e979dd3204ee985b44fe0e6452675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7a340d5b414c7b9974957e17627a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for llama-3.2-3b-instruct-4bit on vllm/sql:\n",
      "prompt_length                       1201.928000 ± 296.321737\n",
      "TTFT                                     0.070320 ± 0.001595\n",
      "ATL                                      0.518158 ± 0.766214\n",
      "GL                                       5.527560 ± 0.037272\n",
      "TPS                                      3.326220 ± 1.812227\n",
      "SPS                                      0.636620 ± 0.550298\n",
      "Avg GPU Mem (MB)                     23433.550000 ± 0.020020\n",
      "Peak GPU Mem (MB)                    23433.560000 ± 0.000000\n",
      "Avg GPU Util (%)                        92.284000 ± 2.753002\n",
      "Peak GPU Util (%)                      100.000000 ± 0.000000\n",
      "Total Energy (Wh)                        0.242757 ± 0.001850\n",
      "Avg Power (W)                          158.110000 ± 1.824043\n",
      "Peak Power (W)                         176.438000 ± 6.182558\n",
      "Energy per Token (J/token)            81.987809 ± 121.745986\n",
      "Energy per Sentence (J/sentence)     552.859421 ± 367.235474\n",
      "Memory Usage (MB)                    23433.560000 ± 0.000000\n",
      "Model Size (MB)                       2155.331536 ± 0.000000\n",
      "Overhead (MB)                        21278.228464 ± 0.000000\n",
      "AST_equal                                0.166000 ± 0.372453\n",
      "Normalized_equal                         0.204000 ± 0.403373\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W512 07:15:22.988529789 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed: llama-3.2-3b-instruct-4bit | vllm | sql\n",
      "Running benchmark for Qwen2.5-1.5B-Instruct-4bit with vllm on summarization\n",
      "INFO 05-12 07:15:23 [config.py:717] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 05-12 07:15:23 [config.py:830] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-12 07:15:23 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-12 07:15:27 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-12 07:15:29 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/home/rag/fast_llm_inference//models/Qwen2.5-1.5B-Instruct-4bit', speculative_config=None, tokenizer='/home/rag/fast_llm_inference//models/Qwen2.5-1.5B-Instruct-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/rag/fast_llm_inference//models/Qwen2.5-1.5B-Instruct-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-12 07:15:29 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f266d2fbad0>\n",
      "INFO 05-12 07:15:30 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-12 07:15:30 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-12 07:15:30 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-12 07:15:30 [gpu_model_runner.py:1329] Starting to load model /home/rag/fast_llm_inference//models/Qwen2.5-1.5B-Instruct-4bit...\n",
      "INFO 05-12 07:15:30 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.56s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.56s/it]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.46it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.46it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 07:15:38 [gpu_model_runner.py:1347] Model loading took 1.0961 GiB and 7.170091 seconds\n",
      "INFO 05-12 07:15:47 [backends.py:420] Using cache directory: /home/rag/.cache/vllm/torch_compile_cache/0830c47310/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-12 07:15:47 [backends.py:430] Dynamo bytecode transform time: 9.64 s\n",
      "INFO 05-12 07:15:51 [backends.py:136] Cache the graph of shape None for later use\n",
      "INFO 05-12 07:16:23 [backends.py:148] Compiling a graph for general shape takes 35.78 s\n",
      "INFO 05-12 07:16:39 [monitor.py:33] torch.compile takes 45.42 s in total\n",
      "INFO 05-12 07:16:40 [kv_cache_utils.py:634] GPU KV cache size: 689,904 tokens\n",
      "INFO 05-12 07:16:40 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 21.05x\n",
      "INFO 05-12 07:17:08 [gpu_model_runner.py:1686] Graph capturing finished in 28 secs, took 1.17 GiB\n",
      "INFO 05-12 07:17:08 [core.py:159] init engine (profile, create kv cache, warmup model) took 90.32 seconds\n",
      "INFO 05-12 07:17:08 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "for backend in backends:\n",
    "    for model in list(missing_models):\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=500,\n",
    "                verbose=False,\n",
    "                batch_size=100,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2610ec",
   "metadata": {},
   "source": [
    "### RQ2 - Comparing different inference engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130bae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 01:47:22.335784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746928042.686955  715944 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746928042.790989  715944 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746928043.633385  715944 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746928043.633515  715944 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746928043.633520  715944 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746928043.633524  715944 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-11 01:47:23.718528: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-11 01:47:36 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-11 01:47:36 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-05-11 01:47:43,255] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Running benchmark for qwen2.5-3b-instruct-fp16.gguf with llama.cpp on summarization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for qwen2.5-3b-instruct-fp16.gguf on llama.cpp/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.037240 ± 0.002196\n",
      "ATL                                       1.907586 ± 0.404739\n",
      "GL                                      103.282380 ± 6.380167\n",
      "TPS                                       0.548400 ± 0.119880\n",
      "SPS                                       0.034100 ± 0.007797\n",
      "Avg GPU Mem (MB)                       7188.030000 ± 5.935774\n",
      "Peak GPU Mem (MB)                      7189.480000 ± 6.407067\n",
      "Avg GPU Util (%)                         91.788000 ± 0.394477\n",
      "Peak GPU Util (%)                       100.000000 ± 0.000000\n",
      "Total Energy (Wh)                         2.051205 ± 0.128664\n",
      "Avg Power (W)                            71.492000 ± 0.212099\n",
      "Peak Power (W)                           74.938000 ± 0.328658\n",
      "Energy per Token (J/token)             136.380099 ± 28.955550\n",
      "Energy per Sentence (J/sentence)     2268.556692 ± 555.147749\n",
      "Memory Usage (MB)                      7189.480000 ± 6.407067\n",
      "Model Size (MB)                        5892.102478 ± 0.000000\n",
      "Overhead (MB)                          1297.377522 ± 6.407067\n",
      "ROUGE-1                                   0.383396 ± 0.095185\n",
      "ROUGE-2                                   0.135917 ± 0.075227\n",
      "ROUGE-L                                   0.242484 ± 0.078435\n",
      "dtype: object\n",
      "✅ Completed: qwen2.5-3b-instruct-fp16.gguf | llama.cpp | summarization\n",
      "Running benchmark for qwen2.5-3b-instruct-fp16.gguf with llama.cpp on qa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for qwen2.5-3b-instruct-fp16.gguf on llama.cpp/qa:\n",
      "prompt_length                       1393.740000 ± 302.397244\n",
      "TTFT                                     0.035200 ± 0.000360\n",
      "ATL                                    24.702549 ± 11.375563\n",
      "GL                                      35.124600 ± 0.881287\n",
      "TPS                                      0.063900 ± 0.071264\n",
      "SPS                                      0.031200 ± 0.005908\n",
      "Avg GPU Mem (MB)                      7167.946000 ± 2.506683\n",
      "Peak GPU Mem (MB)                     7168.680000 ± 2.412091\n",
      "Avg GPU Util (%)                        91.738000 ± 0.576138\n",
      "Peak GPU Util (%)                       95.000000 ± 0.000000\n",
      "Total Energy (Wh)                        0.698264 ± 0.018237\n",
      "Avg Power (W)                           71.566000 ± 0.394590\n",
      "Peak Power (W)                          74.466000 ± 0.552701\n",
      "Energy per Token (J/token)          1768.265073 ± 815.255899\n",
      "Energy per Sentence (J/sentence)    2463.341444 ± 255.088205\n",
      "Memory Usage (MB)                     7168.680000 ± 2.412091\n",
      "Model Size (MB)                       5892.102478 ± 0.000000\n",
      "Overhead (MB)                         1276.577522 ± 2.412091\n",
      "exact_match                              0.620000 ± 0.487832\n",
      "F1_score                                 0.760117 ± 0.358138\n",
      "dtype: object\n",
      "✅ Completed: qwen2.5-3b-instruct-fp16.gguf | llama.cpp | qa\n",
      "Running benchmark for qwen2.5-3b-instruct-fp16.gguf with llama.cpp on sql\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for qwen2.5-3b-instruct-fp16.gguf on llama.cpp/sql:\n",
      "prompt_length                        1163.740000 ± 264.891883\n",
      "TTFT                                      0.035960 ± 0.000972\n",
      "ATL                                       4.107402 ± 3.616989\n",
      "GL                                       56.953800 ± 5.185162\n",
      "TPS                                       0.400000 ± 0.247055\n",
      "SPS                                       0.096800 ± 0.069904\n",
      "Avg GPU Mem (MB)                       7165.032000 ± 1.042286\n",
      "Peak GPU Mem (MB)                      7165.080000 ± 0.984732\n",
      "Avg GPU Util (%)                         91.832000 ± 0.368968\n",
      "Peak GPU Util (%)                        95.000000 ± 0.000000\n",
      "Total Energy (Wh)                         1.135422 ± 0.104077\n",
      "Avg Power (W)                            71.766000 ± 0.182198\n",
      "Peak Power (W)                           73.934000 ± 0.462475\n",
      "Energy per Token (J/token)            294.793370 ± 259.616610\n",
      "Energy per Sentence (J/sentence)    1812.023837 ± 1647.471307\n",
      "Memory Usage (MB)                      7165.080000 ± 0.984732\n",
      "Model Size (MB)                        5892.102478 ± 0.000000\n",
      "Overhead (MB)                          1272.977522 ± 0.984732\n",
      "AST_equal                                 0.110000 ± 0.314466\n",
      "Normalized_equal                          0.150000 ± 0.358870\n",
      "dtype: object\n",
      "✅ Completed: qwen2.5-3b-instruct-fp16.gguf | llama.cpp | sql\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_benchmark(backend, model_name, task, base_path=\"/home/ubuntu/fast_llm_inference/models\", samples=500, verbose=False, batch_size=100):\n",
    "    print(f\"Running benchmark for {model_name} with {backend} on {task}\")\n",
    "    try:\n",
    "        bm = ModelBenchmark(\n",
    "            backend=backend,\n",
    "            model_name=model_name,\n",
    "            model_path=f\"{base_path}/{model_name}\",\n",
    "            task=task,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        bm.run(samples=samples, batch_size=batch_size)\n",
    "        bm.close()\n",
    "        del bm\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"✅ Completed: {model_name} | {backend} | {task}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed: {model_name} | {backend} | {task} -- {e}\")\n",
    "        torch.cuda.empty_cache()  # ensure no memory leak on error\n",
    "\n",
    "\n",
    "base_path = \"/home/ubuntu/fast_llm_inference/models\"\n",
    "\n",
    "backends = [\"huggingface\"] #\"llama.cpp\"] #\"vllm\" ,\"deepspeed_mii\", \"huggingface\"]\n",
    "\n",
    "models   = [\n",
    "    \"gemma-2-9b-it\", \n",
    "    \"gemma-2-2b-it\",\n",
    "\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "   \n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "]\n",
    "\n",
    "tasks    = [\"summarization\", \"qa\", \"sql\",]\n",
    "\n",
    "for backend in backends:\n",
    "    if backend == \"llama.cpp\":\n",
    "        models = [\n",
    "            \"gemma-2-2b-it-fp16.gguf\",\n",
    "            \"gemma-2-9b-it-fp16.gguf\",\n",
    "\n",
    "            \"llama-3.1-8B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "            \n",
    "            \"qwen2.5-0.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-1.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-3b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-7B-instruct-fp16.gguf\",\n",
    "        ]\n",
    "        \n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=100,\n",
    "                verbose=False,\n",
    "                batch_size=20,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
