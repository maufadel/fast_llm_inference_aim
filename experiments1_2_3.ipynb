{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c8f55e",
   "metadata": {},
   "source": [
    "### RQ1 - Comparing different quantization levels\n",
    "\n",
    "**Quantization Dimension**\n",
    "\n",
    "How does quantization in different models and architectures affect system and task-specific metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef740d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 15:48:54.090065: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747064934.110013  260255 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747064934.115811  260255 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747064934.135655  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135683  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135685  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135687  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-12 15:48:54.141568: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 15:48:57 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-12 15:48:57 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-05-12 15:49:00,664] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_benchmark(backend, model_name, task, base_path, samples=500, verbose=False, batch_size=100):\n",
    "    print(f\"Running benchmark for {model_name} with {backend} on {task}\")\n",
    "    try:\n",
    "        bm = ModelBenchmark(\n",
    "            backend=backend,\n",
    "            model_name=model_name,\n",
    "            model_path=f\"{base_path}/models/{model_name}\",\n",
    "            base_path=base_path,\n",
    "            task=task,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        bm.run(samples=samples, batch_size=batch_size)\n",
    "        bm.close()\n",
    "        del bm\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"✅ Completed: {model_name} | {backend} | {task}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed: {model_name} | {backend} | {task} -- {e}\")\n",
    "        torch.cuda.empty_cache()  # ensure no memory leak on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac63b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference/\"\n",
    "\n",
    "backends = [\"vllm\"] #, \"huggingface\",\"deepspeed_mii\", \"llama.cpp\"]\n",
    "models   = [\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.1-8B-Instruct-4bit\",\n",
    "    \"llama-3.1-8B-Instruct-8bit\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "    \"llama-3.2-3b-instruct-4bit\",\n",
    "    \"llama-3.2-1b-instruct-4bit\",\n",
    "    \"llama-3.2-3b-instruct-8bit\",\n",
    "    \"llama-3.2-1b-instruct-8bit\",\n",
    "   \n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-7B-Instruct-4bit\",\n",
    "    \"Qwen2.5-7B-Instruct-8bit\", # some weird error\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct-4bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-3B-Instruct-8bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-8bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-8bit\",\n",
    "\n",
    "\n",
    "    \"gemma-2-9b-it-bnb4\",\n",
    "    \"gemma-2-9b-it-8bit\",\n",
    "    \"gemma-2-9b-it\", # too large\n",
    "    \"gemma-2-2b-it-4bit\",\n",
    "    \"gemma-2-2b-it-8bit\",\n",
    "    \"gemma-2-2b-it\",\n",
    "]\n",
    "\n",
    "tasks    = [\"summarization\", \"qa\", \"sql\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb85145",
   "metadata": {},
   "source": [
    "first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for backend in backends:\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=100,\n",
    "                verbose=False,\n",
    "                batch_size=100,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c7713",
   "metadata": {},
   "source": [
    "check if anything is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a66e3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models with missing files:\n",
      "Qwen2.5-7B-Instruct-8bit\n",
      "gemma-2-9b-it\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define your parameters\n",
    "backends = [\"vllm\"]\n",
    "models = [\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.1-8B-Instruct-4bit\",\n",
    "    \"llama-3.1-8B-Instruct-8bit\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "    \"llama-3.2-3b-instruct-4bit\",\n",
    "    \"llama-3.2-1b-instruct-4bit\",\n",
    "    \"llama-3.2-3b-instruct-8bit\",\n",
    "    \"llama-3.2-1b-instruct-8bit\",\n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-7B-Instruct-4bit\",\n",
    "    \"Qwen2.5-7B-Instruct-8bit\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct-4bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-3B-Instruct-8bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-8bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-8bit\",\n",
    "    \"gemma-2-9b-it-4bit\",\n",
    "    \"gemma-2-9b-it-8bit\",\n",
    "    \"gemma-2-9b-it\",\n",
    "    \"gemma-2-2b-it-4bit\",\n",
    "    \"gemma-2-2b-it-8bit\",\n",
    "    \"gemma-2-2b-it\",\n",
    "]\n",
    "tasks = [\"summarization\", \"qa\", \"sql\"]\n",
    "\n",
    "results_dir = \"./results/experiment_1/\"\n",
    "\n",
    "missing_models = set()\n",
    "\n",
    "for backend in backends:\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            filename = f\"{backend}_{model}_{task}.csv\"\n",
    "            filepath = os.path.join(results_dir, filename)\n",
    "            if not os.path.exists(filepath):\n",
    "                missing_models.add(model)\n",
    "                break  # No need to check more tasks if one is missing\n",
    "\n",
    "# Print models with missing files\n",
    "if missing_models:\n",
    "    print(\"Models with missing files:\")\n",
    "    for model in sorted(missing_models):\n",
    "        print(model)\n",
    "else:\n",
    "    print(\"✅ All models are complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e8200",
   "metadata": {},
   "source": [
    "try again with the missing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for backend in backends:\n",
    "    for model in list(missing_models):\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=500,\n",
    "                verbose=False,\n",
    "                batch_size=100,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2610ec",
   "metadata": {},
   "source": [
    "### RQ2 - Comparing different inference engines\n",
    "\n",
    "**Framework Dimension** \n",
    "\n",
    "Which inference framework (Transformers, vLLM, DeepSpeed MII,172\n",
    "LMDeploy, llama.cpp) strikes the best balance between system resource usage (e.g., GPU173\n",
    "utilization, joules/token) and system performance (tokens/s)?174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference/models\"\n",
    "\n",
    "backends = [\"vllm\", \"huggingface\", \"llama.cpp\"] #,\"deepspeed_mii\", \"huggingface\"]\n",
    "\n",
    "models   = [\n",
    "    \"gemma-2-9b-it\", \n",
    "    \"gemma-2-2b-it\",\n",
    "\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "   \n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "]\n",
    "\n",
    "tasks    = [\"summarization\", \"qa\", \"sql\",]\n",
    "\n",
    "for backend in backends:\n",
    "    if backend == \"llama.cpp\":\n",
    "        models = [\n",
    "            \"gemma-2-2b-it-fp16.gguf\",\n",
    "            \"gemma-2-9b-it-fp16.gguf\",\n",
    "\n",
    "            \"llama-3.1-8B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "            \n",
    "            \"qwen2.5-0.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-1.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-3b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-7B-instruct-fp16.gguf\",\n",
    "        ]\n",
    "        \n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=100,\n",
    "                verbose=False,\n",
    "                batch_size=20,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f940b1",
   "metadata": {},
   "source": [
    "#### RQ 3 - Comparing different use cases\n",
    "\n",
    "**Scenario/Workload Dimension**\n",
    "\n",
    "How do locally deployed LLMs and inference backends\n",
    "perform and scale across the three dominant inference scenarios—single - stream (single user),\n",
    "batched offline processing, and multi- user server workloads? Do system metrics – throughput,\n",
    "GPU utilization, joules/token — evolve as the average number of queries per second varies\n",
    "over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d90de05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 03:42:40.554419: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747366960.579138 1160723 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747366960.586688 1160723 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747366960.607277 1160723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747366960.607300 1160723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747366960.607303 1160723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747366960.607305 1160723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-16 03:42:40.613899: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-16 03:42:48 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-16 03:42:49 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-05-16 03:42:55,873] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from benchmark.benchmark import ModelBenchmark\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_benchmark(\n",
    "    backend: str,\n",
    "    model_name: str,\n",
    "    task: str,\n",
    "    base_path: str,\n",
    "    scenario: str = \"batch\",            # \"single\", \"batch\", or \"server\"\n",
    "    run_time: float = None,             # only for server: total time in seconds\n",
    "    requests_per_sec: float = None,     # only for server: λ (req/s)\n",
    "    batch_size: int = 100,              # only for batch\n",
    "    max_batch_size: int = None,         # only for server: cap per-batch size\n",
    "    sample_interval: float = 0.1,       # telemetry interval (s)\n",
    "    export_path: Optional[str] = None,  # custom export path for server scenario\n",
    "    verbose: bool = False\n",
    "):\n",
    "    print(f\"Running benchmark for {model_name} with {backend} on {task} [{scenario}]\")\n",
    "    try:\n",
    "        bm = ModelBenchmark(\n",
    "            backend=backend,\n",
    "            model_name=model_name,\n",
    "            model_path=f\"{base_path}/models/{model_name}\",\n",
    "            base_path=base_path,\n",
    "            task=task,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        if scenario == \"server\":\n",
    "            assert run_time is not None,    \"Must set run_time in server mode\"\n",
    "            assert requests_per_sec is not None, \"Must set requests_per_sec in server mode\"\n",
    "            df = bm.run(\n",
    "                scenario=\"server\",\n",
    "                run_time=run_time,\n",
    "                requests_per_sec=requests_per_sec,\n",
    "                sample_interval=sample_interval,\n",
    "                max_batch_size=max_batch_size,\n",
    "                export_path=export_path\n",
    "            )\n",
    "\n",
    "        elif scenario == \"single\":\n",
    "            df = bm.run(\n",
    "                samples=100,        # samples ignored\n",
    "                batch_size=1,\n",
    "                scenario=\"single\",\n",
    "                sample_interval=sample_interval,\n",
    "                export_path=export_path\n",
    "            )\n",
    "\n",
    "        elif scenario == \"batch\":\n",
    "            df = bm.run(\n",
    "                samples=100,        # samples ignored\n",
    "                batch_size=batch_size,\n",
    "                scenario=\"batch\",\n",
    "                sample_interval=sample_interval,\n",
    "                export_path=export_path\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scenario: {scenario}\")\n",
    "\n",
    "        bm.close()\n",
    "        del bm\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"✅ Completed: {model_name} | {backend} | {task} | {scenario}\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed: {model_name} | {backend} | {task} | {scenario} -- {e}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb1843",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference\"\n",
    "backends = [\"vllm\", \"llama.cpp\", \"huggingface\"]\n",
    "tasks    = [\"sql\"]\n",
    "server_rps      = [1, 2, 4, 8]\n",
    "run_time        = 120.0     # seconds\n",
    "sample_interval = 0.05      # s\n",
    "max_batch_size  = 64        # cap per batch\n",
    "\n",
    "for backend in backends:\n",
    "    if backend == \"llama.cpp\":\n",
    "        models = [\n",
    "            \"gemma-2-2b-it-fp16.gguf\",\n",
    "            \"gemma-2-9b-it-fp16.gguf\",\n",
    "            \"llama-3.1-8B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "            \"qwen2.5-0.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-1.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-3b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-7B-instruct-fp16.gguf\",\n",
    "        ]\n",
    "    else:\n",
    "        models = [\n",
    "            \"gemma-2-2b-it\",\n",
    "            \"llama-3.1-8B-Instruct\",\n",
    "            \"llama-3.2-3b-instruct\",\n",
    "            \"llama-3.2-1b-instruct\",\n",
    "            \"Qwen2.5-7B-Instruct\",\n",
    "            \"Qwen2.5-3B-Instruct\",\n",
    "            \"Qwen2.5-1.5B-Instruct\",\n",
    "            \"Qwen2.5-0.5B-Instruct\",\n",
    "        ]\n",
    "        if backend != \"vllm\":\n",
    "            models.append(\"gemma-2-9b-it\") # too large for vllm\n",
    "\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            for rps in server_rps:\n",
    "                export_path = f\"{base_path}/results/{backend}_{model}_{task}_{rps}QPS_{int(run_time)}s_server.csv\"\n",
    "                print(f\"→ {backend} | {model} | {task} @ {rps} QPS for {run_time}s -> {export_path}\")\n",
    "                run_benchmark(\n",
    "                    backend=backend,\n",
    "                    model_name=model,\n",
    "                    task=task,\n",
    "                    base_path=base_path,\n",
    "                    scenario=\"server\",\n",
    "                    run_time=run_time,\n",
    "                    requests_per_sec=rps,\n",
    "                    sample_interval=sample_interval,\n",
    "                    max_batch_size=max_batch_size,\n",
    "                    export_path=export_path,\n",
    "                    verbose=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a94ed06",
   "metadata": {},
   "source": [
    "part 2 - batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3f16559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Qwen2.5-3B-Instruct on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.043336 ± 0.012165\n",
      "ATL                                       0.890493 ± 0.207921\n",
      "GL                                       44.109264 ± 5.849148\n",
      "TPS                                       1.199600 ± 0.367668\n",
      "SPS                                       0.076600 ± 0.026713\n",
      "Avg GPU Mem (MB)                      7177.901200 ± 44.251661\n",
      "Peak GPU Mem (MB)                     7251.640000 ± 69.536579\n",
      "Avg GPU Util (%)                         80.702400 ± 1.501442\n",
      "Peak GPU Util (%)                       100.000000 ± 0.000000\n",
      "Total Energy (Wh)                         0.865047 ± 0.115001\n",
      "Avg Power (W)                            70.602000 ± 0.336614\n",
      "Peak Power (W)                           74.170000 ± 0.509284\n",
      "Energy per Token (J/token)              62.871192 ± 14.701504\n",
      "Energy per Sentence (J/sentence)     1014.099423 ± 256.092187\n",
      "Memory Usage (MB)                    6910.560000 ± 134.085822\n",
      "Model Size (MB)                        5896.998499 ± 0.000000\n",
      "Overhead (MB)                        1013.561501 ± 134.085822\n",
      "ROUGE-1                                   0.362262 ± 0.090907\n",
      "ROUGE-2                                   0.120386 ± 0.070228\n",
      "ROUGE-L                                   0.233875 ± 0.074872\n",
      "dtype: object\n",
      "✅ Completed: Qwen2.5-3B-Instruct | huggingface | summarization | batch\n",
      "→ huggingface | Qwen2.5-3B-Instruct | summarization @ batch=16 -> /home/ubuntu/fast_llm_inference/results/huggingface_Qwen2.5-3B-Instruct_summarization_16batch.csv\n",
      "Running benchmark for Qwen2.5-3B-Instruct with huggingface on summarization [batch]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd47972ac9274be5a230d02a2edede2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Qwen2.5-3B-Instruct on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.041168 ± 0.003897\n",
      "ATL                                       1.775063 ± 0.438429\n",
      "GL                                      87.953128 ± 14.472356\n",
      "TPS                                       0.643700 ± 0.403494\n",
      "SPS                                       0.038900 ± 0.027703\n",
      "Avg GPU Mem (MB)                      7255.056400 ± 51.561808\n",
      "Peak GPU Mem (MB)                     7358.040000 ± 95.962821\n",
      "Avg GPU Util (%)                         80.292400 ± 1.618048\n",
      "Peak GPU Util (%)                       100.000000 ± 0.000000\n",
      "Total Energy (Wh)                         1.724037 ± 0.284837\n",
      "Avg Power (W)                            70.550000 ± 0.602379\n",
      "Peak Power (W)                           74.577600 ± 0.521778\n",
      "Energy per Token (J/token)             125.276888 ± 31.096206\n",
      "Energy per Sentence (J/sentence)     2018.691576 ± 529.264784\n",
      "Memory Usage (MB)                    6893.580000 ± 126.417418\n",
      "Model Size (MB)                        5896.998499 ± 0.000000\n",
      "Overhead (MB)                         996.581501 ± 126.417418\n",
      "ROUGE-1                                   0.362262 ± 0.090907\n",
      "ROUGE-2                                   0.120386 ± 0.070228\n",
      "ROUGE-L                                   0.233875 ± 0.074872\n",
      "dtype: object\n",
      "✅ Completed: Qwen2.5-3B-Instruct | huggingface | summarization | batch\n",
      "→ huggingface | Qwen2.5-3B-Instruct | summarization @ batch=32 -> /home/ubuntu/fast_llm_inference/results/huggingface_Qwen2.5-3B-Instruct_summarization_32batch.csv\n",
      "Running benchmark for Qwen2.5-3B-Instruct with huggingface on summarization [batch]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72df7e2b0ad49df838b8aea70c0eae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Qwen2.5-3B-Instruct on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.044432 ± 0.004252\n",
      "ATL                                       3.636051 ± 0.973919\n",
      "GL                                     179.934928 ± 34.050334\n",
      "TPS                                       0.366300 ± 0.453330\n",
      "SPS                                       0.025100 ± 0.028902\n",
      "Avg GPU Mem (MB)                      7306.067200 ± 79.515569\n",
      "Peak GPU Mem (MB)                    7413.080000 ± 104.449688\n",
      "Avg GPU Util (%)                         78.290000 ± 2.641340\n",
      "Peak GPU Util (%)                       100.000000 ± 0.000000\n",
      "Total Energy (Wh)                         3.531741 ± 0.662888\n",
      "Avg Power (W)                            70.664400 ± 0.529792\n",
      "Peak Power (W)                           74.552400 ± 0.224432\n",
      "Energy per Token (J/token)             256.913694 ± 68.432126\n",
      "Energy per Sentence (J/sentence)    4142.222212 ± 1187.406503\n",
      "Memory Usage (MB)                    6880.160000 ± 103.050059\n",
      "Model Size (MB)                        5896.998499 ± 0.000000\n",
      "Overhead (MB)                         983.161501 ± 103.050059\n",
      "ROUGE-1                                   0.362262 ± 0.090907\n",
      "ROUGE-2                                   0.120386 ± 0.070228\n",
      "ROUGE-L                                   0.233875 ± 0.074872\n",
      "dtype: object\n",
      "✅ Completed: Qwen2.5-3B-Instruct | huggingface | summarization | batch\n",
      "→ huggingface | Qwen2.5-3B-Instruct | summarization @ batch=64 -> /home/ubuntu/fast_llm_inference/results/huggingface_Qwen2.5-3B-Instruct_summarization_64batch.csv\n",
      "Running benchmark for Qwen2.5-3B-Instruct with huggingface on summarization [batch]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344d11cc1fa4478389c93d8918aac678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Qwen2.5-3B-Instruct on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.037976 ± 0.000434\n",
      "ATL                                       6.283387 ± 2.243199\n",
      "GL                                     307.553112 ± 82.721832\n",
      "TPS                                       0.184500 ± 0.079039\n",
      "SPS                                       0.014000 ± 0.005505\n",
      "Avg GPU Mem (MB)                      7369.782800 ± 24.955491\n",
      "Peak GPU Mem (MB)                     7609.160000 ± 97.448467\n",
      "Avg GPU Util (%)                         80.193600 ± 3.864169\n",
      "Peak GPU Util (%)                       100.000000 ± 0.000000\n",
      "Total Energy (Wh)                         6.061515 ± 1.602698\n",
      "Avg Power (W)                            71.063200 ± 0.419704\n",
      "Peak Power (W)                           75.269600 ± 0.173671\n",
      "Energy per Token (J/token)            445.727315 ± 157.439062\n",
      "Energy per Sentence (J/sentence)    7179.554992 ± 2702.742526\n",
      "Memory Usage (MB)                    6874.300000 ± 102.459174\n",
      "Model Size (MB)                        5896.998499 ± 0.000000\n",
      "Overhead (MB)                         977.301501 ± 102.459174\n",
      "ROUGE-1                                   0.362262 ± 0.090907\n",
      "ROUGE-2                                   0.120386 ± 0.070228\n",
      "ROUGE-L                                   0.233875 ± 0.074872\n",
      "dtype: object\n",
      "✅ Completed: Qwen2.5-3B-Instruct | huggingface | summarization | batch\n",
      "→ huggingface | Qwen2.5-1.5B-Instruct | summarization @ batch=1 -> /home/ubuntu/fast_llm_inference/results/huggingface_Qwen2.5-1.5B-Instruct_summarization_1batch.csv\n",
      "Running benchmark for Qwen2.5-1.5B-Instruct with huggingface on summarization [batch]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Qwen2.5-1.5B-Instruct on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.035559 ± 0.005939\n",
      "ATL                                       0.042970 ± 0.006811\n",
      "GL                                        3.392327 ± 1.127456\n",
      "TPS                                      23.714500 ± 2.927343\n",
      "SPS                                       1.537300 ± 0.314655\n",
      "Avg GPU Mem (MB)                      3991.456900 ± 46.438190\n",
      "Peak GPU Mem (MB)                     3996.080000 ± 46.549203\n",
      "Avg GPU Util (%)                         49.088300 ± 4.804947\n",
      "Peak GPU Util (%)                       65.730000 ± 14.013597\n",
      "Total Energy (Wh)                         0.055163 ± 0.019309\n",
      "Avg Power (W)                            58.254600 ± 2.794041\n",
      "Peak Power (W)                           63.401400 ± 2.292122\n",
      "Energy per Token (J/token)                2.496276 ± 0.363754\n",
      "Energy per Sentence (J/sentence)         39.614598 ± 9.095874\n",
      "Memory Usage (MB)                     3996.100000 ± 46.523499\n",
      "Model Size (MB)                        2955.394428 ± 0.000000\n",
      "Overhead (MB)                         1040.705572 ± 46.523499\n",
      "ROUGE-1                                   0.328468 ± 0.089410\n",
      "ROUGE-2                                   0.103792 ± 0.068062\n",
      "ROUGE-L                                   0.203113 ± 0.068235\n",
      "dtype: object\n",
      "✅ Completed: Qwen2.5-1.5B-Instruct | huggingface | summarization | batch\n",
      "→ huggingface | Qwen2.5-1.5B-Instruct | summarization @ batch=8 -> /home/ubuntu/fast_llm_inference/results/huggingface_Qwen2.5-1.5B-Instruct_summarization_8batch.csv\n",
      "Running benchmark for Qwen2.5-1.5B-Instruct with huggingface on summarization [batch]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Qwen2.5-1.5B-Instruct on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.032260 ± 0.008642\n",
      "ATL                                       0.329537 ± 0.111393\n",
      "GL                                       24.033352 ± 3.511791\n",
      "TPS                                       3.412000 ± 1.287177\n",
      "SPS                                       0.215300 ± 0.078707\n",
      "Avg GPU Mem (MB)                      4112.424800 ± 31.876998\n",
      "Peak GPU Mem (MB)                     4170.680000 ± 55.320553\n",
      "Avg GPU Util (%)                         58.124800 ± 2.463072\n",
      "Peak GPU Util (%)                        93.440000 ± 5.424077\n",
      "Total Energy (Wh)                         0.424253 ± 0.062221\n",
      "Avg Power (W)                            63.555200 ± 1.321664\n",
      "Peak Power (W)                           68.734400 ± 0.660383\n",
      "Energy per Token (J/token)               20.933554 ± 7.047389\n",
      "Energy per Sentence (J/sentence)       320.091692 ± 79.144415\n",
      "Memory Usage (MB)                    3894.720000 ± 108.075985\n",
      "Model Size (MB)                        2955.394428 ± 0.000000\n",
      "Overhead (MB)                         939.325572 ± 108.075985\n",
      "ROUGE-1                                   0.328468 ± 0.089410\n",
      "ROUGE-2                                   0.103792 ± 0.068062\n",
      "ROUGE-L                                   0.203113 ± 0.068235\n",
      "dtype: object\n",
      "✅ Completed: Qwen2.5-1.5B-Instruct | huggingface | summarization | batch\n",
      "→ huggingface | Qwen2.5-1.5B-Instruct | summarization @ batch=16 -> /home/ubuntu/fast_llm_inference/results/huggingface_Qwen2.5-1.5B-Instruct_summarization_16batch.csv\n",
      "Running benchmark for Qwen2.5-1.5B-Instruct with huggingface on summarization [batch]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Qwen2.5-1.5B-Instruct on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.029964 ± 0.000225\n",
      "ATL                                       0.650746 ± 0.231205\n",
      "GL                                       47.572868 ± 8.435814\n",
      "TPS                                       1.848900 ± 1.180956\n",
      "SPS                                       0.119000 ± 0.086264\n",
      "Avg GPU Mem (MB)                      4172.945200 ± 46.604553\n",
      "Peak GPU Mem (MB)                     4257.400000 ± 85.547610\n",
      "Avg GPU Util (%)                         58.374400 ± 1.893248\n",
      "Peak GPU Util (%)                        94.520000 ± 4.051387\n",
      "Total Energy (Wh)                         0.846878 ± 0.148791\n",
      "Avg Power (W)                            64.044800 ± 1.150470\n",
      "Peak Power (W)                           68.911200 ± 0.866109\n",
      "Energy per Token (J/token)              41.722605 ± 14.888631\n",
      "Energy per Sentence (J/sentence)      636.791454 ± 167.810346\n",
      "Memory Usage (MB)                    3880.960000 ± 102.284776\n",
      "Model Size (MB)                        2955.394428 ± 0.000000\n",
      "Overhead (MB)                         925.565572 ± 102.284776\n",
      "ROUGE-1                                   0.328468 ± 0.089410\n",
      "ROUGE-2                                   0.103792 ± 0.068062\n",
      "ROUGE-L                                   0.203113 ± 0.068235\n",
      "dtype: object\n",
      "✅ Completed: Qwen2.5-1.5B-Instruct | huggingface | summarization | batch\n",
      "→ huggingface | Qwen2.5-1.5B-Instruct | summarization @ batch=32 -> /home/ubuntu/fast_llm_inference/results/huggingface_Qwen2.5-1.5B-Instruct_summarization_32batch.csv\n",
      "Running benchmark for Qwen2.5-1.5B-Instruct with huggingface on summarization [batch]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Qwen2.5-1.5B-Instruct on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.030064 ± 0.000789\n",
      "ATL                                       1.264664 ± 0.468833\n",
      "GL                                      92.380072 ± 17.284702\n",
      "TPS                                       1.081000 ± 1.238386\n",
      "SPS                                       0.067800 ± 0.093610\n",
      "Avg GPU Mem (MB)                      4213.000400 ± 70.936104\n",
      "Peak GPU Mem (MB)                     4302.200000 ± 95.655408\n",
      "Avg GPU Util (%)                         59.971200 ± 0.589609\n",
      "Peak GPU Util (%)                        96.920000 ± 4.234729\n",
      "Total Energy (Wh)                         1.677576 ± 0.315001\n",
      "Avg Power (W)                            65.276400 ± 0.562353\n",
      "Peak Power (W)                           70.020400 ± 0.200776\n",
      "Energy per Token (J/token)              82.677885 ± 30.718723\n",
      "Energy per Sentence (J/sentence)     1262.447219 ± 356.811951\n",
      "Memory Usage (MB)                     3870.060000 ± 83.123418\n",
      "Model Size (MB)                        2955.394428 ± 0.000000\n",
      "Overhead (MB)                          914.665572 ± 83.123418\n",
      "ROUGE-1                                   0.328468 ± 0.089410\n",
      "ROUGE-2                                   0.103792 ± 0.068062\n",
      "ROUGE-L                                   0.203113 ± 0.068235\n",
      "dtype: object\n",
      "✅ Completed: Qwen2.5-1.5B-Instruct | huggingface | summarization | batch\n",
      "→ huggingface | Qwen2.5-1.5B-Instruct | summarization @ batch=64 -> /home/ubuntu/fast_llm_inference/results/huggingface_Qwen2.5-1.5B-Instruct_summarization_64batch.csv\n",
      "Running benchmark for Qwen2.5-1.5B-Instruct with huggingface on summarization [batch]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Qwen2.5-1.5B-Instruct on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.036652 ± 0.005162\n",
      "ATL                                       2.467923 ± 0.950971\n",
      "GL                                     179.710404 ± 45.317686\n",
      "TPS                                       0.475800 ± 0.212228\n",
      "SPS                                       0.028600 ± 0.012228\n",
      "Avg GPU Mem (MB)                      4266.044400 ± 18.471791\n",
      "Peak GPU Mem (MB)                     4459.560000 ± 78.151741\n",
      "Avg GPU Util (%)                         53.317200 ± 0.955188\n",
      "Peak GPU Util (%)                       100.000000 ± 0.000000\n",
      "Total Energy (Wh)                         3.112621 ± 0.808937\n",
      "Avg Power (W)                            62.202800 ± 0.612671\n",
      "Peak Power (W)                           68.163200 ± 0.786342\n",
      "Energy per Token (J/token)             153.835704 ± 60.030974\n",
      "Energy per Sentence (J/sentence)     2347.973179 ± 738.469805\n",
      "Memory Usage (MB)                     3865.540000 ± 82.835144\n",
      "Model Size (MB)                        2955.394428 ± 0.000000\n",
      "Overhead (MB)                          910.145572 ± 82.835144\n",
      "ROUGE-1                                   0.328468 ± 0.089410\n",
      "ROUGE-2                                   0.103792 ± 0.068062\n",
      "ROUGE-L                                   0.203113 ± 0.068235\n",
      "dtype: object\n",
      "✅ Completed: Qwen2.5-1.5B-Instruct | huggingface | summarization | batch\n",
      "→ huggingface | Qwen2.5-0.5B-Instruct | summarization @ batch=1 -> /home/ubuntu/fast_llm_inference/results/huggingface_Qwen2.5-0.5B-Instruct_summarization_1batch.csv\n",
      "Running benchmark for Qwen2.5-0.5B-Instruct with huggingface on summarization [batch]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Qwen2.5-0.5B-Instruct on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.026622 ± 0.001984\n",
      "ATL                                       0.067387 ± 0.029782\n",
      "GL                                        5.607441 ± 1.349760\n",
      "TPS                                      17.968700 ± 8.073792\n",
      "SPS                                       1.110000 ± 0.626945\n",
      "Avg GPU Mem (MB)                      1768.333700 ± 24.619551\n",
      "Peak GPU Mem (MB)                     1770.480000 ± 24.456104\n",
      "Avg GPU Util (%)                         29.925900 ± 1.980484\n",
      "Peak GPU Util (%)                        35.490000 ± 6.039424\n",
      "Total Energy (Wh)                         0.073994 ± 0.018091\n",
      "Avg Power (W)                            47.372500 ± 1.198639\n",
      "Peak Power (W)                           49.504900 ± 1.527811\n",
      "Energy per Token (J/token)                3.197635 ± 1.418702\n",
      "Energy per Sentence (J/sentence)        54.104209 ± 30.684095\n",
      "Memory Usage (MB)                     1770.500000 ± 24.446511\n",
      "Model Size (MB)                         953.279826 ± 0.000000\n",
      "Overhead (MB)                          817.220174 ± 24.446511\n",
      "ROUGE-1                                   0.328205 ± 0.099791\n",
      "ROUGE-2                                   0.107634 ± 0.075601\n",
      "ROUGE-L                                   0.207904 ± 0.077409\n",
      "dtype: object\n",
      "✅ Completed: Qwen2.5-0.5B-Instruct | huggingface | summarization | batch\n",
      "→ huggingface | Qwen2.5-0.5B-Instruct | summarization @ batch=8 -> /home/ubuntu/fast_llm_inference/results/huggingface_Qwen2.5-0.5B-Instruct_summarization_8batch.csv\n",
      "Running benchmark for Qwen2.5-0.5B-Instruct with huggingface on summarization [batch]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Qwen2.5-0.5B-Instruct on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.026556 ± 0.001170\n",
      "ATL                                       0.547821 ± 0.255998\n",
      "GL                                       43.732508 ± 5.382846\n",
      "TPS                                       2.208100 ± 0.998969\n",
      "SPS                                       0.131500 ± 0.058368\n",
      "Avg GPU Mem (MB)                      1813.926400 ± 25.423092\n",
      "Peak GPU Mem (MB)                     1834.120000 ± 27.445122\n",
      "Avg GPU Util (%)                         31.418800 ± 0.743512\n",
      "Peak GPU Util (%)                       57.840000 ± 10.604668\n",
      "Total Energy (Wh)                         0.590118 ± 0.076151\n",
      "Avg Power (W)                            48.525200 ± 0.500538\n",
      "Peak Power (W)                           52.366400 ± 1.193951\n",
      "Energy per Token (J/token)              26.616395 ± 12.515467\n",
      "Energy per Sentence (J/sentence)      439.510849 ± 234.097730\n",
      "Memory Usage (MB)                     1727.880000 ± 42.743302\n",
      "Model Size (MB)                         953.279826 ± 0.000000\n",
      "Overhead (MB)                          774.600174 ± 42.743302\n",
      "ROUGE-1                                   0.328205 ± 0.099791\n",
      "ROUGE-2                                   0.107634 ± 0.075601\n",
      "ROUGE-L                                   0.207904 ± 0.077409\n",
      "dtype: object\n",
      "✅ Completed: Qwen2.5-0.5B-Instruct | huggingface | summarization | batch\n",
      "→ huggingface | Qwen2.5-0.5B-Instruct | summarization @ batch=16 -> /home/ubuntu/fast_llm_inference/results/huggingface_Qwen2.5-0.5B-Instruct_summarization_16batch.csv\n",
      "Running benchmark for Qwen2.5-0.5B-Instruct with huggingface on summarization [batch]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Qwen2.5-0.5B-Instruct on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.030912 ± 0.003834\n",
      "ATL                                       1.168319 ± 0.566697\n",
      "GL                                      93.387536 ± 16.622109\n",
      "TPS                                       1.123000 ± 0.841155\n",
      "SPS                                       0.066800 ± 0.050350\n",
      "Avg GPU Mem (MB)                      1832.974800 ± 20.807874\n",
      "Peak GPU Mem (MB)                     1870.600000 ± 40.930115\n",
      "Avg GPU Util (%)                         29.392400 ± 1.948860\n",
      "Peak GPU Util (%)                        52.160000 ± 6.784891\n",
      "Total Energy (Wh)                         1.229582 ± 0.207937\n",
      "Avg Power (W)                            47.502000 ± 1.226897\n",
      "Peak Power (W)                           52.322400 ± 2.028124\n",
      "Energy per Token (J/token)              55.413391 ± 26.752605\n",
      "Energy per Sentence (J/sentence)      916.878039 ± 517.769281\n",
      "Memory Usage (MB)                     1722.880000 ± 41.593026\n",
      "Model Size (MB)                         953.279826 ± 0.000000\n",
      "Overhead (MB)                          769.600174 ± 41.593026\n",
      "ROUGE-1                                   0.328205 ± 0.099791\n",
      "ROUGE-2                                   0.107634 ± 0.075601\n",
      "ROUGE-L                                   0.207904 ± 0.077409\n",
      "dtype: object\n",
      "✅ Completed: Qwen2.5-0.5B-Instruct | huggingface | summarization | batch\n",
      "→ huggingface | Qwen2.5-0.5B-Instruct | summarization @ batch=32 -> /home/ubuntu/fast_llm_inference/results/huggingface_Qwen2.5-0.5B-Instruct_summarization_32batch.csv\n",
      "Running benchmark for Qwen2.5-0.5B-Instruct with huggingface on summarization [batch]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Qwen2.5-0.5B-Instruct on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.031344 ± 0.003949\n",
      "ATL                                       2.195871 ± 1.074638\n",
      "GL                                     175.279304 ± 31.890578\n",
      "TPS                                       0.666200 ± 0.794084\n",
      "SPS                                       0.039900 ± 0.047662\n",
      "Avg GPU Mem (MB)                      1870.241600 ± 34.011821\n",
      "Peak GPU Mem (MB)                     1918.600000 ± 48.995997\n",
      "Avg GPU Util (%)                         30.971600 ± 0.738306\n",
      "Peak GPU Util (%)                        55.360000 ± 2.524706\n",
      "Total Energy (Wh)                         2.381031 ± 0.432582\n",
      "Avg Power (W)                            48.868800 ± 0.407568\n",
      "Peak Power (W)                           54.216000 ± 0.161808\n",
      "Energy per Token (J/token)             107.384290 ± 52.508403\n",
      "Energy per Sentence (J/sentence)     1773.906019 ± 997.931059\n",
      "Memory Usage (MB)                     1719.600000 ± 39.331813\n",
      "Model Size (MB)                         953.279826 ± 0.000000\n",
      "Overhead (MB)                          766.320174 ± 39.331813\n",
      "ROUGE-1                                   0.328205 ± 0.099791\n",
      "ROUGE-2                                   0.107634 ± 0.075601\n",
      "ROUGE-L                                   0.207904 ± 0.077409\n",
      "dtype: object\n",
      "✅ Completed: Qwen2.5-0.5B-Instruct | huggingface | summarization | batch\n",
      "→ huggingface | Qwen2.5-0.5B-Instruct | summarization @ batch=64 -> /home/ubuntu/fast_llm_inference/results/huggingface_Qwen2.5-0.5B-Instruct_summarization_64batch.csv\n",
      "Running benchmark for Qwen2.5-0.5B-Instruct with huggingface on summarization [batch]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Qwen2.5-0.5B-Instruct on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.029860 ± 0.001688\n",
      "ATL                                       3.836238 ± 2.075804\n",
      "GL                                     305.392596 ± 75.699320\n",
      "TPS                                       0.331700 ± 0.167839\n",
      "SPS                                       0.018600 ± 0.009746\n",
      "Avg GPU Mem (MB)                      1882.446400 ± 15.311952\n",
      "Peak GPU Mem (MB)                     1974.440000 ± 26.050580\n",
      "Avg GPU Util (%)                         30.927200 ± 0.352165\n",
      "Peak GPU Util (%)                        59.800000 ± 2.412091\n",
      "Total Energy (Wh)                         4.138161 ± 1.042800\n",
      "Avg Power (W)                            48.719200 ± 0.255682\n",
      "Peak Power (W)                           54.310400 ± 0.067539\n",
      "Energy per Token (J/token)            187.144790 ± 101.747870\n",
      "Energy per Sentence (J/sentence)    3104.012676 ± 1986.630358\n",
      "Memory Usage (MB)                     1716.980000 ± 36.084497\n",
      "Model Size (MB)                         953.279826 ± 0.000000\n",
      "Overhead (MB)                          763.700174 ± 36.084497\n",
      "ROUGE-1                                   0.328205 ± 0.099791\n",
      "ROUGE-2                                   0.107634 ± 0.075601\n",
      "ROUGE-L                                   0.207904 ± 0.077409\n",
      "dtype: object\n",
      "✅ Completed: Qwen2.5-0.5B-Instruct | huggingface | summarization | batch\n",
      "→ huggingface | gemma-2-9b-it | summarization @ batch=1 -> /home/ubuntu/fast_llm_inference/results/huggingface_gemma-2-9b-it_summarization_1batch.csv\n",
      "Running benchmark for gemma-2-9b-it with huggingface on summarization [batch]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dca1e507ef94d9bb9ba3937626e8cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for gemma-2-9b-it on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.109611 ± 0.008780\n",
      "ATL                                       0.151071 ± 0.015849\n",
      "GL                                        7.422561 ± 1.087148\n",
      "TPS                                       6.689900 ± 0.684713\n",
      "SPS                                       0.437200 ± 0.075893\n",
      "Avg GPU Mem (MB)                    19963.914400 ± 376.959717\n",
      "Peak GPU Mem (MB)                   19969.400000 ± 378.250531\n",
      "Avg GPU Util (%)                         93.962000 ± 3.928514\n",
      "Peak GPU Util (%)                       100.000000 ± 0.000000\n",
      "Total Energy (Wh)                         0.144312 ± 0.021550\n",
      "Avg Power (W)                            69.969700 ± 0.750354\n",
      "Peak Power (W)                           73.197800 ± 0.582042\n",
      "Energy per Token (J/token)               10.568225 ± 1.092814\n",
      "Energy per Sentence (J/sentence)       164.402765 ± 25.974715\n",
      "Memory Usage (MB)                   19969.400000 ± 378.250531\n",
      "Model Size (MB)                       17648.049026 ± 0.000000\n",
      "Overhead (MB)                        2321.350974 ± 378.250531\n",
      "ROUGE-1                                   0.374980 ± 0.086204\n",
      "ROUGE-2                                   0.124238 ± 0.074125\n",
      "ROUGE-L                                   0.240116 ± 0.076754\n",
      "dtype: object\n",
      "✅ Completed: gemma-2-9b-it | huggingface | summarization | batch\n",
      "→ huggingface | gemma-2-9b-it | summarization @ batch=8 -> /home/ubuntu/fast_llm_inference/results/huggingface_gemma-2-9b-it_summarization_8batch.csv\n",
      "Running benchmark for gemma-2-9b-it with huggingface on summarization [batch]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644df779da6c4c2e88e7eea1544fcc36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for gemma-2-9b-it on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.107660 ± 0.003864\n",
      "ATL                                       1.192401 ± 0.220626\n",
      "GL                                       57.734260 ± 7.015240\n",
      "TPS                                       0.875700 ± 0.222167\n",
      "SPS                                       0.056700 ± 0.016271\n",
      "Avg GPU Mem (MB)                    20426.182400 ± 449.369862\n",
      "Peak GPU Mem (MB)                   20727.720000 ± 462.983533\n",
      "Avg GPU Util (%)                         97.914000 ± 1.064627\n",
      "Peak GPU Util (%)                       100.000000 ± 0.000000\n",
      "Total Energy (Wh)                         1.145000 ± 0.140313\n",
      "Avg Power (W)                            71.380800 ± 0.153239\n",
      "Peak Power (W)                           74.233600 ± 0.760786\n",
      "Energy per Token (J/token)              85.131952 ± 15.810740\n",
      "Energy per Sentence (J/sentence)     1312.420154 ± 218.123601\n",
      "Memory Usage (MB)                   19533.720000 ± 540.292377\n",
      "Model Size (MB)                       17648.049026 ± 0.000000\n",
      "Overhead (MB)                        1885.670974 ± 540.292377\n",
      "ROUGE-1                                   0.374980 ± 0.086204\n",
      "ROUGE-2                                   0.124238 ± 0.074125\n",
      "ROUGE-L                                   0.240116 ± 0.076754\n",
      "dtype: object\n",
      "✅ Completed: gemma-2-9b-it | huggingface | summarization | batch\n",
      "→ huggingface | gemma-2-9b-it | summarization @ batch=16 -> /home/ubuntu/fast_llm_inference/results/huggingface_gemma-2-9b-it_summarization_16batch.csv\n",
      "Running benchmark for gemma-2-9b-it with huggingface on summarization [batch]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8b4cab23ba4974b95525275b418e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for gemma-2-9b-it on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.109636 ± 0.003800\n",
      "ATL                                       2.356189 ± 0.519138\n",
      "GL                                     113.976040 ± 18.238956\n",
      "TPS                                       0.472400 ± 0.263604\n",
      "SPS                                       0.031500 ± 0.020170\n",
      "Avg GPU Mem (MB)                    20658.990000 ± 500.739588\n",
      "Peak GPU Mem (MB)                   21112.360000 ± 736.629349\n",
      "Avg GPU Util (%)                         98.144000 ± 0.237801\n",
      "Peak GPU Util (%)                       100.000000 ± 0.000000\n",
      "Total Energy (Wh)                         2.269748 ± 0.364277\n",
      "Avg Power (W)                            71.669200 ± 0.191325\n",
      "Peak Power (W)                           74.831600 ± 1.066650\n",
      "Energy per Token (J/token)             168.916413 ± 37.282173\n",
      "Energy per Sentence (J/sentence)     2605.250354 ± 513.770657\n",
      "Memory Usage (MB)                   19511.980000 ± 522.225105\n",
      "Model Size (MB)                       17648.049026 ± 0.000000\n",
      "Overhead (MB)                        1863.930974 ± 522.225105\n",
      "ROUGE-1                                   0.374980 ± 0.086204\n",
      "ROUGE-2                                   0.124238 ± 0.074125\n",
      "ROUGE-L                                   0.240116 ± 0.076754\n",
      "dtype: object\n",
      "✅ Completed: gemma-2-9b-it | huggingface | summarization | batch\n",
      "→ huggingface | gemma-2-9b-it | summarization @ batch=32 -> /home/ubuntu/fast_llm_inference/results/huggingface_gemma-2-9b-it_summarization_32batch.csv\n",
      "Running benchmark for gemma-2-9b-it with huggingface on summarization [batch]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0022ae8374484c1caa111d60cdd4f2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for gemma-2-9b-it on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.111012 ± 0.000566\n",
      "ATL                                       4.723013 ± 1.135989\n",
      "GL                                     228.547548 ± 42.232385\n",
      "TPS                                       0.268700 ± 0.296950\n",
      "SPS                                       0.016200 ± 0.022820\n",
      "Avg GPU Mem (MB)                    20921.379200 ± 570.148475\n",
      "Peak GPU Mem (MB)                   21401.240000 ± 673.089718\n",
      "Avg GPU Util (%)                         97.641200 ± 0.144287\n",
      "Peak GPU Util (%)                       100.000000 ± 0.000000\n",
      "Total Energy (Wh)                         4.557379 ± 0.843918\n",
      "Avg Power (W)                            71.754000 ± 0.185973\n",
      "Peak Power (W)                           75.909600 ± 2.076478\n",
      "Energy per Token (J/token)             339.040985 ± 81.629997\n",
      "Energy per Sentence (J/sentence)    5233.226280 ± 1133.632431\n",
      "Memory Usage (MB)                   19401.820000 ± 427.805153\n",
      "Model Size (MB)                       17648.049026 ± 0.000000\n",
      "Overhead (MB)                        1753.770974 ± 427.805153\n",
      "ROUGE-1                                   0.374980 ± 0.086204\n",
      "ROUGE-2                                   0.124238 ± 0.074125\n",
      "ROUGE-L                                   0.240116 ± 0.076754\n",
      "dtype: object\n",
      "✅ Completed: gemma-2-9b-it | huggingface | summarization | batch\n",
      "→ huggingface | gemma-2-9b-it | summarization @ batch=64 -> /home/ubuntu/fast_llm_inference/results/huggingface_gemma-2-9b-it_summarization_64batch.csv\n",
      "Running benchmark for gemma-2-9b-it with huggingface on summarization [batch]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a272a1b682b4f55b31fbc0a17e421ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for gemma-2-9b-it on huggingface/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.115560 ± 0.001688\n",
      "ATL                                       8.234410 ± 2.302352\n",
      "GL                                     397.219252 ± 88.580077\n",
      "TPS                                       0.132300 ± 0.040323\n",
      "SPS                                       0.010000 ± 0.000000\n",
      "Avg GPU Mem (MB)                    21224.536400 ± 516.544411\n",
      "Peak GPU Mem (MB)                     22247.480000 ± 4.824182\n",
      "Avg GPU Util (%)                         96.945600 ± 0.221912\n",
      "Peak GPU Util (%)                       100.000000 ± 0.000000\n",
      "Total Energy (Wh)                         7.913674 ± 1.758998\n",
      "Avg Power (W)                            71.736800 ± 0.062714\n",
      "Peak Power (W)                           75.580000 ± 0.603023\n",
      "Energy per Token (J/token)            590.585689 ± 164.773715\n",
      "Energy per Sentence (J/sentence)    9036.273685 ± 2187.584903\n",
      "Memory Usage (MB)                   19625.420000 ± 376.668743\n",
      "Model Size (MB)                       17648.049026 ± 0.000000\n",
      "Overhead (MB)                        1977.370974 ± 376.668743\n",
      "ROUGE-1                                   0.374980 ± 0.086204\n",
      "ROUGE-2                                   0.124238 ± 0.074125\n",
      "ROUGE-L                                   0.240116 ± 0.076754\n",
      "dtype: object\n",
      "✅ Completed: gemma-2-9b-it | huggingface | summarization | batch\n"
     ]
    }
   ],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference\"\n",
    "backends  = [\"huggingface\"] #\"vllm\", \"llama.cpp\", \n",
    "tasks     = [\"summarization\"]\n",
    "batch_sizes = [1, 8, 16, 32, 64]   # ← as requested\n",
    "sample_interval = 0.05             # s\n",
    "max_batch_size  = 64               # keep the same cap\n",
    "\n",
    "for backend in backends:\n",
    "    if backend == \"llama.cpp\":\n",
    "        models = [\n",
    "            \"gemma-2-2b-it-fp16.gguf\",\n",
    "            \"gemma-2-9b-it-fp16.gguf\",\n",
    "            \"llama-3.1-8B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "            \"qwen2.5-0.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-1.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-3b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-7B-instruct-fp16.gguf\",\n",
    "        ]\n",
    "    else:\n",
    "        models = [\n",
    "            #\"gemma-2-2b-it\",\n",
    "            \"llama-3.1-8B-Instruct\",\n",
    "            \"llama-3.2-3b-instruct\",\n",
    "            \"llama-3.2-1b-instruct\",\n",
    "            \"Qwen2.5-7B-Instruct\",\n",
    "            \"Qwen2.5-3B-Instruct\",\n",
    "            \"Qwen2.5-1.5B-Instruct\",\n",
    "            \"Qwen2.5-0.5B-Instruct\",\n",
    "        ]\n",
    "        if backend != \"vllm\":\n",
    "            models.append(\"gemma-2-9b-it\")   # too large for vllm\n",
    "\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            for bs in batch_sizes:\n",
    "                export_path = (\n",
    "                    f\"{base_path}/results/\"\n",
    "                    f\"{backend}_{model}_{task}_{bs}batch.csv\"\n",
    "                )\n",
    "                print(f\"→ {backend} | {model} | {task} @ batch={bs} -> {export_path}\")\n",
    "                run_benchmark(\n",
    "                    backend=backend,\n",
    "                    model_name=model,\n",
    "                    task=task,\n",
    "                    base_path=base_path,\n",
    "                    scenario=\"batch\",\n",
    "                    batch_size=bs,\n",
    "                    sample_interval=sample_interval,\n",
    "                    max_batch_size=max_batch_size,\n",
    "                    export_path=export_path,\n",
    "                    verbose=False,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11221c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-18 11:20:03 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 11:20:04.154985: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747567204.173094   11479 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747567204.178287   11479 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747567204.194063   11479 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747567204.194074   11479 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747567204.194076   11479 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747567204.194078   11479 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-18 11:20:04.199301: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-18 11:20:24 [config.py:717] This model supports multiple tasks: {'generate', 'reward', 'score', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 05-18 11:20:26 [config.py:830] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-18 11:20:26 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-18 11:20:27 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', speculative_config=None, tokenizer='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-18 11:20:27 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7835305e4690>\n",
      "INFO 05-18 11:20:29 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-18 11:20:29 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-18 11:20:29 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-18 11:20:29 [gpu_model_runner.py:1329] Starting to load model /home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit...\n",
      "INFO 05-18 11:20:29 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7150972bd7454a4595f3d63a03e69452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd7008286904425a201d913b7ee1068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-18 11:20:33 [gpu_model_runner.py:1347] Model loading took 5.3132 GiB and 3.308005 seconds\n",
      "INFO 05-18 11:20:44 [backends.py:420] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/a89f85ea99/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-18 11:20:44 [backends.py:430] Dynamo bytecode transform time: 11.28 s\n",
      "INFO 05-18 11:20:51 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 5.912 s\n",
      "INFO 05-18 11:20:54 [monitor.py:33] torch.compile takes 11.28 s in total\n",
      "INFO 05-18 11:20:57 [kv_cache_utils.py:634] GPU KV cache size: 106,880 tokens\n",
      "INFO 05-18 11:20:57 [kv_cache_utils.py:637] Maximum concurrency for 8,192 tokens per request: 13.05x\n",
      "INFO 05-18 11:22:05 [gpu_model_runner.py:1686] Graph capturing finished in 68 secs, took 1.54 GiB\n",
      "INFO 05-18 11:22:06 [core.py:159] init engine (profile, create kv cache, warmup model) took 92.94 seconds\n",
      "INFO 05-18 11:22:06 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "from benchmark.backends.vllm_backend import VLLMBackend\n",
    "\n",
    "model = VLLMBackend(\"/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit\")\n",
    "model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec34ff98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30c43187b234613a8a17236c3dc38a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the purpose of life?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperplexity\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fast_llm_inference/benchmark/backends/vllm_backend.py:93\u001b[39m, in \u001b[36mVLLMBackend.generate\u001b[39m\u001b[34m(self, prompts, task_type, perplexity)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# sample.logprobs is an OpenAI‐style dict:\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# {\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m#    \"tokens\": [...],                # str tokens\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m \u001b[38;5;66;03m#    \"text_offset\": [...]\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[32m     92\u001b[39m lp_dict  = sample.logprobs                             \n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m tokens   = \u001b[43mlp_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m                            \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m     94\u001b[39m logps    = lp_dict[\u001b[33m\"\u001b[39m\u001b[33mtoken_logprobs\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Compute per-token perplexity\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "model.generate(\"What is the purpose of life?\", perplexity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee5b9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-18 11:26:48 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 11:26:49.212117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747567609.229561   12031 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747567609.235248   12031 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747567609.250681   12031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747567609.250693   12031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747567609.250695   12031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747567609.250696   12031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-18 11:26:49.256273: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and setup\n",
    "import os\n",
    "import math\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# (Optional) adjust your model path here\n",
    "MODEL_PATH = \"/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit\"\n",
    "\n",
    "# Cell 2: Load model and define prompts\n",
    "model = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    max_model_len=4096,\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"What is the purpose of life?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21fd6017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acccb472a5d421db912f2ddeff6bdc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 3: Configure SamplingParams for logprobs & perplexity\n",
    "params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    max_tokens=32,\n",
    "    logprobs=1,\n",
    "    prompt_logprobs=1\n",
    ")\n",
    "\n",
    "# Cell 4: Run generation and display results in a table\n",
    "outputs = model.generate(prompts, params)\n",
    "\n",
    "for i, gen_out in enumerate(outputs):\n",
    "    sample  = gen_out.outputs[0]\n",
    "    text    = sample.text.lstrip()\n",
    "    lp_dict = sample.logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc2a6b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{2209: Logprob(logprob=-1.4004144668579102, rank=1, decoded_token='ĠIs')},\n",
       " {433: Logprob(logprob=-0.17977960407733917, rank=1, decoded_token='Ġit')},\n",
       " {311: Logprob(logprob=-0.0869283527135849, rank=1, decoded_token='Ġto')},\n",
       " {1505: Logprob(logprob=-0.9659126996994019, rank=1, decoded_token='Ġfind')},\n",
       " {23871: Logprob(logprob=-0.04803086444735527, rank=1, decoded_token='Ġhappiness')},\n",
       " {11: Logprob(logprob=-0.058653172105550766, rank=1, decoded_token=',')},\n",
       " {311: Logprob(logprob=-0.8425228595733643, rank=1, decoded_token='Ġto')},\n",
       " {11322: Logprob(logprob=-0.7215710878372192, rank=1, decoded_token='Ġachieve')},\n",
       " {2450: Logprob(logprob=-0.043824948370456696, rank=1, decoded_token='Ġsuccess')},\n",
       " {11: Logprob(logprob=-0.0013250865740701556, rank=1, decoded_token=',')},\n",
       " {477: Logprob(logprob=-0.7743627429008484, rank=2, decoded_token='Ġor'),\n",
       "  311: Logprob(logprob=-0.6181127429008484, rank=1, decoded_token='Ġto')},\n",
       " {311: Logprob(logprob=-0.031453102827072144, rank=1, decoded_token='Ġto')},\n",
       " {1304: Logprob(logprob=-0.7800344228744507, rank=1, decoded_token='Ġmake')},\n",
       " {264: Logprob(logprob=-0.018590614199638367, rank=1, decoded_token='Ġa')},\n",
       " {6811: Logprob(logprob=-0.16793595254421234, rank=1, decoded_token='Ġdifference')},\n",
       " {304: Logprob(logprob=-0.1038089394569397, rank=1, decoded_token='Ġin')},\n",
       " {279: Logprob(logprob=-0.0009804924484342337, rank=1, decoded_token='Ġthe')},\n",
       " {1917: Logprob(logprob=-0.0022238779347389936, rank=1, decoded_token='Ġworld')},\n",
       " {30: Logprob(logprob=-0.11063252389431, rank=1, decoded_token='?')},\n",
       " {578: Logprob(logprob=-1.3353930711746216, rank=1, decoded_token='ĠThe')},\n",
       " {4320: Logprob(logprob=-0.25850245356559753, rank=1, decoded_token='Ġanswer')},\n",
       " {311: Logprob(logprob=-0.9216098785400391, rank=1, decoded_token='Ġto')},\n",
       " {420: Logprob(logprob=-0.04812448099255562, rank=1, decoded_token='Ġthis')},\n",
       " {3488: Logprob(logprob=-0.02162298373878002, rank=1, decoded_token='Ġquestion')},\n",
       " {374: Logprob(logprob=-0.520580530166626, rank=1, decoded_token='Ġis')},\n",
       " {44122: Logprob(logprob=-1.261415958404541, rank=1, decoded_token='Ġsubjective')},\n",
       " {323: Logprob(logprob=-0.049682993441820145, rank=1, decoded_token='Ġand')},\n",
       " {649: Logprob(logprob=-0.46952304244041443, rank=1, decoded_token='Ġcan')},\n",
       " {13592: Logprob(logprob=-0.009865455329418182, rank=1, decoded_token='Ġvary')},\n",
       " {19407: Logprob(logprob=-0.37328964471817017, rank=1, decoded_token='Ġgreatly')},\n",
       " {505: Logprob(logprob=-0.14553403854370117, rank=1, decoded_token='Ġfrom')},\n",
       " {1732: Logprob(logprob=-0.0395626574754715, rank=1, decoded_token='Ġperson')}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26fb6b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975be3f036034fe9abea836af4f1d5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prompt 1: The quick brown fox jumps over the lazy dog. ===\n",
      "Generated: The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick\n",
      "\n",
      "       Token |  LogProb |      PPL\n",
      "----------------------------------\n",
      "        ĠThe |  -0.9727 |   2.6450\n",
      "      Ġquick |  -0.3296 |   1.3905\n",
      "      Ġbrown |  -0.0029 |   1.0029\n",
      "        Ġfox |  -0.0016 |   1.0016\n",
      "      Ġjumps |  -0.0055 |   1.0055\n",
      "       Ġover |  -0.0005 |   1.0005\n",
      "        Ġthe |  -0.0005 |   1.0005\n",
      "       Ġlazy |  -0.0005 |   1.0005\n",
      "        Ġdog |  -0.0004 |   1.0004\n",
      "           . |  -0.3360 |   1.3993\n",
      "        ĠThe |  -0.0372 |   1.0379\n",
      "      Ġquick |  -0.0015 |   1.0015\n",
      "      Ġbrown |  -0.0012 |   1.0012\n",
      "        Ġfox |  -0.0007 |   1.0007\n",
      "      Ġjumps |  -0.0013 |   1.0013\n",
      "       Ġover |  -0.0008 |   1.0008\n",
      "        Ġthe |  -0.0010 |   1.0010\n",
      "       Ġlazy |  -0.0010 |   1.0010\n",
      "        Ġdog |  -0.0005 |   1.0005\n",
      "           . |  -0.2609 |   1.2981\n",
      "        ĠThe |  -0.0465 |   1.0476\n",
      "      Ġquick |  -0.0021 |   1.0021\n",
      "      Ġbrown |  -0.0006 |   1.0006\n",
      "        Ġfox |  -0.0014 |   1.0014\n",
      "      Ġjumps |  -0.0008 |   1.0008\n",
      "       Ġover |  -0.0002 |   1.0002\n",
      "        Ġthe |  -0.0008 |   1.0008\n",
      "       Ġlazy |  -0.0007 |   1.0007\n",
      "        Ġdog |  -0.0004 |   1.0004\n",
      "           . |  -0.1384 |   1.1484\n",
      "        ĠThe |  -0.0204 |   1.0206\n",
      "      Ġquick |  -0.0011 |   1.0011\n",
      "\n",
      "=== Prompt 2: What is the purpose of life? ===\n",
      "Generated: Is it to find happiness, to achieve success, to make a difference, or to fulfill a purpose? The answer to this question is different for each individual,\n",
      "\n",
      "       Token |  LogProb |      PPL\n",
      "----------------------------------\n",
      "         ĠIs |  -1.4004 |   4.0569\n",
      "         Ġit |  -0.1798 |   1.1970\n",
      "         Ġto |  -0.0869 |   1.0908\n",
      "       Ġfind |  -0.9659 |   2.6272\n",
      "  Ġhappiness |  -0.0480 |   1.0492\n",
      "           , |  -0.0587 |   1.0604\n",
      "         Ġto |  -0.8425 |   2.3222\n",
      "    Ġachieve |  -0.7216 |   2.0577\n",
      "    Ġsuccess |  -0.0438 |   1.0448\n",
      "           , |  -0.0013 |   1.0013\n",
      "         Ġto |  -0.6181 |   1.8554\n",
      "       Ġmake |  -0.8055 |   2.2377\n",
      "          Ġa |  -0.0282 |   1.0286\n",
      " Ġdifference |  -0.0444 |   1.0454\n",
      "           , |  -0.5264 |   1.6929\n",
      "         Ġor |  -0.0248 |   1.0251\n",
      "  Ġsomething |  -0.7462 |   2.1090\n",
      "    Ġfulfill |  -1.0393 |   2.8273\n",
      "          Ġa |  -1.0664 |   2.9050\n",
      "    Ġpurpose |  -0.8271 |   2.2867\n",
      "           ? |  -0.4777 |   1.6124\n",
      "        ĠThe |  -0.9232 |   2.5173\n",
      "     Ġanswer |  -0.2054 |   1.2280\n",
      "         Ġto |  -1.1311 |   3.0989\n",
      "       Ġthis |  -0.0250 |   1.0253\n",
      "   Ġquestion |  -0.0269 |   1.0272\n",
      "         Ġis |  -0.6289 |   1.8755\n",
      " Ġsubjective |  -1.4515 |   4.2693\n",
      "        Ġfor |  -0.0012 |   1.0012\n",
      "       Ġeach |  -0.1205 |   1.1281\n",
      " Ġindividual |  -0.4515 |   1.5707\n",
      "           , |  -0.2584 |   1.2949\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Run generation and display results in a table\n",
    "outputs = model.generate(prompts, params)\n",
    "\n",
    "for i, gen_out in enumerate(outputs):\n",
    "    sample   = gen_out.outputs[0]\n",
    "    text     = sample.text.lstrip()\n",
    "    lp_list  = sample.logprobs            # list of dicts\n",
    "    token_ids = sample.token_ids\n",
    "\n",
    "    # Extract the chosen-token strings & logprobs\n",
    "    tokens, logps = [], []\n",
    "    for entry in lp_list:\n",
    "        # each entry is {token_id: Logprob(...), ...}\n",
    "        for tid, lp_obj in entry.items():\n",
    "            if lp_obj.rank == 1:\n",
    "                tokens.append(lp_obj.decoded_token)\n",
    "                logps.append(lp_obj.logprob)\n",
    "                break\n",
    "\n",
    "    # Compute per-token perplexity\n",
    "    ppl = [math.exp(-lp) for lp in logps]\n",
    "\n",
    "    print(f\"\\n=== Prompt {i+1}: {prompts[i]} ===\")\n",
    "    print(f\"Generated: {text}\\n\")\n",
    "    print(f\"{'Token':>12} | {'LogProb':>8} | {'PPL':>8}\")\n",
    "    print(\"-\" * 34)\n",
    "    for tok, lp, p in zip(tokens, logps, ppl):\n",
    "        print(f\"{tok:>12} | {lp:8.4f} | {p:8.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
