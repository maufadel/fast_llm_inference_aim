{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c8f55e",
   "metadata": {},
   "source": [
    "### RQ1 - Comparing different quantization levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef740d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 15:48:54.090065: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747064934.110013  260255 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747064934.115811  260255 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747064934.135655  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135683  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135685  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135687  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-12 15:48:54.141568: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 15:48:57 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-12 15:48:57 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-05-12 15:49:00,664] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_benchmark(backend, model_name, task, base_path, samples=500, verbose=False, batch_size=100):\n",
    "    print(f\"Running benchmark for {model_name} with {backend} on {task}\")\n",
    "    try:\n",
    "        bm = ModelBenchmark(\n",
    "            backend=backend,\n",
    "            model_name=model_name,\n",
    "            model_path=f\"{base_path}/models/{model_name}\",\n",
    "            base_path=base_path,\n",
    "            task=task,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        bm.run(samples=samples, batch_size=batch_size)\n",
    "        bm.close()\n",
    "        del bm\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"✅ Completed: {model_name} | {backend} | {task}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed: {model_name} | {backend} | {task} -- {e}\")\n",
    "        torch.cuda.empty_cache()  # ensure no memory leak on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac63b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference/\"\n",
    "\n",
    "backends = [\"vllm\"] #, \"huggingface\",\"deepspeed_mii\", \"llama.cpp\"]\n",
    "models   = [\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.1-8B-Instruct-4bit\",\n",
    "    \"llama-3.1-8B-Instruct-8bit\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "    \"llama-3.2-3b-instruct-4bit\",\n",
    "    \"llama-3.2-1b-instruct-4bit\",\n",
    "    \"llama-3.2-3b-instruct-8bit\",\n",
    "    \"llama-3.2-1b-instruct-8bit\",\n",
    "   \n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-7B-Instruct-4bit\",\n",
    "    \"Qwen2.5-7B-Instruct-8bit\", # some weird error\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct-4bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-3B-Instruct-8bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-8bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-8bit\",\n",
    "\n",
    "\n",
    "    \"gemma-2-9b-it-bnb4\",\n",
    "    \"gemma-2-9b-it-8bit\",\n",
    "    \"gemma-2-9b-it\", # too large\n",
    "    \"gemma-2-2b-it-4bit\",\n",
    "    \"gemma-2-2b-it-8bit\",\n",
    "    \"gemma-2-2b-it\",\n",
    "]\n",
    "\n",
    "tasks    = [\"summarization\", \"qa\", \"sql\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb85145",
   "metadata": {},
   "source": [
    "first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for backend in backends:\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=100,\n",
    "                verbose=False,\n",
    "                batch_size=100,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c7713",
   "metadata": {},
   "source": [
    "check if anything is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a66e3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models with missing files:\n",
      "Qwen2.5-7B-Instruct-8bit\n",
      "gemma-2-9b-it\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define your parameters\n",
    "backends = [\"vllm\"]\n",
    "models = [\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.1-8B-Instruct-4bit\",\n",
    "    \"llama-3.1-8B-Instruct-8bit\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "    \"llama-3.2-3b-instruct-4bit\",\n",
    "    \"llama-3.2-1b-instruct-4bit\",\n",
    "    \"llama-3.2-3b-instruct-8bit\",\n",
    "    \"llama-3.2-1b-instruct-8bit\",\n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-7B-Instruct-4bit\",\n",
    "    \"Qwen2.5-7B-Instruct-8bit\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct-4bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-3B-Instruct-8bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-8bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-8bit\",\n",
    "    \"gemma-2-9b-it-4bit\",\n",
    "    \"gemma-2-9b-it-8bit\",\n",
    "    \"gemma-2-9b-it\",\n",
    "    \"gemma-2-2b-it-4bit\",\n",
    "    \"gemma-2-2b-it-8bit\",\n",
    "    \"gemma-2-2b-it\",\n",
    "]\n",
    "tasks = [\"summarization\", \"qa\", \"sql\"]\n",
    "\n",
    "results_dir = \"./results/experiment_1/\"\n",
    "\n",
    "missing_models = set()\n",
    "\n",
    "for backend in backends:\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            filename = f\"{backend}_{model}_{task}.csv\"\n",
    "            filepath = os.path.join(results_dir, filename)\n",
    "            if not os.path.exists(filepath):\n",
    "                missing_models.add(model)\n",
    "                break  # No need to check more tasks if one is missing\n",
    "\n",
    "# Print models with missing files\n",
    "if missing_models:\n",
    "    print(\"Models with missing files:\")\n",
    "    for model in sorted(missing_models):\n",
    "        print(model)\n",
    "else:\n",
    "    print(\"✅ All models are complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e8200",
   "metadata": {},
   "source": [
    "try again with the missing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for backend in backends:\n",
    "    for model in list(missing_models):\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=500,\n",
    "                verbose=False,\n",
    "                batch_size=100,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2610ec",
   "metadata": {},
   "source": [
    "### RQ2 - Comparing different inference engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference/models\"\n",
    "\n",
    "backends = [\"vllm\", \"huggingface\", \"llama.cpp\"] #,\"deepspeed_mii\", \"huggingface\"]\n",
    "\n",
    "models   = [\n",
    "    \"gemma-2-9b-it\", \n",
    "    \"gemma-2-2b-it\",\n",
    "\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "   \n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "]\n",
    "\n",
    "tasks    = [\"summarization\", \"qa\", \"sql\",]\n",
    "\n",
    "for backend in backends:\n",
    "    if backend == \"llama.cpp\":\n",
    "        models = [\n",
    "            \"gemma-2-2b-it-fp16.gguf\",\n",
    "            \"gemma-2-9b-it-fp16.gguf\",\n",
    "\n",
    "            \"llama-3.1-8B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "            \n",
    "            \"qwen2.5-0.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-1.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-3b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-7B-instruct-fp16.gguf\",\n",
    "        ]\n",
    "        \n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=100,\n",
    "                verbose=False,\n",
    "                batch_size=20,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
