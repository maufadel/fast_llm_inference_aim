{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c8f55e",
   "metadata": {},
   "source": [
    "### RQ1 - Comparing different quantization levels\n",
    "\n",
    "**Quantization Dimension**\n",
    "\n",
    "How does quantization in different models and architectures affect system and task-specific metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef740d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 15:48:54.090065: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747064934.110013  260255 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747064934.115811  260255 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747064934.135655  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135683  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135685  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135687  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-12 15:48:54.141568: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 15:48:57 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-12 15:48:57 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-05-12 15:49:00,664] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_benchmark(backend, model_name, task, base_path, samples=500, verbose=False, batch_size=100):\n",
    "    print(f\"Running benchmark for {model_name} with {backend} on {task}\")\n",
    "    try:\n",
    "        bm = ModelBenchmark(\n",
    "            backend=backend,\n",
    "            model_name=model_name,\n",
    "            model_path=f\"{base_path}/models/{model_name}\",\n",
    "            base_path=base_path,\n",
    "            task=task,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        bm.run(samples=samples, batch_size=batch_size)\n",
    "        bm.close()\n",
    "        del bm\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"✅ Completed: {model_name} | {backend} | {task}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed: {model_name} | {backend} | {task} -- {e}\")\n",
    "        torch.cuda.empty_cache()  # ensure no memory leak on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac63b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference/\"\n",
    "\n",
    "backends = [\"vllm\"] #, \"huggingface\",\"deepspeed_mii\", \"llama.cpp\"]\n",
    "models   = [\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.1-8B-Instruct-4bit\",\n",
    "    \"llama-3.1-8B-Instruct-8bit\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "    \"llama-3.2-3b-instruct-4bit\",\n",
    "    \"llama-3.2-1b-instruct-4bit\",\n",
    "    \"llama-3.2-3b-instruct-8bit\",\n",
    "    \"llama-3.2-1b-instruct-8bit\",\n",
    "   \n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-7B-Instruct-4bit\",\n",
    "    \"Qwen2.5-7B-Instruct-8bit\", # some weird error\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct-4bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-3B-Instruct-8bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-8bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-8bit\",\n",
    "\n",
    "\n",
    "    \"gemma-2-9b-it-bnb4\",\n",
    "    \"gemma-2-9b-it-8bit\",\n",
    "    \"gemma-2-9b-it\", # too large\n",
    "    \"gemma-2-2b-it-4bit\",\n",
    "    \"gemma-2-2b-it-8bit\",\n",
    "    \"gemma-2-2b-it\",\n",
    "]\n",
    "\n",
    "tasks    = [\"summarization\", \"qa\", \"sql\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb85145",
   "metadata": {},
   "source": [
    "first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for backend in backends:\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=100,\n",
    "                verbose=False,\n",
    "                batch_size=100,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c7713",
   "metadata": {},
   "source": [
    "check if anything is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a66e3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models with missing files:\n",
      "Qwen2.5-7B-Instruct-8bit\n",
      "gemma-2-9b-it\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define your parameters\n",
    "backends = [\"vllm\"]\n",
    "models = [\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.1-8B-Instruct-4bit\",\n",
    "    \"llama-3.1-8B-Instruct-8bit\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "    \"llama-3.2-3b-instruct-4bit\",\n",
    "    \"llama-3.2-1b-instruct-4bit\",\n",
    "    \"llama-3.2-3b-instruct-8bit\",\n",
    "    \"llama-3.2-1b-instruct-8bit\",\n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-7B-Instruct-4bit\",\n",
    "    \"Qwen2.5-7B-Instruct-8bit\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct-4bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-3B-Instruct-8bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-8bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-8bit\",\n",
    "    \"gemma-2-9b-it-4bit\",\n",
    "    \"gemma-2-9b-it-8bit\",\n",
    "    \"gemma-2-9b-it\",\n",
    "    \"gemma-2-2b-it-4bit\",\n",
    "    \"gemma-2-2b-it-8bit\",\n",
    "    \"gemma-2-2b-it\",\n",
    "]\n",
    "tasks = [\"summarization\", \"qa\", \"sql\"]\n",
    "\n",
    "results_dir = \"./results/experiment_1/\"\n",
    "\n",
    "missing_models = set()\n",
    "\n",
    "for backend in backends:\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            filename = f\"{backend}_{model}_{task}.csv\"\n",
    "            filepath = os.path.join(results_dir, filename)\n",
    "            if not os.path.exists(filepath):\n",
    "                missing_models.add(model)\n",
    "                break  # No need to check more tasks if one is missing\n",
    "\n",
    "# Print models with missing files\n",
    "if missing_models:\n",
    "    print(\"Models with missing files:\")\n",
    "    for model in sorted(missing_models):\n",
    "        print(model)\n",
    "else:\n",
    "    print(\"✅ All models are complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e8200",
   "metadata": {},
   "source": [
    "try again with the missing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for backend in backends:\n",
    "    for model in list(missing_models):\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=500,\n",
    "                verbose=False,\n",
    "                batch_size=100,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2610ec",
   "metadata": {},
   "source": [
    "### RQ2 - Comparing different inference engines\n",
    "\n",
    "**Framework Dimension** \n",
    "\n",
    "Which inference framework (Transformers, vLLM, DeepSpeed MII,172\n",
    "LMDeploy, llama.cpp) strikes the best balance between system resource usage (e.g., GPU173\n",
    "utilization, joules/token) and system performance (tokens/s)?174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference/models\"\n",
    "\n",
    "backends = [\"vllm\", \"huggingface\", \"llama.cpp\"] #,\"deepspeed_mii\", \"huggingface\"]\n",
    "\n",
    "models   = [\n",
    "    \"gemma-2-9b-it\", \n",
    "    \"gemma-2-2b-it\",\n",
    "\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "   \n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "]\n",
    "\n",
    "tasks    = [\"summarization\", \"qa\", \"sql\",]\n",
    "\n",
    "for backend in backends:\n",
    "    if backend == \"llama.cpp\":\n",
    "        models = [\n",
    "            \"gemma-2-2b-it-fp16.gguf\",\n",
    "            \"gemma-2-9b-it-fp16.gguf\",\n",
    "\n",
    "            \"llama-3.1-8B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "            \n",
    "            \"qwen2.5-0.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-1.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-3b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-7B-instruct-fp16.gguf\",\n",
    "        ]\n",
    "        \n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=100,\n",
    "                verbose=False,\n",
    "                batch_size=20,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f940b1",
   "metadata": {},
   "source": [
    "#### RQ 3 - Comparing different use cases\n",
    "\n",
    "**Scenario/Workload Dimension**\n",
    "\n",
    "How do locally deployed LLMs and inference backends\n",
    "perform and scale across the three dominant inference scenarios—single - stream (single user),\n",
    "batched offline processing, and multi- user server workloads? Do system metrics – throughput,\n",
    "GPU utilization, joules/token — evolve as the average number of queries per second varies\n",
    "over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d90de05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 09:13:22.627181: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747300402.646876  908162 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747300402.653198  908162 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747300402.671978  908162 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747300402.671990  908162 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747300402.671991  908162 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747300402.671993  908162 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-15 09:13:22.677138: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 09:13:26 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-15 09:13:26 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-05-15 09:13:29,214] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from benchmark.benchmark import ModelBenchmark\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_benchmark(\n",
    "    backend: str,\n",
    "    model_name: str,\n",
    "    task: str,\n",
    "    base_path: str,\n",
    "    scenario: str = \"batch\",            # \"single\", \"batch\", or \"server\"\n",
    "    run_time: float = None,             # only for server: total time in seconds\n",
    "    requests_per_sec: float = None,     # only for server: λ (req/s)\n",
    "    batch_size: int = 100,              # only for batch\n",
    "    max_batch_size: int = None,         # only for server: cap per-batch size\n",
    "    sample_interval: float = 0.1,       # telemetry interval (s)\n",
    "    export_path: Optional[str] = None,  # custom export path for server scenario\n",
    "    verbose: bool = False\n",
    "):\n",
    "    print(f\"Running benchmark for {model_name} with {backend} on {task} [{scenario}]\")\n",
    "    try:\n",
    "        bm = ModelBenchmark(\n",
    "            backend=backend,\n",
    "            model_name=model_name,\n",
    "            model_path=f\"{base_path}/models/{model_name}\",\n",
    "            base_path=base_path,\n",
    "            task=task,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        if scenario == \"server\":\n",
    "            assert run_time is not None,    \"Must set run_time in server mode\"\n",
    "            assert requests_per_sec is not None, \"Must set requests_per_sec in server mode\"\n",
    "            df = bm.run(\n",
    "                scenario=\"server\",\n",
    "                run_time=run_time,\n",
    "                requests_per_sec=requests_per_sec,\n",
    "                sample_interval=sample_interval,\n",
    "                max_batch_size=max_batch_size,\n",
    "                export_path=export_path\n",
    "            )\n",
    "\n",
    "        elif scenario == \"single\":\n",
    "            df = bm.run(\n",
    "                samples=None,        # samples ignored\n",
    "                batch_size=1,\n",
    "                scenario=\"single\",\n",
    "                sample_interval=sample_interval,\n",
    "                export_path=export_path\n",
    "            )\n",
    "\n",
    "        elif scenario == \"batch\":\n",
    "            df = bm.run(\n",
    "                samples=None,        # samples ignored\n",
    "                batch_size=batch_size,\n",
    "                scenario=\"batch\",\n",
    "                sample_interval=sample_interval,\n",
    "                export_path=export_path\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scenario: {scenario}\")\n",
    "\n",
    "        bm.close()\n",
    "        del bm\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"✅ Completed: {model_name} | {backend} | {task} | {scenario}\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed: {model_name} | {backend} | {task} | {scenario} -- {e}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb1843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for llama-3.1-8B-Instruct-f16.gguf on llama.cpp/sql (server‐real‐time):\n",
      "prompt_length                          1201.328358 ± 293.119243\n",
      "queue_size                               207.417910 ± 44.591802\n",
      "batch_size                                61.268657 ± 12.710747\n",
      "wait_time                                  18.211364 ± 4.520303\n",
      "response_time                           506.116948 ± 105.411269\n",
      "scheduled_ts                                3.904397 ± 2.329000\n",
      "start_ts                                   22.115788 ± 4.804496\n",
      "GL                                      487.905603 ± 101.424678\n",
      "ATL                                      79.403387 ± 116.597777\n",
      "TTFT                                        0.090900 ± 0.000000\n",
      "TPS                                         0.052985 ± 0.128547\n",
      "SPS                                         0.006119 ± 0.027577\n",
      "Avg GPU Mem (MB)                        16593.755522 ± 0.579277\n",
      "Peak GPU Mem (MB)                       16593.880000 ± 0.000000\n",
      "Avg GPU Util (%)                           96.257761 ± 0.429248\n",
      "Peak GPU Util (%)                          99.000000 ± 0.000000\n",
      "Total Energy (Wh)                           9.747664 ± 2.029316\n",
      "Avg Power (W)                              71.833284 ± 0.450085\n",
      "Peak Power (W)                             75.421493 ± 0.598030\n",
      "Energy per Token (J/token)            5710.419887 ± 8387.030201\n",
      "Energy per Sentence (J/sentence)    29938.042413 ± 12349.161570\n",
      "Memory Usage (MB)                       16593.880000 ± 0.000000\n",
      "Model Size (MB)                         15324.488922 ± 0.000000\n",
      "Overhead (MB)                            1269.391078 ± 0.000000\n",
      "dtype: object\n",
      "✅ Completed: llama-3.1-8B-Instruct-f16.gguf | llama.cpp | sql | server\n",
      "→ llama.cpp | Llama-3.2-1B-Instruct-f16.gguf | sql @ 1 QPS for 120.0s -> /home/ubuntu/fast_llm_inference/results/llama.cpp_Llama-3.2-1B-Instruct-f16.gguf_sql_1QPS_120s_server.csv\n",
      "Running benchmark for Llama-3.2-1B-Instruct-f16.gguf with llama.cpp on sql [server]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Llama-3.2-1B-Instruct-f16.gguf on llama.cpp/sql (server‐real‐time):\n",
      "prompt_length                        1171.452381 ± 278.071431\n",
      "queue_size                              21.166667 ± 13.074240\n",
      "batch_size                              21.166667 ± 13.074240\n",
      "wait_time                                 8.682090 ± 8.431408\n",
      "response_time                           39.773515 ± 25.492254\n",
      "scheduled_ts                            42.775548 ± 20.741945\n",
      "start_ts                                51.457632 ± 25.468583\n",
      "GL                                      31.091429 ± 18.972756\n",
      "ATL                                       2.182264 ± 2.418025\n",
      "TTFT                                      0.022700 ± 0.000000\n",
      "TPS                                       1.875714 ± 2.863882\n",
      "SPS                                       0.157857 ± 0.317175\n",
      "Avg GPU Mem (MB)                       3850.460476 ± 1.086220\n",
      "Peak GPU Mem (MB)                      3850.618095 ± 1.233264\n",
      "Avg GPU Util (%)                         85.928571 ± 3.449819\n",
      "Peak GPU Util (%)                        89.869048 ± 1.190118\n",
      "Total Energy (Wh)                         0.620484 ± 0.379867\n",
      "Avg Power (W)                            71.325238 ± 2.666859\n",
      "Peak Power (W)                           73.287024 ± 0.579900\n",
      "Energy per Token (J/token)            156.771575 ± 174.030911\n",
      "Energy per Sentence (J/sentence)    1669.177374 ± 1418.644896\n",
      "Memory Usage (MB)                      3850.618095 ± 1.233264\n",
      "Model Size (MB)                        2364.726410 ± 0.000000\n",
      "Overhead (MB)                          1485.891685 ± 1.233264\n",
      "dtype: object\n",
      "✅ Completed: Llama-3.2-1B-Instruct-f16.gguf | llama.cpp | sql | server\n",
      "→ llama.cpp | Llama-3.2-1B-Instruct-f16.gguf | sql @ 2 QPS for 120.0s -> /home/ubuntu/fast_llm_inference/results/llama.cpp_Llama-3.2-1B-Instruct-f16.gguf_sql_2QPS_120s_server.csv\n",
      "Running benchmark for Llama-3.2-1B-Instruct-f16.gguf with llama.cpp on sql [server]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Llama-3.2-1B-Instruct-f16.gguf on llama.cpp/sql (server‐real‐time):\n",
      "prompt_length                        1174.679245 ± 273.968491\n",
      "queue_size                              48.886792 ± 23.287978\n",
      "batch_size                              47.075472 ± 21.869177\n",
      "wait_time                               15.151030 ± 12.499842\n",
      "response_time                           84.413990 ± 41.332346\n",
      "scheduled_ts                            30.094266 ± 17.912141\n",
      "start_ts                                45.245295 ± 23.623012\n",
      "GL                                      69.262962 ± 31.970436\n",
      "ATL                                       4.695110 ± 4.550376\n",
      "TTFT                                      0.016500 ± 0.000000\n",
      "TPS                                       0.862358 ± 2.167227\n",
      "SPS                                       0.056887 ± 0.112896\n",
      "Avg GPU Mem (MB)                       3850.757642 ± 0.978709\n",
      "Peak GPU Mem (MB)                      3851.049811 ± 1.099538\n",
      "Avg GPU Util (%)                         87.372264 ± 2.381415\n",
      "Peak GPU Util (%)                        89.943396 ± 0.582772\n",
      "Total Energy (Wh)                         1.382975 ± 0.638173\n",
      "Avg Power (W)                            71.751038 ± 1.111209\n",
      "Peak Power (W)                           73.486226 ± 0.240683\n",
      "Energy per Token (J/token)            337.502430 ± 327.051797\n",
      "Energy per Sentence (J/sentence)    3820.693190 ± 2661.034431\n",
      "Memory Usage (MB)                      3851.049811 ± 1.099538\n",
      "Model Size (MB)                        2364.726410 ± 0.000000\n",
      "Overhead (MB)                          1486.323401 ± 1.099538\n",
      "dtype: object\n",
      "✅ Completed: Llama-3.2-1B-Instruct-f16.gguf | llama.cpp | sql | server\n",
      "→ llama.cpp | Llama-3.2-1B-Instruct-f16.gguf | sql @ 4 QPS for 120.0s -> /home/ubuntu/fast_llm_inference/results/llama.cpp_Llama-3.2-1B-Instruct-f16.gguf_sql_4QPS_120s_server.csv\n",
      "Running benchmark for Llama-3.2-1B-Instruct-f16.gguf with llama.cpp on sql [server]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Llama-3.2-1B-Instruct-f16.gguf on llama.cpp/sql (server‐real‐time):\n",
      "prompt_length                        1170.253623 ± 278.600519\n",
      "queue_size                            204.652174 ± 157.824982\n",
      "batch_size                              59.956522 ± 14.533560\n",
      "wait_time                               44.017252 ± 40.986128\n",
      "response_time                          132.241839 ± 50.943881\n",
      "scheduled_ts                             14.514193 ± 8.918438\n",
      "start_ts                                58.531475 ± 48.639284\n",
      "GL                                      88.224630 ± 21.297353\n",
      "ATL                                       6.057458 ± 5.121387\n",
      "TTFT                                      0.105000 ± 0.000000\n",
      "TPS                                       0.529275 ± 1.414255\n",
      "SPS                                       0.038623 ± 0.072647\n",
      "Avg GPU Mem (MB)                       3851.022174 ± 0.925959\n",
      "Peak GPU Mem (MB)                      3851.706087 ± 0.703620\n",
      "Avg GPU Util (%)                         87.807899 ± 1.902382\n",
      "Peak GPU Util (%)                        89.992754 ± 0.085126\n",
      "Total Energy (Wh)                         1.761385 ± 0.425410\n",
      "Avg Power (W)                            71.724203 ± 1.852735\n",
      "Peak Power (W)                           74.185725 ± 0.574367\n",
      "Energy per Token (J/token)            435.377507 ± 368.118283\n",
      "Energy per Sentence (J/sentence)    4962.758666 ± 2701.863661\n",
      "Memory Usage (MB)                      3851.706087 ± 0.703620\n",
      "Model Size (MB)                        2364.726410 ± 0.000000\n",
      "Overhead (MB)                          1486.979677 ± 0.703620\n",
      "dtype: object\n",
      "✅ Completed: Llama-3.2-1B-Instruct-f16.gguf | llama.cpp | sql | server\n",
      "→ llama.cpp | Llama-3.2-1B-Instruct-f16.gguf | sql @ 8 QPS for 120.0s -> /home/ubuntu/fast_llm_inference/results/llama.cpp_Llama-3.2-1B-Instruct-f16.gguf_sql_8QPS_120s_server.csv\n",
      "Running benchmark for Llama-3.2-1B-Instruct-f16.gguf with llama.cpp on sql [server]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Llama-3.2-1B-Instruct-f16.gguf on llama.cpp/sql (server‐real‐time):\n",
      "prompt_length                        1168.659574 ± 275.843533\n",
      "queue_size                            442.673759 ± 377.528514\n",
      "batch_size                              59.127660 ± 15.369195\n",
      "wait_time                               50.941438 ± 43.900898\n",
      "response_time                          138.040221 ± 55.793783\n",
      "scheduled_ts                             10.843368 ± 6.024778\n",
      "start_ts                                61.784801 ± 49.174143\n",
      "GL                                      87.098793 ± 22.478313\n",
      "ATL                                       6.099038 ± 5.315782\n",
      "TTFT                                      0.015300 ± 0.000000\n",
      "TPS                                       0.497305 ± 1.248260\n",
      "SPS                                       0.037021 ± 0.064140\n",
      "Avg GPU Mem (MB)                       3851.041773 ± 0.905747\n",
      "Peak GPU Mem (MB)                      3851.667234 ± 0.744585\n",
      "Avg GPU Util (%)                         88.006738 ± 1.592738\n",
      "Peak GPU Util (%)                        89.978723 ± 0.252646\n",
      "Total Energy (Wh)                         1.739563 ± 0.449625\n",
      "Avg Power (W)                            71.746879 ± 1.569538\n",
      "Peak Power (W)                           73.479929 ± 0.383369\n",
      "Energy per Token (J/token)            438.526660 ± 382.283210\n",
      "Energy per Sentence (J/sentence)    4867.022625 ± 2721.467335\n",
      "Memory Usage (MB)                      3851.667234 ± 0.744585\n",
      "Model Size (MB)                        2364.726410 ± 0.000000\n",
      "Overhead (MB)                          1486.940824 ± 0.744585\n",
      "dtype: object\n",
      "✅ Completed: Llama-3.2-1B-Instruct-f16.gguf | llama.cpp | sql | server\n",
      "→ llama.cpp | Llama-3.2-3B-Instruct-f16.gguf | sql @ 1 QPS for 120.0s -> /home/ubuntu/fast_llm_inference/results/llama.cpp_Llama-3.2-3B-Instruct-f16.gguf_sql_1QPS_120s_server.csv\n",
      "Running benchmark for Llama-3.2-3B-Instruct-f16.gguf with llama.cpp on sql [server]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference\"\n",
    "backends = [\"vllm\", \"llama.cpp\", \"huggingface\"]\n",
    "tasks    = [\"sql\"]\n",
    "server_rps      = [1, 2, 4, 8]\n",
    "run_time        = 120.0     # seconds\n",
    "sample_interval = 0.05      # s\n",
    "max_batch_size  = 64        # cap per batch\n",
    "\n",
    "for backend in backends:\n",
    "    if backend == \"llama.cpp\":\n",
    "        models = [\n",
    "            \"gemma-2-2b-it-fp16.gguf\",\n",
    "            \"gemma-2-9b-it-fp16.gguf\",\n",
    "            \"llama-3.1-8B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "            \"qwen2.5-0.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-1.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-3b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-7B-instruct-fp16.gguf\",\n",
    "        ]\n",
    "    else:\n",
    "        models = [\n",
    "            \"gemma-2-2b-it\",\n",
    "            \"llama-3.1-8B-Instruct\",\n",
    "            \"llama-3.2-3b-instruct\",\n",
    "            \"llama-3.2-1b-instruct\",\n",
    "            \"Qwen2.5-7B-Instruct\",\n",
    "            \"Qwen2.5-3B-Instruct\",\n",
    "            \"Qwen2.5-1.5B-Instruct\",\n",
    "            \"Qwen2.5-0.5B-Instruct\",\n",
    "        ]\n",
    "        if backend != \"vllm\":\n",
    "            models.append(\"gemma-2-9b-it\") # too large for vllm\n",
    "\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            for rps in server_rps:\n",
    "                export_path = f\"{base_path}/results/{backend}_{model}_{task}_{rps}QPS_{int(run_time)}s_server.csv\"\n",
    "                print(f\"→ {backend} | {model} | {task} @ {rps} QPS for {run_time}s -> {export_path}\")\n",
    "                run_benchmark(\n",
    "                    backend=backend,\n",
    "                    model_name=model,\n",
    "                    task=task,\n",
    "                    base_path=base_path,\n",
    "                    scenario=\"server\",\n",
    "                    run_time=run_time,\n",
    "                    requests_per_sec=rps,\n",
    "                    sample_interval=sample_interval,\n",
    "                    max_batch_size=max_batch_size,\n",
    "                    export_path=export_path,\n",
    "                    verbose=False\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
