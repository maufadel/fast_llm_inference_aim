{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c8f55e",
   "metadata": {},
   "source": [
    "### RQ1 - Comparing different quantization levels\n",
    "\n",
    "**Quantization Dimension**\n",
    "\n",
    "How does quantization in different models and architectures affect system and task-specific metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef740d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 15:48:54.090065: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747064934.110013  260255 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747064934.115811  260255 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747064934.135655  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135683  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135685  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135687  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-12 15:48:54.141568: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 15:48:57 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-12 15:48:57 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-05-12 15:49:00,664] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_benchmark(backend, model_name, task, base_path, samples=500, verbose=False, batch_size=100):\n",
    "    print(f\"Running benchmark for {model_name} with {backend} on {task}\")\n",
    "    try:\n",
    "        bm = ModelBenchmark(\n",
    "            backend=backend,\n",
    "            model_name=model_name,\n",
    "            model_path=f\"{base_path}/models/{model_name}\",\n",
    "            base_path=base_path,\n",
    "            task=task,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        bm.run(samples=samples, batch_size=batch_size)\n",
    "        bm.close()\n",
    "        del bm\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"✅ Completed: {model_name} | {backend} | {task}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed: {model_name} | {backend} | {task} -- {e}\")\n",
    "        torch.cuda.empty_cache()  # ensure no memory leak on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac63b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference/\"\n",
    "\n",
    "backends = [\"vllm\"] #, \"huggingface\",\"deepspeed_mii\", \"llama.cpp\"]\n",
    "models   = [\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.1-8B-Instruct-4bit\",\n",
    "    \"llama-3.1-8B-Instruct-8bit\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "    \"llama-3.2-3b-instruct-4bit\",\n",
    "    \"llama-3.2-1b-instruct-4bit\",\n",
    "    \"llama-3.2-3b-instruct-8bit\",\n",
    "    \"llama-3.2-1b-instruct-8bit\",\n",
    "   \n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-7B-Instruct-4bit\",\n",
    "    \"Qwen2.5-7B-Instruct-8bit\", # some weird error\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct-4bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-3B-Instruct-8bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-8bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-8bit\",\n",
    "\n",
    "\n",
    "    \"gemma-2-9b-it-bnb4\",\n",
    "    \"gemma-2-9b-it-8bit\",\n",
    "    \"gemma-2-9b-it\", # too large\n",
    "    \"gemma-2-2b-it-4bit\",\n",
    "    \"gemma-2-2b-it-8bit\",\n",
    "    \"gemma-2-2b-it\",\n",
    "]\n",
    "\n",
    "tasks    = [\"summarization\", \"qa\", \"sql\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb85145",
   "metadata": {},
   "source": [
    "first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for backend in backends:\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=100,\n",
    "                verbose=False,\n",
    "                batch_size=100,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c7713",
   "metadata": {},
   "source": [
    "check if anything is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a66e3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models with missing files:\n",
      "Qwen2.5-7B-Instruct-8bit\n",
      "gemma-2-9b-it\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define your parameters\n",
    "backends = [\"vllm\"]\n",
    "models = [\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.1-8B-Instruct-4bit\",\n",
    "    \"llama-3.1-8B-Instruct-8bit\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "    \"llama-3.2-3b-instruct-4bit\",\n",
    "    \"llama-3.2-1b-instruct-4bit\",\n",
    "    \"llama-3.2-3b-instruct-8bit\",\n",
    "    \"llama-3.2-1b-instruct-8bit\",\n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-7B-Instruct-4bit\",\n",
    "    \"Qwen2.5-7B-Instruct-8bit\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct-4bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-3B-Instruct-8bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-8bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-8bit\",\n",
    "    \"gemma-2-9b-it-4bit\",\n",
    "    \"gemma-2-9b-it-8bit\",\n",
    "    \"gemma-2-9b-it\",\n",
    "    \"gemma-2-2b-it-4bit\",\n",
    "    \"gemma-2-2b-it-8bit\",\n",
    "    \"gemma-2-2b-it\",\n",
    "]\n",
    "tasks = [\"summarization\", \"qa\", \"sql\"]\n",
    "\n",
    "results_dir = \"./results/experiment_1/\"\n",
    "\n",
    "missing_models = set()\n",
    "\n",
    "for backend in backends:\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            filename = f\"{backend}_{model}_{task}.csv\"\n",
    "            filepath = os.path.join(results_dir, filename)\n",
    "            if not os.path.exists(filepath):\n",
    "                missing_models.add(model)\n",
    "                break  # No need to check more tasks if one is missing\n",
    "\n",
    "# Print models with missing files\n",
    "if missing_models:\n",
    "    print(\"Models with missing files:\")\n",
    "    for model in sorted(missing_models):\n",
    "        print(model)\n",
    "else:\n",
    "    print(\"✅ All models are complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e8200",
   "metadata": {},
   "source": [
    "try again with the missing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for backend in backends:\n",
    "    for model in list(missing_models):\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=500,\n",
    "                verbose=False,\n",
    "                batch_size=100,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2610ec",
   "metadata": {},
   "source": [
    "### RQ2 - Comparing different inference engines\n",
    "\n",
    "**Framework Dimension** \n",
    "\n",
    "Which inference framework (Transformers, vLLM, DeepSpeed MII,172\n",
    "LMDeploy, llama.cpp) strikes the best balance between system resource usage (e.g., GPU173\n",
    "utilization, joules/token) and system performance (tokens/s)?174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference/models\"\n",
    "\n",
    "backends = [\"vllm\", \"huggingface\", \"llama.cpp\"] #,\"deepspeed_mii\", \"huggingface\"]\n",
    "\n",
    "models   = [\n",
    "    \"gemma-2-9b-it\", \n",
    "    \"gemma-2-2b-it\",\n",
    "\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "   \n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "]\n",
    "\n",
    "tasks    = [\"summarization\", \"qa\", \"sql\",]\n",
    "\n",
    "for backend in backends:\n",
    "    if backend == \"llama.cpp\":\n",
    "        models = [\n",
    "            \"gemma-2-2b-it-fp16.gguf\",\n",
    "            \"gemma-2-9b-it-fp16.gguf\",\n",
    "\n",
    "            \"llama-3.1-8B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "            \n",
    "            \"qwen2.5-0.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-1.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-3b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-7B-instruct-fp16.gguf\",\n",
    "        ]\n",
    "        \n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=100,\n",
    "                verbose=False,\n",
    "                batch_size=20,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f940b1",
   "metadata": {},
   "source": [
    "#### RQ 3 - Comparing different use cases\n",
    "\n",
    "**Scenario/Workload Dimension**\n",
    "\n",
    "How do locally deployed LLMs and inference backends\n",
    "perform and scale across the three dominant inference scenarios—single - stream (single user),\n",
    "batched offline processing, and multi- user server workloads? Do system metrics – throughput,\n",
    "GPU utilization, joules/token — evolve as the average number of queries per second varies\n",
    "over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d90de05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 03:42:40.554419: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747366960.579138 1160723 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747366960.586688 1160723 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747366960.607277 1160723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747366960.607300 1160723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747366960.607303 1160723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747366960.607305 1160723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-16 03:42:40.613899: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-16 03:42:48 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-16 03:42:49 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-05-16 03:42:55,873] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from benchmark.benchmark import ModelBenchmark\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_benchmark(\n",
    "    backend: str,\n",
    "    model_name: str,\n",
    "    task: str,\n",
    "    base_path: str,\n",
    "    scenario: str = \"batch\",            # \"single\", \"batch\", or \"server\"\n",
    "    run_time: float = None,             # only for server: total time in seconds\n",
    "    requests_per_sec: float = None,     # only for server: λ (req/s)\n",
    "    batch_size: int = 100,              # only for batch\n",
    "    max_batch_size: int = None,         # only for server: cap per-batch size\n",
    "    sample_interval: float = 0.1,       # telemetry interval (s)\n",
    "    export_path: Optional[str] = None,  # custom export path for server scenario\n",
    "    verbose: bool = False\n",
    "):\n",
    "    print(f\"Running benchmark for {model_name} with {backend} on {task} [{scenario}]\")\n",
    "    try:\n",
    "        bm = ModelBenchmark(\n",
    "            backend=backend,\n",
    "            model_name=model_name,\n",
    "            model_path=f\"{base_path}/models/{model_name}\",\n",
    "            base_path=base_path,\n",
    "            task=task,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        if scenario == \"server\":\n",
    "            assert run_time is not None,    \"Must set run_time in server mode\"\n",
    "            assert requests_per_sec is not None, \"Must set requests_per_sec in server mode\"\n",
    "            df = bm.run(\n",
    "                scenario=\"server\",\n",
    "                run_time=run_time,\n",
    "                requests_per_sec=requests_per_sec,\n",
    "                sample_interval=sample_interval,\n",
    "                max_batch_size=max_batch_size,\n",
    "                export_path=export_path\n",
    "            )\n",
    "\n",
    "        elif scenario == \"single\":\n",
    "            df = bm.run(\n",
    "                samples=100,        # samples ignored\n",
    "                batch_size=1,\n",
    "                scenario=\"single\",\n",
    "                sample_interval=sample_interval,\n",
    "                export_path=export_path\n",
    "            )\n",
    "\n",
    "        elif scenario == \"batch\":\n",
    "            df = bm.run(\n",
    "                samples=100,        # samples ignored\n",
    "                batch_size=batch_size,\n",
    "                scenario=\"batch\",\n",
    "                sample_interval=sample_interval,\n",
    "                export_path=export_path\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scenario: {scenario}\")\n",
    "\n",
    "        bm.close()\n",
    "        del bm\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"✅ Completed: {model_name} | {backend} | {task} | {scenario}\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed: {model_name} | {backend} | {task} | {scenario} -- {e}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb1843",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference\"\n",
    "backends = [\"vllm\", \"llama.cpp\", \"huggingface\"]\n",
    "tasks    = [\"sql\"]\n",
    "server_rps      = [1, 2, 4, 8]\n",
    "run_time        = 120.0     # seconds\n",
    "sample_interval = 0.05      # s\n",
    "max_batch_size  = 64        # cap per batch\n",
    "\n",
    "for backend in backends:\n",
    "    if backend == \"llama.cpp\":\n",
    "        models = [\n",
    "            \"gemma-2-2b-it-fp16.gguf\",\n",
    "            \"gemma-2-9b-it-fp16.gguf\",\n",
    "            \"llama-3.1-8B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "            \"qwen2.5-0.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-1.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-3b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-7B-instruct-fp16.gguf\",\n",
    "        ]\n",
    "    else:\n",
    "        models = [\n",
    "            \"gemma-2-2b-it\",\n",
    "            \"llama-3.1-8B-Instruct\",\n",
    "            \"llama-3.2-3b-instruct\",\n",
    "            \"llama-3.2-1b-instruct\",\n",
    "            \"Qwen2.5-7B-Instruct\",\n",
    "            \"Qwen2.5-3B-Instruct\",\n",
    "            \"Qwen2.5-1.5B-Instruct\",\n",
    "            \"Qwen2.5-0.5B-Instruct\",\n",
    "        ]\n",
    "        if backend != \"vllm\":\n",
    "            models.append(\"gemma-2-9b-it\") # too large for vllm\n",
    "\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            for rps in server_rps:\n",
    "                export_path = f\"{base_path}/results/{backend}_{model}_{task}_{rps}QPS_{int(run_time)}s_server.csv\"\n",
    "                print(f\"→ {backend} | {model} | {task} @ {rps} QPS for {run_time}s -> {export_path}\")\n",
    "                run_benchmark(\n",
    "                    backend=backend,\n",
    "                    model_name=model,\n",
    "                    task=task,\n",
    "                    base_path=base_path,\n",
    "                    scenario=\"server\",\n",
    "                    run_time=run_time,\n",
    "                    requests_per_sec=rps,\n",
    "                    sample_interval=sample_interval,\n",
    "                    max_batch_size=max_batch_size,\n",
    "                    export_path=export_path,\n",
    "                    verbose=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a94ed06",
   "metadata": {},
   "source": [
    "part 2 - batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f16559",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference\"\n",
    "backends  = [\"huggingface\"] #\"vllm\", \"llama.cpp\", \n",
    "tasks     = [\"summarization\"]\n",
    "batch_sizes = [1, 8, 16, 32, 64]   # ← as requested\n",
    "sample_interval = 0.05             # s\n",
    "max_batch_size  = 64               # keep the same cap\n",
    "\n",
    "for backend in backends:\n",
    "    if backend == \"llama.cpp\":\n",
    "        models = [\n",
    "            \"gemma-2-2b-it-fp16.gguf\",\n",
    "            \"gemma-2-9b-it-fp16.gguf\",\n",
    "            \"llama-3.1-8B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "            \"qwen2.5-0.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-1.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-3b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-7B-instruct-fp16.gguf\",\n",
    "        ]\n",
    "    else:\n",
    "        models = [\n",
    "            #\"gemma-2-2b-it\",\n",
    "            \"llama-3.1-8B-Instruct\",\n",
    "            \"llama-3.2-3b-instruct\",\n",
    "            \"llama-3.2-1b-instruct\",\n",
    "            \"Qwen2.5-7B-Instruct\",\n",
    "            \"Qwen2.5-3B-Instruct\",\n",
    "            \"Qwen2.5-1.5B-Instruct\",\n",
    "            \"Qwen2.5-0.5B-Instruct\",\n",
    "        ]\n",
    "        if backend != \"vllm\":\n",
    "            models.append(\"gemma-2-9b-it\")   # too large for vllm\n",
    "\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            for bs in batch_sizes:\n",
    "                export_path = (\n",
    "                    f\"{base_path}/results/\"\n",
    "                    f\"{backend}_{model}_{task}_{bs}batch.csv\"\n",
    "                )\n",
    "                print(f\"→ {backend} | {model} | {task} @ batch={bs} -> {export_path}\")\n",
    "                run_benchmark(\n",
    "                    backend=backend,\n",
    "                    model_name=model,\n",
    "                    task=task,\n",
    "                    base_path=base_path,\n",
    "                    scenario=\"batch\",\n",
    "                    batch_size=bs,\n",
    "                    sample_interval=sample_interval,\n",
    "                    max_batch_size=max_batch_size,\n",
    "                    export_path=export_path,\n",
    "                    verbose=False,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11221c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-19 08:54:15 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 08:54:16.711341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747644856.737085   58511 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747644856.744882   58511 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747644856.767440   58511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747644856.767460   58511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747644856.767463   58511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747644856.767465   58511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-19 08:54:16.774362: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-19 08:54:35 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 05-19 08:54:37 [config.py:830] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-19 08:54:37 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-19 08:54:38 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', speculative_config=None, tokenizer='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-19 08:54:39 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x6ffd3be5c590>\n",
      "INFO 05-19 08:54:40 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-19 08:54:40 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-19 08:54:40 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-19 08:54:40 [gpu_model_runner.py:1329] Starting to load model /home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit...\n",
      "INFO 05-19 08:54:41 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c9487da48d448b995ce9ba8d5f7432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93d19597f79491cb38fd8c2d617e011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-19 08:54:45 [gpu_model_runner.py:1347] Model loading took 5.3132 GiB and 4.338470 seconds\n",
      "INFO 05-19 08:54:55 [backends.py:420] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/a89f85ea99/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-19 08:54:55 [backends.py:430] Dynamo bytecode transform time: 9.70 s\n",
      "INFO 05-19 08:55:04 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 6.534 s\n",
      "INFO 05-19 08:55:07 [monitor.py:33] torch.compile takes 9.70 s in total\n",
      "INFO 05-19 08:55:10 [kv_cache_utils.py:634] GPU KV cache size: 106,880 tokens\n",
      "INFO 05-19 08:55:10 [kv_cache_utils.py:637] Maximum concurrency for 8,192 tokens per request: 13.05x\n",
      "INFO 05-19 08:56:21 [gpu_model_runner.py:1686] Graph capturing finished in 71 secs, took 1.54 GiB\n",
      "INFO 05-19 08:56:21 [core.py:159] init engine (profile, create kv cache, warmup model) took 95.70 seconds\n",
      "INFO 05-19 08:56:21 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "from benchmark.backends.vllm_backend import VLLMBackend\n",
    "\n",
    "model = VLLMBackend(\"/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit\")\n",
    "model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec34ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(\"What is the purpose of life?\", perplexity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee5b9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-18 11:26:48 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 11:26:49.212117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747567609.229561   12031 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747567609.235248   12031 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747567609.250681   12031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747567609.250693   12031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747567609.250695   12031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747567609.250696   12031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-18 11:26:49.256273: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and setup\n",
    "import os\n",
    "import math\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# (Optional) adjust your model path here\n",
    "MODEL_PATH = \"/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit\"\n",
    "\n",
    "# Cell 2: Load model and define prompts\n",
    "model = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    max_model_len=4096,\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"What is the purpose of life?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd6017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acccb472a5d421db912f2ddeff6bdc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 3: Configure SamplingParams for logprobs & perplexity\n",
    "params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    max_tokens=32,\n",
    "    logprobs=1,\n",
    "    prompt_logprobs=1\n",
    ")\n",
    "\n",
    "# Cell 4: Run generation and display results in a table\n",
    "outputs = model.generate(prompts, params)\n",
    "\n",
    "for i, gen_out in enumerate(outputs):\n",
    "    sample  = gen_out.outputs[0]\n",
    "    text    = sample.text.lstrip()\n",
    "    lp_dict = sample.logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a6b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{2209: Logprob(logprob=-1.4004144668579102, rank=1, decoded_token='ĠIs')},\n",
       " {433: Logprob(logprob=-0.17977960407733917, rank=1, decoded_token='Ġit')},\n",
       " {311: Logprob(logprob=-0.0869283527135849, rank=1, decoded_token='Ġto')},\n",
       " {1505: Logprob(logprob=-0.9659126996994019, rank=1, decoded_token='Ġfind')},\n",
       " {23871: Logprob(logprob=-0.04803086444735527, rank=1, decoded_token='Ġhappiness')},\n",
       " {11: Logprob(logprob=-0.058653172105550766, rank=1, decoded_token=',')},\n",
       " {311: Logprob(logprob=-0.8425228595733643, rank=1, decoded_token='Ġto')},\n",
       " {11322: Logprob(logprob=-0.7215710878372192, rank=1, decoded_token='Ġachieve')},\n",
       " {2450: Logprob(logprob=-0.043824948370456696, rank=1, decoded_token='Ġsuccess')},\n",
       " {11: Logprob(logprob=-0.0013250865740701556, rank=1, decoded_token=',')},\n",
       " {477: Logprob(logprob=-0.7743627429008484, rank=2, decoded_token='Ġor'),\n",
       "  311: Logprob(logprob=-0.6181127429008484, rank=1, decoded_token='Ġto')},\n",
       " {311: Logprob(logprob=-0.031453102827072144, rank=1, decoded_token='Ġto')},\n",
       " {1304: Logprob(logprob=-0.7800344228744507, rank=1, decoded_token='Ġmake')},\n",
       " {264: Logprob(logprob=-0.018590614199638367, rank=1, decoded_token='Ġa')},\n",
       " {6811: Logprob(logprob=-0.16793595254421234, rank=1, decoded_token='Ġdifference')},\n",
       " {304: Logprob(logprob=-0.1038089394569397, rank=1, decoded_token='Ġin')},\n",
       " {279: Logprob(logprob=-0.0009804924484342337, rank=1, decoded_token='Ġthe')},\n",
       " {1917: Logprob(logprob=-0.0022238779347389936, rank=1, decoded_token='Ġworld')},\n",
       " {30: Logprob(logprob=-0.11063252389431, rank=1, decoded_token='?')},\n",
       " {578: Logprob(logprob=-1.3353930711746216, rank=1, decoded_token='ĠThe')},\n",
       " {4320: Logprob(logprob=-0.25850245356559753, rank=1, decoded_token='Ġanswer')},\n",
       " {311: Logprob(logprob=-0.9216098785400391, rank=1, decoded_token='Ġto')},\n",
       " {420: Logprob(logprob=-0.04812448099255562, rank=1, decoded_token='Ġthis')},\n",
       " {3488: Logprob(logprob=-0.02162298373878002, rank=1, decoded_token='Ġquestion')},\n",
       " {374: Logprob(logprob=-0.520580530166626, rank=1, decoded_token='Ġis')},\n",
       " {44122: Logprob(logprob=-1.261415958404541, rank=1, decoded_token='Ġsubjective')},\n",
       " {323: Logprob(logprob=-0.049682993441820145, rank=1, decoded_token='Ġand')},\n",
       " {649: Logprob(logprob=-0.46952304244041443, rank=1, decoded_token='Ġcan')},\n",
       " {13592: Logprob(logprob=-0.009865455329418182, rank=1, decoded_token='Ġvary')},\n",
       " {19407: Logprob(logprob=-0.37328964471817017, rank=1, decoded_token='Ġgreatly')},\n",
       " {505: Logprob(logprob=-0.14553403854370117, rank=1, decoded_token='Ġfrom')},\n",
       " {1732: Logprob(logprob=-0.0395626574754715, rank=1, decoded_token='Ġperson')}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb6b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975be3f036034fe9abea836af4f1d5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prompt 1: The quick brown fox jumps over the lazy dog. ===\n",
      "Generated: The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick\n",
      "\n",
      "       Token |  LogProb |      PPL\n",
      "----------------------------------\n",
      "        ĠThe |  -0.9727 |   2.6450\n",
      "      Ġquick |  -0.3296 |   1.3905\n",
      "      Ġbrown |  -0.0029 |   1.0029\n",
      "        Ġfox |  -0.0016 |   1.0016\n",
      "      Ġjumps |  -0.0055 |   1.0055\n",
      "       Ġover |  -0.0005 |   1.0005\n",
      "        Ġthe |  -0.0005 |   1.0005\n",
      "       Ġlazy |  -0.0005 |   1.0005\n",
      "        Ġdog |  -0.0004 |   1.0004\n",
      "           . |  -0.3360 |   1.3993\n",
      "        ĠThe |  -0.0372 |   1.0379\n",
      "      Ġquick |  -0.0015 |   1.0015\n",
      "      Ġbrown |  -0.0012 |   1.0012\n",
      "        Ġfox |  -0.0007 |   1.0007\n",
      "      Ġjumps |  -0.0013 |   1.0013\n",
      "       Ġover |  -0.0008 |   1.0008\n",
      "        Ġthe |  -0.0010 |   1.0010\n",
      "       Ġlazy |  -0.0010 |   1.0010\n",
      "        Ġdog |  -0.0005 |   1.0005\n",
      "           . |  -0.2609 |   1.2981\n",
      "        ĠThe |  -0.0465 |   1.0476\n",
      "      Ġquick |  -0.0021 |   1.0021\n",
      "      Ġbrown |  -0.0006 |   1.0006\n",
      "        Ġfox |  -0.0014 |   1.0014\n",
      "      Ġjumps |  -0.0008 |   1.0008\n",
      "       Ġover |  -0.0002 |   1.0002\n",
      "        Ġthe |  -0.0008 |   1.0008\n",
      "       Ġlazy |  -0.0007 |   1.0007\n",
      "        Ġdog |  -0.0004 |   1.0004\n",
      "           . |  -0.1384 |   1.1484\n",
      "        ĠThe |  -0.0204 |   1.0206\n",
      "      Ġquick |  -0.0011 |   1.0011\n",
      "\n",
      "=== Prompt 2: What is the purpose of life? ===\n",
      "Generated: Is it to find happiness, to achieve success, to make a difference, or to fulfill a purpose? The answer to this question is different for each individual,\n",
      "\n",
      "       Token |  LogProb |      PPL\n",
      "----------------------------------\n",
      "         ĠIs |  -1.4004 |   4.0569\n",
      "         Ġit |  -0.1798 |   1.1970\n",
      "         Ġto |  -0.0869 |   1.0908\n",
      "       Ġfind |  -0.9659 |   2.6272\n",
      "  Ġhappiness |  -0.0480 |   1.0492\n",
      "           , |  -0.0587 |   1.0604\n",
      "         Ġto |  -0.8425 |   2.3222\n",
      "    Ġachieve |  -0.7216 |   2.0577\n",
      "    Ġsuccess |  -0.0438 |   1.0448\n",
      "           , |  -0.0013 |   1.0013\n",
      "         Ġto |  -0.6181 |   1.8554\n",
      "       Ġmake |  -0.8055 |   2.2377\n",
      "          Ġa |  -0.0282 |   1.0286\n",
      " Ġdifference |  -0.0444 |   1.0454\n",
      "           , |  -0.5264 |   1.6929\n",
      "         Ġor |  -0.0248 |   1.0251\n",
      "  Ġsomething |  -0.7462 |   2.1090\n",
      "    Ġfulfill |  -1.0393 |   2.8273\n",
      "          Ġa |  -1.0664 |   2.9050\n",
      "    Ġpurpose |  -0.8271 |   2.2867\n",
      "           ? |  -0.4777 |   1.6124\n",
      "        ĠThe |  -0.9232 |   2.5173\n",
      "     Ġanswer |  -0.2054 |   1.2280\n",
      "         Ġto |  -1.1311 |   3.0989\n",
      "       Ġthis |  -0.0250 |   1.0253\n",
      "   Ġquestion |  -0.0269 |   1.0272\n",
      "         Ġis |  -0.6289 |   1.8755\n",
      " Ġsubjective |  -1.4515 |   4.2693\n",
      "        Ġfor |  -0.0012 |   1.0012\n",
      "       Ġeach |  -0.1205 |   1.1281\n",
      " Ġindividual |  -0.4515 |   1.5707\n",
      "           , |  -0.2584 |   1.2949\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Run generation and display results in a table plus sequence PPL\n",
    "outputs = model.generate(prompts, params)\n",
    "\n",
    "for i, gen_out in enumerate(outputs):\n",
    "    sample    = gen_out.outputs[0]\n",
    "    text      = sample.text.lstrip()\n",
    "    lp_list   = sample.logprobs            # list of dicts\n",
    "    token_ids = sample.token_ids\n",
    "\n",
    "    # 1) Extract the chosen-token strings & logprobs\n",
    "    tokens, logps = [], []\n",
    "    for entry in lp_list:\n",
    "        # each entry is {token_id: Logprob(...), ...}\n",
    "        for tid, lp_obj in entry.items():\n",
    "            if lp_obj.rank == 1:\n",
    "                tokens.append(lp_obj.decoded_token)\n",
    "                logps.append(lp_obj.logprob)\n",
    "                break\n",
    "\n",
    "    # 2) Compute per-token perplexity\n",
    "    ppl = [math.exp(-lp) for lp in logps]\n",
    "\n",
    "    # 3) Print per-token table\n",
    "    print(f\"\\n=== Prompt {i+1}: {prompts[i]} ===\")\n",
    "    print(f\"Generated: {text}\\n\")\n",
    "    print(f\"{'Token':>12} | {'LogProb':>8} | {'PPL':>8}\")\n",
    "    print(\"-\" * 34)\n",
    "    for tok, lp, p in zip(tokens, logps, ppl):\n",
    "        print(f\"{tok:>12} | {lp:8.4f} | {p:8.4f}\")\n",
    "\n",
    "    # 4) Compute sequence-level perplexity\n",
    "    ppl_seq = math.exp(- sum(logps) / len(logps))\n",
    "    print(f\"\\nSequence-level Perplexity: {ppl_seq:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c265094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-24 08:53:42 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 08:53:42.902549: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748076822.920393  365621 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748076822.926101  365621 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748076822.941715  365621 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748076822.941728  365621 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748076822.941730  365621 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748076822.941731  365621 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-24 08:53:42.946886: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-24 08:54:02 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'score', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 05-24 08:54:04 [config.py:830] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-24 08:54:04 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-24 08:54:05 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', speculative_config=None, tokenizer='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-24 08:54:05 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x778114ff65d0>\n",
      "ERROR 05-24 08:54:05 [core.py:396] EngineCore failed to start.\n",
      "ERROR 05-24 08:54:05 [core.py:396] Traceback (most recent call last):\n",
      "ERROR 05-24 08:54:05 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "ERROR 05-24 08:54:05 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 05-24 08:54:05 [core.py:396]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-24 08:54:05 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "ERROR 05-24 08:54:05 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 05-24 08:54:05 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 64, in __init__\n",
      "ERROR 05-24 08:54:05 [core.py:396]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 05-24 08:54:05 [core.py:396]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-24 08:54:05 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "ERROR 05-24 08:54:05 [core.py:396]     self._init_executor()\n",
      "ERROR 05-24 08:54:05 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 46, in _init_executor\n",
      "ERROR 05-24 08:54:05 [core.py:396]     self.collective_rpc(\"init_device\")\n",
      "ERROR 05-24 08:54:05 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 05-24 08:54:05 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 05-24 08:54:05 [core.py:396]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-24 08:54:05 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "ERROR 05-24 08:54:05 [core.py:396]     return func(*args, **kwargs)\n",
      "ERROR 05-24 08:54:05 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-24 08:54:05 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 604, in init_device\n",
      "ERROR 05-24 08:54:05 [core.py:396]     self.worker.init_device()  # type: ignore\n",
      "ERROR 05-24 08:54:05 [core.py:396]     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-24 08:54:05 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 125, in init_device\n",
      "ERROR 05-24 08:54:05 [core.py:396]     torch.cuda.set_device(self.device)\n",
      "ERROR 05-24 08:54:05 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 476, in set_device\n",
      "ERROR 05-24 08:54:05 [core.py:396]     torch._C._cuda_setDevice(device)\n",
      "ERROR 05-24 08:54:05 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n",
      "ERROR 05-24 08:54:05 [core.py:396]     raise RuntimeError(\n",
      "ERROR 05-24 08:54:05 [core.py:396] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 400, in run_engine_core\n",
      "    raise e\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 64, in __init__\n",
      "    self.model_executor = executor_class(vllm_config)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "    self._init_executor()\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 46, in _init_executor\n",
      "    self.collective_rpc(\"init_device\")\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 604, in init_device\n",
      "    self.worker.init_device()  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 125, in init_device\n",
      "    torch.cuda.set_device(self.device)\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 476, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbenchmark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbenchmark\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelBenchmark\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m mb = \u001b[43mModelBenchmark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvllm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllama-3.1-8B-Instruct-4bit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/ubuntu/fast_llm_inference/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fast_llm_inference/benchmark/benchmark.py:56\u001b[39m, in \u001b[36mModelBenchmark.__init__\u001b[39m\u001b[34m(self, backend, model_name, model_path, base_path, verbose)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mself\u001b[39m.startup_time = time.time() - start\n\u001b[32m     55\u001b[39m start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mself\u001b[39m.load_model_time = time.time() - start\n\u001b[32m     59\u001b[39m \u001b[38;5;28mself\u001b[39m.model_size = \u001b[38;5;28mself\u001b[39m.get_model_size_mb(model_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fast_llm_inference/benchmark/backends/vllm_backend.py:9\u001b[39m, in \u001b[36mVLLMBackend.load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/utils.py:1161\u001b[39m, in \u001b[36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1154\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1156\u001b[39m         warnings.warn(\n\u001b[32m   1157\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1158\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1159\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:247\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    217\u001b[39m engine_args = EngineArgs(\n\u001b[32m    218\u001b[39m     model=model,\n\u001b[32m    219\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m     **kwargs,\n\u001b[32m    244\u001b[39m )\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    251\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/engine/llm_engine.py:510\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    507\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    508\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py:112\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    111\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py:92\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m.output_processor = OutputProcessor(\u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m     89\u001b[39m                                         log_stats=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# FIXME: implement\u001b[39;49;00m\n\u001b[32m     98\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[38;5;28mself\u001b[39m.engine_core.engine_core.model_executor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:73\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m AsyncMPClient(vllm_config, executor_class, log_stats)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:494\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[32m    493\u001b[39m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n\u001b[32m    503\u001b[39m     \u001b[38;5;66;03m# Ensure that the outputs socket processing thread does not have\u001b[39;00m\n\u001b[32m    504\u001b[39m     \u001b[38;5;66;03m# a ref to the client which prevents gc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:398\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28mself\u001b[39m._init_core_engines(vllm_config, new_core_engine,\n\u001b[32m    395\u001b[39m                         \u001b[38;5;28mself\u001b[39m.resources.core_engines)\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# Wait for engine core process(es) to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[38;5;28mself\u001b[39m.utility_results: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, AnyFuture] = {}\n\u001b[32m    402\u001b[39m \u001b[38;5;66;03m# Request objects which may contain pytorch-allocated tensors\u001b[39;00m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# that we need to keep references to until zmq is done with the\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# underlying data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:430\u001b[39m, in \u001b[36mMPClient._wait_for_engine_startup\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(events) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m events[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m] != sync_input_socket:\n\u001b[32m    429\u001b[39m     \u001b[38;5;66;03m# One of the core processes exited.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    431\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mSee root cause above.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    433\u001b[39m eng_id_bytes, msg = sync_input_socket.recv_multipart()\n\u001b[32m    434\u001b[39m eng_id = \u001b[38;5;28mint\u001b[39m.from_bytes(eng_id_bytes, byteorder=\u001b[33m\"\u001b[39m\u001b[33mlittle\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above."
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import torch\n",
    "\n",
    "mb = ModelBenchmark(\n",
    "    backend=\"vllm\",\n",
    "    model_name=\"llama-3.1-8B-Instruct-4bit\",\n",
    "    model_path=\"/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit\",\n",
    "    base_path=\"/home/ubuntu/fast_llm_inference/\",\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2de41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-24 08:47:32 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 08:47:32.683955: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748076452.701685  362728 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748076452.707101  362728 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748076452.723169  362728 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748076452.723181  362728 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748076452.723183  362728 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748076452.723184  362728 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-24 08:47:32.728317: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-24 08:47:52 [config.py:717] This model supports multiple tasks: {'generate', 'classify', 'embed', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 05-24 08:47:54 [config.py:830] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-24 08:47:54 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-24 08:47:56 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', speculative_config=None, tokenizer='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-24 08:47:56 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7b52ad6c6410>\n",
      "ERROR 05-24 08:47:56 [core.py:396] EngineCore failed to start.\n",
      "ERROR 05-24 08:47:56 [core.py:396] Traceback (most recent call last):\n",
      "ERROR 05-24 08:47:56 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "ERROR 05-24 08:47:56 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 05-24 08:47:56 [core.py:396]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-24 08:47:56 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "ERROR 05-24 08:47:56 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 05-24 08:47:56 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 64, in __init__\n",
      "ERROR 05-24 08:47:56 [core.py:396]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 05-24 08:47:56 [core.py:396]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-24 08:47:56 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "ERROR 05-24 08:47:56 [core.py:396]     self._init_executor()\n",
      "ERROR 05-24 08:47:56 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 46, in _init_executor\n",
      "ERROR 05-24 08:47:56 [core.py:396]     self.collective_rpc(\"init_device\")\n",
      "ERROR 05-24 08:47:56 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 05-24 08:47:56 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 05-24 08:47:56 [core.py:396]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-24 08:47:56 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "ERROR 05-24 08:47:56 [core.py:396]     return func(*args, **kwargs)\n",
      "ERROR 05-24 08:47:56 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-24 08:47:56 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 604, in init_device\n",
      "ERROR 05-24 08:47:56 [core.py:396]     self.worker.init_device()  # type: ignore\n",
      "ERROR 05-24 08:47:56 [core.py:396]     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-24 08:47:56 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 125, in init_device\n",
      "ERROR 05-24 08:47:56 [core.py:396]     torch.cuda.set_device(self.device)\n",
      "ERROR 05-24 08:47:56 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 476, in set_device\n",
      "ERROR 05-24 08:47:56 [core.py:396]     torch._C._cuda_setDevice(device)\n",
      "ERROR 05-24 08:47:56 [core.py:396]   File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n",
      "ERROR 05-24 08:47:56 [core.py:396]     raise RuntimeError(\n",
      "ERROR 05-24 08:47:56 [core.py:396] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 400, in run_engine_core\n",
      "    raise e\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 64, in __init__\n",
      "    self.model_executor = executor_class(vllm_config)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "    self._init_executor()\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 46, in _init_executor\n",
      "    self.collective_rpc(\"init_device\")\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 604, in init_device\n",
      "    self.worker.init_device()  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 125, in init_device\n",
      "    torch.cuda.set_device(self.device)\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 476, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "  File \"/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m run_report, details_df = \u001b[43mmb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m            \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msummarization\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m            \u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[43msample_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m mb.close()\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m mb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fast_llm_inference/benchmark/benchmark.py:534\u001b[39m, in \u001b[36mModelBenchmark.run\u001b[39m\u001b[34m(self, samples, batch_size, sample_interval, task, scenario, requests_per_sec, run_time, max_batch_size, export_path, quality_metric)\u001b[39m\n\u001b[32m    526\u001b[39m     run_report, details_df = \u001b[38;5;28mself\u001b[39m._run_batch(\n\u001b[32m    527\u001b[39m         task=task,\n\u001b[32m    528\u001b[39m         samples=samples,\n\u001b[32m    529\u001b[39m         batch_size=\u001b[32m1\u001b[39m,\n\u001b[32m    530\u001b[39m         sample_interval=sample_interval\n\u001b[32m    531\u001b[39m     )\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m scenario == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     run_report, details_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m        \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m=\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_interval\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m scenario == \u001b[33m\"\u001b[39m\u001b[33mserver\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    542\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m requests_per_sec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \\\n\u001b[32m    543\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMust set requests_per_sec in server mode\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fast_llm_inference/benchmark/benchmark.py:210\u001b[39m, in \u001b[36mModelBenchmark._run_batch\u001b[39m\u001b[34m(self, task, samples, batch_size, sample_interval)\u001b[39m\n\u001b[32m    207\u001b[39m startup_time = time.time() - t0\n\u001b[32m    209\u001b[39m t0 = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m load_model_time = time.time() - t0\n\u001b[32m    213\u001b[39m \u001b[38;5;28mself\u001b[39m.model_size = \u001b[38;5;28mself\u001b[39m.get_model_size_mb(\u001b[38;5;28mself\u001b[39m.model_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fast_llm_inference/benchmark/backends/vllm_backend.py:9\u001b[39m, in \u001b[36mVLLMBackend.load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/utils.py:1161\u001b[39m, in \u001b[36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1154\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1156\u001b[39m         warnings.warn(\n\u001b[32m   1157\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1158\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1159\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:247\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    217\u001b[39m engine_args = EngineArgs(\n\u001b[32m    218\u001b[39m     model=model,\n\u001b[32m    219\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m     **kwargs,\n\u001b[32m    244\u001b[39m )\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    251\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/engine/llm_engine.py:510\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    507\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    508\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py:112\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    111\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py:92\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m.output_processor = OutputProcessor(\u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m     89\u001b[39m                                         log_stats=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# FIXME: implement\u001b[39;49;00m\n\u001b[32m     98\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[38;5;28mself\u001b[39m.engine_core.engine_core.model_executor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:73\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m AsyncMPClient(vllm_config, executor_class, log_stats)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:494\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[32m    493\u001b[39m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n\u001b[32m    503\u001b[39m     \u001b[38;5;66;03m# Ensure that the outputs socket processing thread does not have\u001b[39;00m\n\u001b[32m    504\u001b[39m     \u001b[38;5;66;03m# a ref to the client which prevents gc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:398\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28mself\u001b[39m._init_core_engines(vllm_config, new_core_engine,\n\u001b[32m    395\u001b[39m                         \u001b[38;5;28mself\u001b[39m.resources.core_engines)\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# Wait for engine core process(es) to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[38;5;28mself\u001b[39m.utility_results: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, AnyFuture] = {}\n\u001b[32m    402\u001b[39m \u001b[38;5;66;03m# Request objects which may contain pytorch-allocated tensors\u001b[39;00m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# that we need to keep references to until zmq is done with the\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# underlying data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fastllm_venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:430\u001b[39m, in \u001b[36mMPClient._wait_for_engine_startup\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(events) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m events[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m] != sync_input_socket:\n\u001b[32m    429\u001b[39m     \u001b[38;5;66;03m# One of the core processes exited.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    431\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mSee root cause above.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    433\u001b[39m eng_id_bytes, msg = sync_input_socket.recv_multipart()\n\u001b[32m    434\u001b[39m eng_id = \u001b[38;5;28mint\u001b[39m.from_bytes(eng_id_bytes, byteorder=\u001b[33m\"\u001b[39m\u001b[33mlittle\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above."
     ]
    }
   ],
   "source": [
    "run_report, details_df = mb.run(\n",
    "            samples=8,\n",
    "            batch_size=4,\n",
    "            task=\"summarization\",\n",
    "            scenario=\"batch\",\n",
    "            sample_interval=0.1\n",
    "        )\n",
    "\n",
    "mb.close()\n",
    "del mb\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d390ea80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "startup_time_sec              5.497100\n",
       "load_model_time_sec           3.980000\n",
       "ttft_sec                      0.369300\n",
       "cold_start_sec                9.846400\n",
       "batch_size                    4.000000\n",
       "num_queries                   8.000000\n",
       "model_size_mb              5455.151597\n",
       "total_time_s                 40.128900\n",
       "max_overhead_mb            1382.730000\n",
       "avg_gpu_util_pct             61.520000\n",
       "peak_gpu_util_pct           100.000000\n",
       "avg_cpu_util_pct              6.270000\n",
       "peak_cpu_util_pct             6.860000\n",
       "avg_power_w                  65.780000\n",
       "peak_power_w                 66.750000\n",
       "total_energy_wh               0.732043\n",
       "total_energy_j             2635.352623\n",
       "total_tokens                518.000000\n",
       "total_sentences              27.000000\n",
       "avg_prompt_length          4406.500000\n",
       "avg_num_tokens_req           64.750000\n",
       "avg_num_sentences_req         3.375000\n",
       "avg_ATL                       0.077315\n",
       "avg_GL                        5.016111\n",
       "avg_TPS                      12.940000\n",
       "avg_SPS                       0.677500\n",
       "avg_energy_per_token          5.084373\n",
       "avg_energy_per_sentence      97.734764\n",
       "avg_ROUGE-1                   0.393239\n",
       "avg_ROUGE-2                   0.164769\n",
       "avg_ROUGE-L                   0.237786\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_report.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb3f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>num_tokens_req</th>\n",
       "      <th>num_sentences_req</th>\n",
       "      <th>ATL</th>\n",
       "      <th>GL</th>\n",
       "      <th>TPS</th>\n",
       "      <th>SPS</th>\n",
       "      <th>energy_per_token</th>\n",
       "      <th>energy_per_sentence</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5425</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>President Barack Obama honored the Super Bowl ...</td>\n",
       "      <td>Brady cited 'prior family commitments' in bowi...</td>\n",
       "      <td>59</td>\n",
       "      <td>3</td>\n",
       "      <td>0.078977</td>\n",
       "      <td>4.659635</td>\n",
       "      <td>12.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>5.118698</td>\n",
       "      <td>96.572765</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.113636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3706</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>Rangers have promised to investigate claims th...</td>\n",
       "      <td>Reports emerged on social media  suggesting Mi...</td>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>0.078977</td>\n",
       "      <td>6.002242</td>\n",
       "      <td>12.66</td>\n",
       "      <td>0.67</td>\n",
       "      <td>5.118698</td>\n",
       "      <td>96.572765</td>\n",
       "      <td>0.347107</td>\n",
       "      <td>0.134454</td>\n",
       "      <td>0.198347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6579</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>Adam Gadahn, a 36-year-old American citizen, w...</td>\n",
       "      <td>In his final known video, Adam Gadahn called f...</td>\n",
       "      <td>82</td>\n",
       "      <td>5</td>\n",
       "      <td>0.078977</td>\n",
       "      <td>6.476103</td>\n",
       "      <td>12.66</td>\n",
       "      <td>0.77</td>\n",
       "      <td>5.118698</td>\n",
       "      <td>96.572765</td>\n",
       "      <td>0.417910</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.268657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3493</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>John Barnes and Jamie Redknapp, former Liverpo...</td>\n",
       "      <td>John Barnes appeared as a guest on Sky's A Lea...</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>0.078977</td>\n",
       "      <td>5.212473</td>\n",
       "      <td>12.66</td>\n",
       "      <td>0.58</td>\n",
       "      <td>5.118698</td>\n",
       "      <td>96.572765</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.330579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4193</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>John Higgins and Graeme Dott led the Scottish ...</td>\n",
       "      <td>John Higgins beats Robert Milkins 10-5 to reac...</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>0.075653</td>\n",
       "      <td>3.933952</td>\n",
       "      <td>13.22</td>\n",
       "      <td>0.76</td>\n",
       "      <td>5.050047</td>\n",
       "      <td>98.896762</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.229167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4136</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>Arsenal will wear their yellow and blue away s...</td>\n",
       "      <td>Arsenal face Aston Villa in the FA Cup final a...</td>\n",
       "      <td>74</td>\n",
       "      <td>3</td>\n",
       "      <td>0.075653</td>\n",
       "      <td>5.598316</td>\n",
       "      <td>13.22</td>\n",
       "      <td>0.54</td>\n",
       "      <td>5.050047</td>\n",
       "      <td>98.896762</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.257143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4381</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>Liverpool advanced to the FA Cup semi-finals w...</td>\n",
       "      <td>Coutinho hit the only goal of the game as Live...</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>0.075653</td>\n",
       "      <td>4.312217</td>\n",
       "      <td>13.22</td>\n",
       "      <td>0.70</td>\n",
       "      <td>5.050047</td>\n",
       "      <td>98.896762</td>\n",
       "      <td>0.414815</td>\n",
       "      <td>0.135338</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3339</td>\n",
       "      <td>### SYSTEM\\nYou are a news-summarization assis...</td>\n",
       "      <td>West Coast Shaving blended the faces of 30 leg...</td>\n",
       "      <td>Members of 30 rock band members merged into on...</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>0.075653</td>\n",
       "      <td>3.933952</td>\n",
       "      <td>13.22</td>\n",
       "      <td>0.76</td>\n",
       "      <td>5.050047</td>\n",
       "      <td>98.896762</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_length                                             prompt  \\\n",
       "0           5425  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "1           3706  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "2           6579  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "3           3493  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "4           4193  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "5           4136  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "6           4381  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "7           3339  ### SYSTEM\\nYou are a news-summarization assis...   \n",
       "\n",
       "                                    generated_answer  \\\n",
       "0  President Barack Obama honored the Super Bowl ...   \n",
       "1  Rangers have promised to investigate claims th...   \n",
       "2  Adam Gadahn, a 36-year-old American citizen, w...   \n",
       "3  John Barnes and Jamie Redknapp, former Liverpo...   \n",
       "4  John Higgins and Graeme Dott led the Scottish ...   \n",
       "5  Arsenal will wear their yellow and blue away s...   \n",
       "6  Liverpool advanced to the FA Cup semi-finals w...   \n",
       "7  West Coast Shaving blended the faces of 30 leg...   \n",
       "\n",
       "                                    reference_answer  num_tokens_req  \\\n",
       "0  Brady cited 'prior family commitments' in bowi...              59   \n",
       "1  Reports emerged on social media  suggesting Mi...              76   \n",
       "2  In his final known video, Adam Gadahn called f...              82   \n",
       "3  John Barnes appeared as a guest on Sky's A Lea...              66   \n",
       "4  John Higgins beats Robert Milkins 10-5 to reac...              52   \n",
       "5  Arsenal face Aston Villa in the FA Cup final a...              74   \n",
       "6  Coutinho hit the only goal of the game as Live...              57   \n",
       "7  Members of 30 rock band members merged into on...              52   \n",
       "\n",
       "   num_sentences_req       ATL        GL    TPS   SPS  energy_per_token  \\\n",
       "0                  3  0.078977  4.659635  12.66  0.64          5.118698   \n",
       "1                  4  0.078977  6.002242  12.66  0.67          5.118698   \n",
       "2                  5  0.078977  6.476103  12.66  0.77          5.118698   \n",
       "3                  3  0.078977  5.212473  12.66  0.58          5.118698   \n",
       "4                  3  0.075653  3.933952  13.22  0.76          5.050047   \n",
       "5                  3  0.075653  5.598316  13.22  0.54          5.050047   \n",
       "6                  3  0.075653  4.312217  13.22  0.70          5.050047   \n",
       "7                  3  0.075653  3.933952  13.22  0.76          5.050047   \n",
       "\n",
       "   energy_per_sentence   ROUGE-1   ROUGE-2   ROUGE-L  \n",
       "0            96.572765  0.272727  0.139535  0.113636  \n",
       "1            96.572765  0.347107  0.134454  0.198347  \n",
       "2            96.572765  0.417910  0.151515  0.268657  \n",
       "3            96.572765  0.512397  0.285714  0.330579  \n",
       "4            98.896762  0.333333  0.127660  0.229167  \n",
       "5            98.896762  0.514286  0.246377  0.257143  \n",
       "6            98.896762  0.414815  0.135338  0.266667  \n",
       "7            98.896762  0.333333  0.097561  0.238095  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
