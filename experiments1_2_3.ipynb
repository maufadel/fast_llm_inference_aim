{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c8f55e",
   "metadata": {},
   "source": [
    "### RQ1 - Comparing different quantization levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_benchmark(backend, model_name, task, base_path=\"/home/ubuntu/fast_llm_inference/models\", samples=500, verbose=False, batch_size=100):\n",
    "    print(f\"Running benchmark for {model_name} with {backend} on {task}\")\n",
    "    try:\n",
    "        bm = ModelBenchmark(\n",
    "            backend=backend,\n",
    "            model_name=model_name,\n",
    "            model_path=f\"{base_path}/{model_name}\",\n",
    "            task=task,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        bm.run(samples=samples, batch_size=batch_size)\n",
    "        bm.close()\n",
    "        del bm\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"✅ Completed: {model_name} | {backend} | {task}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed: {model_name} | {backend} | {task} -- {e}\")\n",
    "        torch.cuda.empty_cache()  # ensure no memory leak on error\n",
    "\n",
    "\n",
    "base_path = \"/home/ubuntu/fast_llm_inference/models\"\n",
    "\n",
    "backends = [\"vllm\"] #, \"huggingface\",\"deepspeed_mii\", \"llama.cpp\"]\n",
    "models   = [\n",
    "   # \"llama-3.1-8B-Instruct\",\n",
    "   # \"llama-3.1-8B-Instruct-4bit\",\n",
    "   # \"llama-3.1-8B-Instruct-8bit\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "    \"llama-3.2-3b-instruct-4bit\",\n",
    "    \"llama-3.2-1b-instruct-4bit\",\n",
    "    \"llama-3.2-3b-instruct-8bit\",\n",
    "    \"llama-3.2-1b-instruct-8bit\",\n",
    "   \n",
    "   # \"Qwen2.5-7B-Instruct\",\n",
    "   # \"Qwen2.5-7B-Instruct-4bit\",\n",
    "   ### \"Qwen2.5-7B-Instruct-8bit\", # some weird error\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct-4bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-3B-Instruct-8bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-8bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-8bit\",\n",
    "\n",
    "\n",
    "    #\"gemma-2-9b-it-bnb4\",\n",
    "    #\"gemma-2-9b-it-8bit\",\n",
    "    ### \"gemma-2-9b-it\", # too large\n",
    "    #\"gemma-2-2b-it-4bit\",\n",
    "    #\"gemma-2-2b-it-8bit\",\n",
    "    #\"gemma-2-2b-it\",\n",
    "]\n",
    "tasks    = [\"summarization\", \"qa\", \"sql\",]\n",
    "\n",
    "for backend in backends:\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=500,\n",
    "                verbose=False,\n",
    "                batch_size=100,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2610ec",
   "metadata": {},
   "source": [
    "### RQ2 - Comapring different inference engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130bae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 01:47:22.335784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746928042.686955  715944 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746928042.790989  715944 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746928043.633385  715944 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746928043.633515  715944 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746928043.633520  715944 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746928043.633524  715944 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-11 01:47:23.718528: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-11 01:47:36 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-11 01:47:36 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-05-11 01:47:43,255] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Running benchmark for qwen2.5-3b-instruct-fp16.gguf with llama.cpp on summarization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for qwen2.5-3b-instruct-fp16.gguf on llama.cpp/summarization:\n",
      "prompt_length                       5467.110000 ± 1780.621183\n",
      "TTFT                                      0.037240 ± 0.002196\n",
      "ATL                                       1.907586 ± 0.404739\n",
      "GL                                      103.282380 ± 6.380167\n",
      "TPS                                       0.548400 ± 0.119880\n",
      "SPS                                       0.034100 ± 0.007797\n",
      "Avg GPU Mem (MB)                       7188.030000 ± 5.935774\n",
      "Peak GPU Mem (MB)                      7189.480000 ± 6.407067\n",
      "Avg GPU Util (%)                         91.788000 ± 0.394477\n",
      "Peak GPU Util (%)                       100.000000 ± 0.000000\n",
      "Total Energy (Wh)                         2.051205 ± 0.128664\n",
      "Avg Power (W)                            71.492000 ± 0.212099\n",
      "Peak Power (W)                           74.938000 ± 0.328658\n",
      "Energy per Token (J/token)             136.380099 ± 28.955550\n",
      "Energy per Sentence (J/sentence)     2268.556692 ± 555.147749\n",
      "Memory Usage (MB)                      7189.480000 ± 6.407067\n",
      "Model Size (MB)                        5892.102478 ± 0.000000\n",
      "Overhead (MB)                          1297.377522 ± 6.407067\n",
      "ROUGE-1                                   0.383396 ± 0.095185\n",
      "ROUGE-2                                   0.135917 ± 0.075227\n",
      "ROUGE-L                                   0.242484 ± 0.078435\n",
      "dtype: object\n",
      "✅ Completed: qwen2.5-3b-instruct-fp16.gguf | llama.cpp | summarization\n",
      "Running benchmark for qwen2.5-3b-instruct-fp16.gguf with llama.cpp on qa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for qwen2.5-3b-instruct-fp16.gguf on llama.cpp/qa:\n",
      "prompt_length                       1393.740000 ± 302.397244\n",
      "TTFT                                     0.035200 ± 0.000360\n",
      "ATL                                    24.702549 ± 11.375563\n",
      "GL                                      35.124600 ± 0.881287\n",
      "TPS                                      0.063900 ± 0.071264\n",
      "SPS                                      0.031200 ± 0.005908\n",
      "Avg GPU Mem (MB)                      7167.946000 ± 2.506683\n",
      "Peak GPU Mem (MB)                     7168.680000 ± 2.412091\n",
      "Avg GPU Util (%)                        91.738000 ± 0.576138\n",
      "Peak GPU Util (%)                       95.000000 ± 0.000000\n",
      "Total Energy (Wh)                        0.698264 ± 0.018237\n",
      "Avg Power (W)                           71.566000 ± 0.394590\n",
      "Peak Power (W)                          74.466000 ± 0.552701\n",
      "Energy per Token (J/token)          1768.265073 ± 815.255899\n",
      "Energy per Sentence (J/sentence)    2463.341444 ± 255.088205\n",
      "Memory Usage (MB)                     7168.680000 ± 2.412091\n",
      "Model Size (MB)                       5892.102478 ± 0.000000\n",
      "Overhead (MB)                         1276.577522 ± 2.412091\n",
      "exact_match                              0.620000 ± 0.487832\n",
      "F1_score                                 0.760117 ± 0.358138\n",
      "dtype: object\n",
      "✅ Completed: qwen2.5-3b-instruct-fp16.gguf | llama.cpp | qa\n",
      "Running benchmark for qwen2.5-3b-instruct-fp16.gguf with llama.cpp on sql\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for qwen2.5-3b-instruct-fp16.gguf on llama.cpp/sql:\n",
      "prompt_length                        1163.740000 ± 264.891883\n",
      "TTFT                                      0.035960 ± 0.000972\n",
      "ATL                                       4.107402 ± 3.616989\n",
      "GL                                       56.953800 ± 5.185162\n",
      "TPS                                       0.400000 ± 0.247055\n",
      "SPS                                       0.096800 ± 0.069904\n",
      "Avg GPU Mem (MB)                       7165.032000 ± 1.042286\n",
      "Peak GPU Mem (MB)                      7165.080000 ± 0.984732\n",
      "Avg GPU Util (%)                         91.832000 ± 0.368968\n",
      "Peak GPU Util (%)                        95.000000 ± 0.000000\n",
      "Total Energy (Wh)                         1.135422 ± 0.104077\n",
      "Avg Power (W)                            71.766000 ± 0.182198\n",
      "Peak Power (W)                           73.934000 ± 0.462475\n",
      "Energy per Token (J/token)            294.793370 ± 259.616610\n",
      "Energy per Sentence (J/sentence)    1812.023837 ± 1647.471307\n",
      "Memory Usage (MB)                      7165.080000 ± 0.984732\n",
      "Model Size (MB)                        5892.102478 ± 0.000000\n",
      "Overhead (MB)                          1272.977522 ± 0.984732\n",
      "AST_equal                                 0.110000 ± 0.314466\n",
      "Normalized_equal                          0.150000 ± 0.358870\n",
      "dtype: object\n",
      "✅ Completed: qwen2.5-3b-instruct-fp16.gguf | llama.cpp | sql\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_benchmark(backend, model_name, task, base_path=\"/home/ubuntu/fast_llm_inference/models\", samples=500, verbose=False, batch_size=100):\n",
    "    print(f\"Running benchmark for {model_name} with {backend} on {task}\")\n",
    "    try:\n",
    "        bm = ModelBenchmark(\n",
    "            backend=backend,\n",
    "            model_name=model_name,\n",
    "            model_path=f\"{base_path}/{model_name}\",\n",
    "            task=task,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        bm.run(samples=samples, batch_size=batch_size)\n",
    "        bm.close()\n",
    "        del bm\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"✅ Completed: {model_name} | {backend} | {task}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed: {model_name} | {backend} | {task} -- {e}\")\n",
    "        torch.cuda.empty_cache()  # ensure no memory leak on error\n",
    "\n",
    "\n",
    "base_path = \"/home/ubuntu/fast_llm_inference/models\"\n",
    "\n",
    "backends = [\"huggingface\"] #\"llama.cpp\"] #\"vllm\" ,\"deepspeed_mii\", \"huggingface\"]\n",
    "\n",
    "models   = [\n",
    "    \"gemma-2-9b-it\", \n",
    "    \"gemma-2-2b-it\",\n",
    "\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "   \n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "]\n",
    "\n",
    "tasks    = [\"summarization\", \"qa\", \"sql\",]\n",
    "\n",
    "for backend in backends:\n",
    "    if backend == \"llama.cpp\":\n",
    "        models = [\n",
    "            \"gemma-2-2b-it-fp16.gguf\",\n",
    "            \"gemma-2-9b-it-fp16.gguf\",\n",
    "\n",
    "            \"llama-3.1-8B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "            \n",
    "            \"qwen2.5-0.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-1.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-3b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-7B-instruct-fp16.gguf\",\n",
    "        ]\n",
    "        \n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=100,\n",
    "                verbose=False,\n",
    "                batch_size=20,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
