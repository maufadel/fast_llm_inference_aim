{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c8f55e",
   "metadata": {},
   "source": [
    "### RQ1 - Comparing different quantization levels\n",
    "\n",
    "**Quantization Dimension**\n",
    "\n",
    "How does quantization in different models and architectures affect system and task-specific metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef740d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 15:48:54.090065: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747064934.110013  260255 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747064934.115811  260255 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747064934.135655  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135683  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135685  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747064934.135687  260255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-12 15:48:54.141568: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-12 15:48:57 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-12 15:48:57 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-05-12 15:49:00,664] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_benchmark(backend, model_name, task, base_path, samples=500, verbose=False, batch_size=100):\n",
    "    print(f\"Running benchmark for {model_name} with {backend} on {task}\")\n",
    "    try:\n",
    "        bm = ModelBenchmark(\n",
    "            backend=backend,\n",
    "            model_name=model_name,\n",
    "            model_path=f\"{base_path}/models/{model_name}\",\n",
    "            base_path=base_path,\n",
    "            task=task,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        bm.run(samples=samples, batch_size=batch_size)\n",
    "        bm.close()\n",
    "        del bm\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"✅ Completed: {model_name} | {backend} | {task}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed: {model_name} | {backend} | {task} -- {e}\")\n",
    "        torch.cuda.empty_cache()  # ensure no memory leak on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac63b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference/\"\n",
    "\n",
    "backends = [\"vllm\"] #, \"huggingface\",\"deepspeed_mii\", \"llama.cpp\"]\n",
    "models   = [\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.1-8B-Instruct-4bit\",\n",
    "    \"llama-3.1-8B-Instruct-8bit\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "    \"llama-3.2-3b-instruct-4bit\",\n",
    "    \"llama-3.2-1b-instruct-4bit\",\n",
    "    \"llama-3.2-3b-instruct-8bit\",\n",
    "    \"llama-3.2-1b-instruct-8bit\",\n",
    "   \n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-7B-Instruct-4bit\",\n",
    "    \"Qwen2.5-7B-Instruct-8bit\", # some weird error\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct-4bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-3B-Instruct-8bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-8bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-8bit\",\n",
    "\n",
    "\n",
    "    \"gemma-2-9b-it-bnb4\",\n",
    "    \"gemma-2-9b-it-8bit\",\n",
    "    \"gemma-2-9b-it\", # too large\n",
    "    \"gemma-2-2b-it-4bit\",\n",
    "    \"gemma-2-2b-it-8bit\",\n",
    "    \"gemma-2-2b-it\",\n",
    "]\n",
    "\n",
    "tasks    = [\"summarization\", \"qa\", \"sql\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb85145",
   "metadata": {},
   "source": [
    "first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for backend in backends:\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=100,\n",
    "                verbose=False,\n",
    "                batch_size=100,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c7713",
   "metadata": {},
   "source": [
    "check if anything is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a66e3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models with missing files:\n",
      "Qwen2.5-7B-Instruct-8bit\n",
      "gemma-2-9b-it\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define your parameters\n",
    "backends = [\"vllm\"]\n",
    "models = [\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.1-8B-Instruct-4bit\",\n",
    "    \"llama-3.1-8B-Instruct-8bit\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "    \"llama-3.2-3b-instruct-4bit\",\n",
    "    \"llama-3.2-1b-instruct-4bit\",\n",
    "    \"llama-3.2-3b-instruct-8bit\",\n",
    "    \"llama-3.2-1b-instruct-8bit\",\n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-7B-Instruct-4bit\",\n",
    "    \"Qwen2.5-7B-Instruct-8bit\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct-4bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-4bit\",\n",
    "    \"Qwen2.5-3B-Instruct-8bit\",\n",
    "    \"Qwen2.5-1.5B-Instruct-8bit\",\n",
    "    \"Qwen2.5-0.5B-Instruct-8bit\",\n",
    "    \"gemma-2-9b-it-4bit\",\n",
    "    \"gemma-2-9b-it-8bit\",\n",
    "    \"gemma-2-9b-it\",\n",
    "    \"gemma-2-2b-it-4bit\",\n",
    "    \"gemma-2-2b-it-8bit\",\n",
    "    \"gemma-2-2b-it\",\n",
    "]\n",
    "tasks = [\"summarization\", \"qa\", \"sql\"]\n",
    "\n",
    "results_dir = \"./results/experiment_1/\"\n",
    "\n",
    "missing_models = set()\n",
    "\n",
    "for backend in backends:\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            filename = f\"{backend}_{model}_{task}.csv\"\n",
    "            filepath = os.path.join(results_dir, filename)\n",
    "            if not os.path.exists(filepath):\n",
    "                missing_models.add(model)\n",
    "                break  # No need to check more tasks if one is missing\n",
    "\n",
    "# Print models with missing files\n",
    "if missing_models:\n",
    "    print(\"Models with missing files:\")\n",
    "    for model in sorted(missing_models):\n",
    "        print(model)\n",
    "else:\n",
    "    print(\"✅ All models are complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e8200",
   "metadata": {},
   "source": [
    "try again with the missing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for backend in backends:\n",
    "    for model in list(missing_models):\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=500,\n",
    "                verbose=False,\n",
    "                batch_size=100,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2610ec",
   "metadata": {},
   "source": [
    "### RQ2 - Comparing different inference engines\n",
    "\n",
    "**Framework Dimension** \n",
    "\n",
    "Which inference framework (Transformers, vLLM, DeepSpeed MII,172\n",
    "LMDeploy, llama.cpp) strikes the best balance between system resource usage (e.g., GPU173\n",
    "utilization, joules/token) and system performance (tokens/s)?174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference/models\"\n",
    "\n",
    "backends = [\"vllm\", \"huggingface\", \"llama.cpp\"] #,\"deepspeed_mii\", \"huggingface\"]\n",
    "\n",
    "models   = [\n",
    "    \"gemma-2-9b-it\", \n",
    "    \"gemma-2-2b-it\",\n",
    "\n",
    "    \"llama-3.1-8B-Instruct\",\n",
    "    \"llama-3.2-3b-instruct\",\n",
    "    \"llama-3.2-1b-instruct\",\n",
    "   \n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen2.5-0.5B-Instruct\",\n",
    "]\n",
    "\n",
    "tasks    = [\"summarization\", \"qa\", \"sql\",]\n",
    "\n",
    "for backend in backends:\n",
    "    if backend == \"llama.cpp\":\n",
    "        models = [\n",
    "            \"gemma-2-2b-it-fp16.gguf\",\n",
    "            \"gemma-2-9b-it-fp16.gguf\",\n",
    "\n",
    "            \"llama-3.1-8B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "            \n",
    "            \"qwen2.5-0.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-1.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-3b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-7B-instruct-fp16.gguf\",\n",
    "        ]\n",
    "        \n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            run_benchmark(\n",
    "                backend=backend,\n",
    "                model_name=model,\n",
    "                task=task,\n",
    "                base_path=base_path,\n",
    "                samples=100,\n",
    "                verbose=False,\n",
    "                batch_size=20,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f940b1",
   "metadata": {},
   "source": [
    "#### RQ 3 - Comparing different use cases\n",
    "\n",
    "**Scenario/Workload Dimension**\n",
    "\n",
    "How do locally deployed LLMs and inference backends\n",
    "perform and scale across the three dominant inference scenarios—single - stream (single user),\n",
    "batched offline processing, and multi- user server workloads? Do system metrics – throughput,\n",
    "GPU utilization, joules/token — evolve as the average number of queries per second varies\n",
    "over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d90de05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 03:42:40.554419: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747366960.579138 1160723 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747366960.586688 1160723 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747366960.607277 1160723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747366960.607300 1160723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747366960.607303 1160723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747366960.607305 1160723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-16 03:42:40.613899: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-16 03:42:48 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-16 03:42:49 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-05-16 03:42:55,873] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from benchmark.benchmark import ModelBenchmark\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_benchmark(\n",
    "    backend: str,\n",
    "    model_name: str,\n",
    "    task: str,\n",
    "    base_path: str,\n",
    "    scenario: str = \"batch\",            # \"single\", \"batch\", or \"server\"\n",
    "    run_time: float = None,             # only for server: total time in seconds\n",
    "    requests_per_sec: float = None,     # only for server: λ (req/s)\n",
    "    batch_size: int = 100,              # only for batch\n",
    "    max_batch_size: int = None,         # only for server: cap per-batch size\n",
    "    sample_interval: float = 0.1,       # telemetry interval (s)\n",
    "    export_path: Optional[str] = None,  # custom export path for server scenario\n",
    "    verbose: bool = False\n",
    "):\n",
    "    print(f\"Running benchmark for {model_name} with {backend} on {task} [{scenario}]\")\n",
    "    try:\n",
    "        bm = ModelBenchmark(\n",
    "            backend=backend,\n",
    "            model_name=model_name,\n",
    "            model_path=f\"{base_path}/models/{model_name}\",\n",
    "            base_path=base_path,\n",
    "            task=task,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        if scenario == \"server\":\n",
    "            assert run_time is not None,    \"Must set run_time in server mode\"\n",
    "            assert requests_per_sec is not None, \"Must set requests_per_sec in server mode\"\n",
    "            df = bm.run(\n",
    "                scenario=\"server\",\n",
    "                run_time=run_time,\n",
    "                requests_per_sec=requests_per_sec,\n",
    "                sample_interval=sample_interval,\n",
    "                max_batch_size=max_batch_size,\n",
    "                export_path=export_path\n",
    "            )\n",
    "\n",
    "        elif scenario == \"single\":\n",
    "            df = bm.run(\n",
    "                samples=100,        # samples ignored\n",
    "                batch_size=1,\n",
    "                scenario=\"single\",\n",
    "                sample_interval=sample_interval,\n",
    "                export_path=export_path\n",
    "            )\n",
    "\n",
    "        elif scenario == \"batch\":\n",
    "            df = bm.run(\n",
    "                samples=100,        # samples ignored\n",
    "                batch_size=batch_size,\n",
    "                scenario=\"batch\",\n",
    "                sample_interval=sample_interval,\n",
    "                export_path=export_path\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scenario: {scenario}\")\n",
    "\n",
    "        bm.close()\n",
    "        del bm\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"✅ Completed: {model_name} | {backend} | {task} | {scenario}\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed: {model_name} | {backend} | {task} | {scenario} -- {e}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb1843",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference\"\n",
    "backends = [\"vllm\", \"llama.cpp\", \"huggingface\"]\n",
    "tasks    = [\"sql\"]\n",
    "server_rps      = [1, 2, 4, 8]\n",
    "run_time        = 120.0     # seconds\n",
    "sample_interval = 0.05      # s\n",
    "max_batch_size  = 64        # cap per batch\n",
    "\n",
    "for backend in backends:\n",
    "    if backend == \"llama.cpp\":\n",
    "        models = [\n",
    "            \"gemma-2-2b-it-fp16.gguf\",\n",
    "            \"gemma-2-9b-it-fp16.gguf\",\n",
    "            \"llama-3.1-8B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "            \"qwen2.5-0.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-1.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-3b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-7B-instruct-fp16.gguf\",\n",
    "        ]\n",
    "    else:\n",
    "        models = [\n",
    "            \"gemma-2-2b-it\",\n",
    "            \"llama-3.1-8B-Instruct\",\n",
    "            \"llama-3.2-3b-instruct\",\n",
    "            \"llama-3.2-1b-instruct\",\n",
    "            \"Qwen2.5-7B-Instruct\",\n",
    "            \"Qwen2.5-3B-Instruct\",\n",
    "            \"Qwen2.5-1.5B-Instruct\",\n",
    "            \"Qwen2.5-0.5B-Instruct\",\n",
    "        ]\n",
    "        if backend != \"vllm\":\n",
    "            models.append(\"gemma-2-9b-it\") # too large for vllm\n",
    "\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            for rps in server_rps:\n",
    "                export_path = f\"{base_path}/results/{backend}_{model}_{task}_{rps}QPS_{int(run_time)}s_server.csv\"\n",
    "                print(f\"→ {backend} | {model} | {task} @ {rps} QPS for {run_time}s -> {export_path}\")\n",
    "                run_benchmark(\n",
    "                    backend=backend,\n",
    "                    model_name=model,\n",
    "                    task=task,\n",
    "                    base_path=base_path,\n",
    "                    scenario=\"server\",\n",
    "                    run_time=run_time,\n",
    "                    requests_per_sec=rps,\n",
    "                    sample_interval=sample_interval,\n",
    "                    max_batch_size=max_batch_size,\n",
    "                    export_path=export_path,\n",
    "                    verbose=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a94ed06",
   "metadata": {},
   "source": [
    "part 2 - batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f16559",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/ubuntu/fast_llm_inference\"\n",
    "backends  = [\"huggingface\"] #\"vllm\", \"llama.cpp\", \n",
    "tasks     = [\"summarization\"]\n",
    "batch_sizes = [1, 8, 16, 32, 64]   # ← as requested\n",
    "sample_interval = 0.05             # s\n",
    "max_batch_size  = 64               # keep the same cap\n",
    "\n",
    "for backend in backends:\n",
    "    if backend == \"llama.cpp\":\n",
    "        models = [\n",
    "            \"gemma-2-2b-it-fp16.gguf\",\n",
    "            \"gemma-2-9b-it-fp16.gguf\",\n",
    "            \"llama-3.1-8B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "            \"Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "            \"qwen2.5-0.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-1.5b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-3b-instruct-fp16.gguf\",\n",
    "            \"qwen2.5-7B-instruct-fp16.gguf\",\n",
    "        ]\n",
    "    else:\n",
    "        models = [\n",
    "            #\"gemma-2-2b-it\",\n",
    "            \"llama-3.1-8B-Instruct\",\n",
    "            \"llama-3.2-3b-instruct\",\n",
    "            \"llama-3.2-1b-instruct\",\n",
    "            \"Qwen2.5-7B-Instruct\",\n",
    "            \"Qwen2.5-3B-Instruct\",\n",
    "            \"Qwen2.5-1.5B-Instruct\",\n",
    "            \"Qwen2.5-0.5B-Instruct\",\n",
    "        ]\n",
    "        if backend != \"vllm\":\n",
    "            models.append(\"gemma-2-9b-it\")   # too large for vllm\n",
    "\n",
    "    for model in models:\n",
    "        for task in tasks:\n",
    "            for bs in batch_sizes:\n",
    "                export_path = (\n",
    "                    f\"{base_path}/results/\"\n",
    "                    f\"{backend}_{model}_{task}_{bs}batch.csv\"\n",
    "                )\n",
    "                print(f\"→ {backend} | {model} | {task} @ batch={bs} -> {export_path}\")\n",
    "                run_benchmark(\n",
    "                    backend=backend,\n",
    "                    model_name=model,\n",
    "                    task=task,\n",
    "                    base_path=base_path,\n",
    "                    scenario=\"batch\",\n",
    "                    batch_size=bs,\n",
    "                    sample_interval=sample_interval,\n",
    "                    max_batch_size=max_batch_size,\n",
    "                    export_path=export_path,\n",
    "                    verbose=False,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11221c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-19 08:54:15 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 08:54:16.711341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747644856.737085   58511 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747644856.744882   58511 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747644856.767440   58511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747644856.767460   58511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747644856.767463   58511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747644856.767465   58511 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-19 08:54:16.774362: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-19 08:54:35 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 05-19 08:54:37 [config.py:830] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-19 08:54:37 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-19 08:54:38 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', speculative_config=None, tokenizer='/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-19 08:54:39 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x6ffd3be5c590>\n",
      "INFO 05-19 08:54:40 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-19 08:54:40 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-19 08:54:40 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-19 08:54:40 [gpu_model_runner.py:1329] Starting to load model /home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit...\n",
      "INFO 05-19 08:54:41 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c9487da48d448b995ce9ba8d5f7432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93d19597f79491cb38fd8c2d617e011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-19 08:54:45 [gpu_model_runner.py:1347] Model loading took 5.3132 GiB and 4.338470 seconds\n",
      "INFO 05-19 08:54:55 [backends.py:420] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/a89f85ea99/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-19 08:54:55 [backends.py:430] Dynamo bytecode transform time: 9.70 s\n",
      "INFO 05-19 08:55:04 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 6.534 s\n",
      "INFO 05-19 08:55:07 [monitor.py:33] torch.compile takes 9.70 s in total\n",
      "INFO 05-19 08:55:10 [kv_cache_utils.py:634] GPU KV cache size: 106,880 tokens\n",
      "INFO 05-19 08:55:10 [kv_cache_utils.py:637] Maximum concurrency for 8,192 tokens per request: 13.05x\n",
      "INFO 05-19 08:56:21 [gpu_model_runner.py:1686] Graph capturing finished in 71 secs, took 1.54 GiB\n",
      "INFO 05-19 08:56:21 [core.py:159] init engine (profile, create kv cache, warmup model) took 95.70 seconds\n",
      "INFO 05-19 08:56:21 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "from benchmark.backends.vllm_backend import VLLMBackend\n",
    "\n",
    "model = VLLMBackend(\"/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit\")\n",
    "model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec34ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(\"What is the purpose of life?\", perplexity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee5b9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-18 11:26:48 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 11:26:49.212117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747567609.229561   12031 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747567609.235248   12031 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747567609.250681   12031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747567609.250693   12031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747567609.250695   12031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747567609.250696   12031 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-18 11:26:49.256273: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and setup\n",
    "import os\n",
    "import math\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# (Optional) adjust your model path here\n",
    "MODEL_PATH = \"/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit\"\n",
    "\n",
    "# Cell 2: Load model and define prompts\n",
    "model = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    max_model_len=4096,\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"What is the purpose of life?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd6017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acccb472a5d421db912f2ddeff6bdc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 3: Configure SamplingParams for logprobs & perplexity\n",
    "params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    max_tokens=32,\n",
    "    logprobs=1,\n",
    "    prompt_logprobs=1\n",
    ")\n",
    "\n",
    "# Cell 4: Run generation and display results in a table\n",
    "outputs = model.generate(prompts, params)\n",
    "\n",
    "for i, gen_out in enumerate(outputs):\n",
    "    sample  = gen_out.outputs[0]\n",
    "    text    = sample.text.lstrip()\n",
    "    lp_dict = sample.logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a6b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{2209: Logprob(logprob=-1.4004144668579102, rank=1, decoded_token='ĠIs')},\n",
       " {433: Logprob(logprob=-0.17977960407733917, rank=1, decoded_token='Ġit')},\n",
       " {311: Logprob(logprob=-0.0869283527135849, rank=1, decoded_token='Ġto')},\n",
       " {1505: Logprob(logprob=-0.9659126996994019, rank=1, decoded_token='Ġfind')},\n",
       " {23871: Logprob(logprob=-0.04803086444735527, rank=1, decoded_token='Ġhappiness')},\n",
       " {11: Logprob(logprob=-0.058653172105550766, rank=1, decoded_token=',')},\n",
       " {311: Logprob(logprob=-0.8425228595733643, rank=1, decoded_token='Ġto')},\n",
       " {11322: Logprob(logprob=-0.7215710878372192, rank=1, decoded_token='Ġachieve')},\n",
       " {2450: Logprob(logprob=-0.043824948370456696, rank=1, decoded_token='Ġsuccess')},\n",
       " {11: Logprob(logprob=-0.0013250865740701556, rank=1, decoded_token=',')},\n",
       " {477: Logprob(logprob=-0.7743627429008484, rank=2, decoded_token='Ġor'),\n",
       "  311: Logprob(logprob=-0.6181127429008484, rank=1, decoded_token='Ġto')},\n",
       " {311: Logprob(logprob=-0.031453102827072144, rank=1, decoded_token='Ġto')},\n",
       " {1304: Logprob(logprob=-0.7800344228744507, rank=1, decoded_token='Ġmake')},\n",
       " {264: Logprob(logprob=-0.018590614199638367, rank=1, decoded_token='Ġa')},\n",
       " {6811: Logprob(logprob=-0.16793595254421234, rank=1, decoded_token='Ġdifference')},\n",
       " {304: Logprob(logprob=-0.1038089394569397, rank=1, decoded_token='Ġin')},\n",
       " {279: Logprob(logprob=-0.0009804924484342337, rank=1, decoded_token='Ġthe')},\n",
       " {1917: Logprob(logprob=-0.0022238779347389936, rank=1, decoded_token='Ġworld')},\n",
       " {30: Logprob(logprob=-0.11063252389431, rank=1, decoded_token='?')},\n",
       " {578: Logprob(logprob=-1.3353930711746216, rank=1, decoded_token='ĠThe')},\n",
       " {4320: Logprob(logprob=-0.25850245356559753, rank=1, decoded_token='Ġanswer')},\n",
       " {311: Logprob(logprob=-0.9216098785400391, rank=1, decoded_token='Ġto')},\n",
       " {420: Logprob(logprob=-0.04812448099255562, rank=1, decoded_token='Ġthis')},\n",
       " {3488: Logprob(logprob=-0.02162298373878002, rank=1, decoded_token='Ġquestion')},\n",
       " {374: Logprob(logprob=-0.520580530166626, rank=1, decoded_token='Ġis')},\n",
       " {44122: Logprob(logprob=-1.261415958404541, rank=1, decoded_token='Ġsubjective')},\n",
       " {323: Logprob(logprob=-0.049682993441820145, rank=1, decoded_token='Ġand')},\n",
       " {649: Logprob(logprob=-0.46952304244041443, rank=1, decoded_token='Ġcan')},\n",
       " {13592: Logprob(logprob=-0.009865455329418182, rank=1, decoded_token='Ġvary')},\n",
       " {19407: Logprob(logprob=-0.37328964471817017, rank=1, decoded_token='Ġgreatly')},\n",
       " {505: Logprob(logprob=-0.14553403854370117, rank=1, decoded_token='Ġfrom')},\n",
       " {1732: Logprob(logprob=-0.0395626574754715, rank=1, decoded_token='Ġperson')}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb6b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975be3f036034fe9abea836af4f1d5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prompt 1: The quick brown fox jumps over the lazy dog. ===\n",
      "Generated: The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick\n",
      "\n",
      "       Token |  LogProb |      PPL\n",
      "----------------------------------\n",
      "        ĠThe |  -0.9727 |   2.6450\n",
      "      Ġquick |  -0.3296 |   1.3905\n",
      "      Ġbrown |  -0.0029 |   1.0029\n",
      "        Ġfox |  -0.0016 |   1.0016\n",
      "      Ġjumps |  -0.0055 |   1.0055\n",
      "       Ġover |  -0.0005 |   1.0005\n",
      "        Ġthe |  -0.0005 |   1.0005\n",
      "       Ġlazy |  -0.0005 |   1.0005\n",
      "        Ġdog |  -0.0004 |   1.0004\n",
      "           . |  -0.3360 |   1.3993\n",
      "        ĠThe |  -0.0372 |   1.0379\n",
      "      Ġquick |  -0.0015 |   1.0015\n",
      "      Ġbrown |  -0.0012 |   1.0012\n",
      "        Ġfox |  -0.0007 |   1.0007\n",
      "      Ġjumps |  -0.0013 |   1.0013\n",
      "       Ġover |  -0.0008 |   1.0008\n",
      "        Ġthe |  -0.0010 |   1.0010\n",
      "       Ġlazy |  -0.0010 |   1.0010\n",
      "        Ġdog |  -0.0005 |   1.0005\n",
      "           . |  -0.2609 |   1.2981\n",
      "        ĠThe |  -0.0465 |   1.0476\n",
      "      Ġquick |  -0.0021 |   1.0021\n",
      "      Ġbrown |  -0.0006 |   1.0006\n",
      "        Ġfox |  -0.0014 |   1.0014\n",
      "      Ġjumps |  -0.0008 |   1.0008\n",
      "       Ġover |  -0.0002 |   1.0002\n",
      "        Ġthe |  -0.0008 |   1.0008\n",
      "       Ġlazy |  -0.0007 |   1.0007\n",
      "        Ġdog |  -0.0004 |   1.0004\n",
      "           . |  -0.1384 |   1.1484\n",
      "        ĠThe |  -0.0204 |   1.0206\n",
      "      Ġquick |  -0.0011 |   1.0011\n",
      "\n",
      "=== Prompt 2: What is the purpose of life? ===\n",
      "Generated: Is it to find happiness, to achieve success, to make a difference, or to fulfill a purpose? The answer to this question is different for each individual,\n",
      "\n",
      "       Token |  LogProb |      PPL\n",
      "----------------------------------\n",
      "         ĠIs |  -1.4004 |   4.0569\n",
      "         Ġit |  -0.1798 |   1.1970\n",
      "         Ġto |  -0.0869 |   1.0908\n",
      "       Ġfind |  -0.9659 |   2.6272\n",
      "  Ġhappiness |  -0.0480 |   1.0492\n",
      "           , |  -0.0587 |   1.0604\n",
      "         Ġto |  -0.8425 |   2.3222\n",
      "    Ġachieve |  -0.7216 |   2.0577\n",
      "    Ġsuccess |  -0.0438 |   1.0448\n",
      "           , |  -0.0013 |   1.0013\n",
      "         Ġto |  -0.6181 |   1.8554\n",
      "       Ġmake |  -0.8055 |   2.2377\n",
      "          Ġa |  -0.0282 |   1.0286\n",
      " Ġdifference |  -0.0444 |   1.0454\n",
      "           , |  -0.5264 |   1.6929\n",
      "         Ġor |  -0.0248 |   1.0251\n",
      "  Ġsomething |  -0.7462 |   2.1090\n",
      "    Ġfulfill |  -1.0393 |   2.8273\n",
      "          Ġa |  -1.0664 |   2.9050\n",
      "    Ġpurpose |  -0.8271 |   2.2867\n",
      "           ? |  -0.4777 |   1.6124\n",
      "        ĠThe |  -0.9232 |   2.5173\n",
      "     Ġanswer |  -0.2054 |   1.2280\n",
      "         Ġto |  -1.1311 |   3.0989\n",
      "       Ġthis |  -0.0250 |   1.0253\n",
      "   Ġquestion |  -0.0269 |   1.0272\n",
      "         Ġis |  -0.6289 |   1.8755\n",
      " Ġsubjective |  -1.4515 |   4.2693\n",
      "        Ġfor |  -0.0012 |   1.0012\n",
      "       Ġeach |  -0.1205 |   1.1281\n",
      " Ġindividual |  -0.4515 |   1.5707\n",
      "           , |  -0.2584 |   1.2949\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Run generation and display results in a table plus sequence PPL\n",
    "outputs = model.generate(prompts, params)\n",
    "\n",
    "for i, gen_out in enumerate(outputs):\n",
    "    sample    = gen_out.outputs[0]\n",
    "    text      = sample.text.lstrip()\n",
    "    lp_list   = sample.logprobs            # list of dicts\n",
    "    token_ids = sample.token_ids\n",
    "\n",
    "    # 1) Extract the chosen-token strings & logprobs\n",
    "    tokens, logps = [], []\n",
    "    for entry in lp_list:\n",
    "        # each entry is {token_id: Logprob(...), ...}\n",
    "        for tid, lp_obj in entry.items():\n",
    "            if lp_obj.rank == 1:\n",
    "                tokens.append(lp_obj.decoded_token)\n",
    "                logps.append(lp_obj.logprob)\n",
    "                break\n",
    "\n",
    "    # 2) Compute per-token perplexity\n",
    "    ppl = [math.exp(-lp) for lp in logps]\n",
    "\n",
    "    # 3) Print per-token table\n",
    "    print(f\"\\n=== Prompt {i+1}: {prompts[i]} ===\")\n",
    "    print(f\"Generated: {text}\\n\")\n",
    "    print(f\"{'Token':>12} | {'LogProb':>8} | {'PPL':>8}\")\n",
    "    print(\"-\" * 34)\n",
    "    for tok, lp, p in zip(tokens, logps, ppl):\n",
    "        print(f\"{tok:>12} | {lp:8.4f} | {p:8.4f}\")\n",
    "\n",
    "    # 4) Compute sequence-level perplexity\n",
    "    ppl_seq = math.exp(- sum(logps) / len(logps))\n",
    "    print(f\"\\nSequence-level Perplexity: {ppl_seq:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c265094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.benchmark import ModelBenchmark\n",
    "\n",
    "mb = ModelBenchmark(\n",
    "    backend=\"huggingface\",\n",
    "    model_name=\"llama-3.1-8B-Instruct-4bit\",\n",
    "    model_path=\"/home/ubuntu/fast_llm_inference/models/llama-3.1-8B-Instruct-4bit\",\n",
    "    base_path=\"/home/ubuntu/fast_llm_inference\",\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2de41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 10:35:57.687143: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747823757.705104  164898 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747823757.710341  164898 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747823757.726062  164898 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747823757.726078  164898 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747823757.726080  164898 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747823757.726082  164898 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-21 10:35:57.731229: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fd3320ba8343d8b36c2d79bfaf29ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/fastllm_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "meta_df, batch_df, query_df = mb.run(\n",
    "            samples=32,\n",
    "            batch_size=8,\n",
    "            task=\"qa\",\n",
    "            scenario=\"batch\",\n",
    "            sample_interval=0.1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae89cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "startup_time_sec        0.0000\n",
       "load_model_time_sec     3.4649\n",
       "ttft_sec                0.3601\n",
       "cold_start_sec          3.8250\n",
       "batch_size              8.0000\n",
       "num_queries            32.0000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a5a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_id                1.500000\n",
       "batch_time_s           11.538825\n",
       "batch_tokens           32.500000\n",
       "batch_sentences         8.250000\n",
       "avg_gpu_mem_mb       6519.325000\n",
       "peak_gpu_mem_mb      6521.880000\n",
       "avg_gpu_util_pct       62.170000\n",
       "peak_gpu_util_pct     100.000000\n",
       "avg_power_w            68.510000\n",
       "peak_power_w           72.822500\n",
       "total_energy_wh         0.220467\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb3f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_id                               1.500000\n",
       "num_tokens                             4.062500\n",
       "num_sentences                          1.031250\n",
       "ATL                                    0.439653\n",
       "GL                                     1.442352\n",
       "TPS                                    0.352813\n",
       "SPS                                    0.090000\n",
       "Energy per Token (J/token)           413.994013\n",
       "Energy per Sentence (J/sentence)     779.852874\n",
       "Memory Usage (MB)                   6521.880000\n",
       "Model Size (MB)                     5455.151597\n",
       "Overhead (MB)                       1066.728403\n",
       "exact_match                            0.812500\n",
       "F1_score                               0.884936\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_df.mean(numeric_only=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastllm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
